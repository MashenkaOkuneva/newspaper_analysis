# Nowcasting German GDP with text data

## Overview

This repository contains the necessary resources to replicate the main results from the paper "Nowcasting German GDP with Text Data" by Mariia Okuneva, Philipp Hauber, Kai Carstensen, and Jasper BÃ¤r. It includes the code for sentiment extraction, topic modeling, and out-of-sample forecasting exercise.

## Repository Structure

The repository is organized into several folders, each dedicated to a specific part of the paper:

- **[data](https://github.com/MashenkaOkuneva/newspaper_analysis/tree/main/data)**: This folder contains notebooks that calculate descriptive statistics for the dataset, both before and after pre-processing. It also includes visualizations of daily publications across all news media.
  - **[Descriptive Statistics.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/data/Descriptive%20Statistics.ipynb)**: Calculates descriptive statistics for the full pre-processed dataset (all news media) and generates a figure showing daily publications across all media sources.
  - **[Descriptive Statistics (no pre-processing).ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/data/Descriptive%20Statistics%20(no%20pre-processing).ipynb)**: Performs similar calculations and generates figures for the unprocessed version of the full dataset.
  - **[Descriptive Statistics (Handelsblatt).ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/data/Descriptive%20Statistics%20(Handelsblatt).ipynb)**: Analyzes the effect of pre-processing on Handelsblatt alone, with figures showing daily publications before and after pre-processing.
  - **[Descriptive Statistics (SZ).ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/data/Descriptive%20Statistics%20(SZ).ipynb)**: Focuses on SZ, presenting statistics and daily publication figures pre- and post-processing.
  - **[Descriptive Statistics (Welt).ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/data/Descriptive%20Statistics%20(Welt).ipynb)**: Examines Welt, detailing statistics and visualizing daily publications before and after pre-processing.
  - **[Descriptive Statistics (dpa).ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/data/Descriptive%20Statistics%20(dpa).ipynb)**: Dedicated to dpa, providing statistics and publication figures highlighting the impact of pre-processing.

- **[MediaTenor_processing](https://github.com/MashenkaOkuneva/newspaper_analysis/tree/main/MediaTenor_processing)**: This folder includes notebooks dedicated to loading full texts of articles annotated by Media Tenor and providing summary and descriptive statistics of the dataset. The articles come from various sources, including BamS, BILD, Spiegel, Focus, Capital, and WamS. We use the full texts of these articles and their sentiment labels to prepare input data for sentiment analysis models.
  - **[MediaTenor_LexisNexis_Factiva](https://github.com/MashenkaOkuneva/newspaper_analysis/tree/main/MediaTenor_processing/MediaTenor_LexisNexis_Factiva)**: Contains the directory structure where we keep full texts of articles annotated by Media Tenor and downloaded from LexisNexis or Factiva, or scraped from Spiegel and Focus. With the exception of the **Spiegel_scrape_2008_txt** and **Focus_scrape_example_txt** directories, all other directories are empty due to copyright restrictions.
  	- **[Spiegel_scrape_2008_txt](https://github.com/MashenkaOkuneva/newspaper_analysis/tree/main/MediaTenor_processing/MediaTenor_LexisNexis_Factiva/Spiegel_scrape_2008_txt)**: Contains 5 example articles scraped from Spiegel from one of the issues of 2008.
	- **[Focus_scrape_example_txt](https://github.com/MashenkaOkuneva/newspaper_analysis/tree/main/MediaTenor_processing/MediaTenor_LexisNexis_Factiva/Focus_scrape_example_txt)**: Contains 5 example articles scraped from Focus website from the 19th issue of 2019.
  - **[Metadata Corrections for Sentiment Analysis.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/Metadata%20Corrections%20for%20Sentiment%20Analysis.ipynb)**: Demonstrates the modifications made to metadata in the Excel file acquired from Media Tenor International. The goal was to ensure accurate matching of articles downloaded from LexisNexis and Factiva with their metadata from the file.
  - **[Summary of the Media Tenor Dataset.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/Summary%20of%20the%20Media%20Tenor%20Dataset.ipynb)**: Analyzes the Media Tenor Dataset to produce its summary and descriptive statistics. This notebook emphasizes the importance of aspect-based sentiment analysis by demonstrating how sentiment indices for different topics, such as business cycle conditions and the labour market, can behave differently over time.
  - **[BILD and BamS.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/BILD%20and%20BamS.ipynb)**: This notebook focuses on loading the full texts of articles from BILD and BamS related to business cycle conditions. It includes the following steps: converting RTF files from Factiva into TXT format, extracting relevant information from the TXT files, and organizing this data into a structured DataFrame.
  - **[Web scraping from Spiegel.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/Web%20scraping%20from%20Spiegel.ipynb)**: This notebook demonstrates how we web scraped 505 articles from Spiegel's website that were annotated by Media Tenor to speed up the downloading process. Additionally, the notebook provides an example of how one can scrape Spiegel articles currently, considering that only articles published up to and including 2009 are now available without a subscription.
  - **[Spiegel.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/Spiegel.ipynb)**: This notebook is dedicated to loading the full texts of articles from Spiegel related to business cycle conditions. It involves several steps: matching articles scraped from Spiegel's website with their metadata, downloading RTF files from Factiva and LexisNexis, converting these RTF files into TXT format, extracting relevant information from the TXT files, and combining all articles into a single dataset.
  - **[Web scraping from Focus.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/Web%20scraping%20from%20Focus.ipynb)**: This notebook details the process of web scraping 223 articles from Focus website that were annotated by Media Tenor, facilitating a faster collection process.
  - **[Focus.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/Focus.ipynb)**: This notebook focuses on loading the full texts of articles from Focus that pertain to business cycle conditions. The process includes several key steps: matching articles scraped from the Focus website with their metadata, downloading RTF files from Factiva and LexisNexis, converting these RTF files into TXT format, extracting relevant information from the TXT files, and integrating all articles into a single dataset.
  - **[Capital.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/Capital.ipynb)**: This notebook loads the full texts of articles from Capital. 
  - **[WamS.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/WamS.ipynb)**: This notebook loads the full texts of articles from WamS.
  - **[Input for Sentiment Models.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/MediaTenor_processing/Input%20for%20Sentiment%20Models.ipynb)**: Prepares input data for sentiment analysis models using the full texts of articles from BamS, BILD, Spiegel, Focus, Capital, and WamS along with their sentiment labels. The notebook combines all articles into a single DataFrame, randomizes their sequence, creates binary sentiment labels, and saves the texts and sentiment labels to separate text files. It also provides descriptive statistics for the sentiment distribution in counts and percentages.

- **[word_embeddings](https://github.com/MashenkaOkuneva/newspaper_analysis/tree/main/word_embeddings)**: This folder contains a notebook for estimating a word2vec model. It also includes code for visualizing the estimated embeddings and exploring how these embeddings can be used to identify words related to business cycle conditions.
  - **[word2vec_estimation.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/word_embeddings/word2vec_estimation.ipynb)**: Estimates the word2vec model using news articles from Handelsblatt, SZ, dpa, and Welt.
  - **[Visualizing_estimated_embeddings.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/word_embeddings/Visualizing_estimated_embeddings.ipynb)**: Visualizes the embeddings of 1000 words closest to 'business cycle conditions' based on cosine similarity. The purpose is to demonstrate that the estimated embeddings effectively capture the semantic properties of words.
  - **[Business_cycle_conditions_words.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/word_embeddings/Business_cycle_conditions_words.ipynb)**: Identifies words related to business cycle conditions by analyzing their semantic proximity in the embedding space. 

- **[sentiment](https://github.com/MashenkaOkuneva/newspaper_analysis/tree/main/sentiment)**: This folder includes notebooks related to sentiment analysis using different approaches such as LSTM, LSVM, and the dictionary-based method.
  - **[LSTM.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/sentiment/LSTM.ipynb)**: Trains an LSTM model on articles from the Media Tenor Dataset. The trained model is then used to predict the sentiment of articles from the main corpus.
  - **[Identify words without embeddings.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/sentiment/Identify%20words%20without%20embeddings.ipynb)**: Identifies words in the Media Tenor Dataset that do not have pre-trained word2vec embeddings. These words will be excluded during LSTM training.  
  - **[Descriptive Statistics (after pre-processing for LSTM).ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/sentiment/Descriptive%20Statistics%20(after%20pre-processing%20for%20LSTM).ipynb)**: Provides descriptive statistics for the main corpus after applying the pre-processing steps required for the LSTM model.
  - **[Training set](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/sentiment/Training%20set.ipynb)**: Identifies articles from the training set and evaluates the effectiveness of the filtering step by retaining only sentences that contain at least one word related to business cycle conditions.
  - **[Validation set](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/sentiment/Validation%20set.ipynb)**: Identifies articles from the validation set and again tests the effectiveness of the filtering step.
  - **[Test set](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/sentiment/Test%20set.ipynb)**: Explores articles from the test set.
  - **[LSVM.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/sentiment/LSVM.ipynb)**: Implements a Linear Support Vector Machine (LSVM) model as a benchmark for sentiment classification. The LSVM model is trained on the same MTI dataset and its results are compared to those of the LSTM model.
  - **[Dictionary Approach.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/sentiment/Dictionary%20Approach.ipynb)**: Utilizes a dictionary-based method for sentiment analysis. The BPW dictionary, a German business sentiment dictionary, is applied to classify sentiment in the test articles. The results are compared to the LSTM and LSVM models to evaluate performance.

- **[topics](https://github.com/MashenkaOkuneva/newspaper_analysis/tree/main/topics)**: This folder contains notebooks for data pre-processing tailored to topic modeling, Latent Dirichlet Allocation (LDA) model estimation, selecting the optimal number of topics through cross-validation, and adjusting the sign of daily topic time series based on sentiment towards business cycle conditions. It also includes a notebook for analyzing the top articles associated with each topic. The LDA estimation was performed using [Stephen Hansen's topic modelling tools](https://www.dropbox.com/scl/fi/x4ypi8dnypwgytlhkjt1y/topic-modelling-tools-with_gsl.zip?rlkey=3hdqorb6mrc18kuvlhe4cnxhk&e=1&dl=0).
  - **[Pre-processing (train data).ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/topics/Pre-processing%20(train%20data).ipynb)**: Pre-processes the training portion of the dataset for LDA modeling. 
  - **[Pre-processing (test data).ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/topics/Pre-processing%20(test%20data).ipynb)**: Pre-processes the test portion of the dataset for LDA modeling. 
  - **[Cross-validation, K={10,50,100}.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/topics/Cross-validation%2C%20K%3D%7B10%2C50%2C100%7D.ipynb)**: Performs cross-validation for LDA model using 10, 50, and 100 topics.
  - **[Cross-validation, K={150,200,250}.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/topics/Cross-validation%2C%20K%3D%7B150%2C200%2C250%7D.ipynb)**: Performs cross-validation for LDA model using 150, 200, and 250 topics.
  - **[Cross-validation (perplexity plot).ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/topics/Cross-validation%20(perplexity%20plot).ipynb)**: Plots perplexity against different values of K, offering a visual guide for selecting the optimal number of topics. 
  - **[Topic model estimation.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/topics/Topic%20model%20estimation.ipynb)**: Estimates the LDA model on the training portion of the data and predicts document-specific topic distributions for the test data. It also generates daily, monthly, and quarterly time series for each topic.
  - **[Sign_adjusted_topics.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/topics/Sign_adjusted_topics.ipynb)**: Generates sign-adjusted topic time series, visualizes them, and conducts robustness checks for the sentiment adjustment methodology.
  - **[Top N articles for Each Topic.ipynb](https://github.com/MashenkaOkuneva/newspaper_analysis/blob/main/topics/Top%20N%20articles%20for%20Each%20Topic.ipynb)**: Explores the top N articles with the highest proportion of each topic, providing insight into the key discussions associated with each topic.
