{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the number of topics K based on 10-fold cross-validation.\n",
    "\n",
    "We proceed as follows:\n",
    "\n",
    "1. Split the train portion of the dataset, specifically the articles published before 2010, randomly into 10 folds.\n",
    "\n",
    "2.1. Estimate the LDA model on the first 9 folds which serve as a training set. Keep the last 10 samples where the chain has converged.\n",
    "\n",
    "2.2. Estimate perplexity on the 10th fold which serves as a test set. To do that, calculate perplexity for each of the 10 samples, using the formula:\n",
    "\n",
    "$ Perplexity = exp\\left(- \\frac{\\sum_{d=1}^{D}\\sum_{v=1}^{V} n_{d,v} log(\\sum_{k=1}^{K} \\hat{\\theta_{d}^{k}} \\hat{\\beta_{k}^{v}})}{\\sum_{d=1}^{D} N_d}\\right)$,\n",
    "\n",
    "where $n_{d,v}$ is a count of word $v$ in document $d$, and $N_d$ is the number of words in a document $d$. In this formula, $\\hat{\\beta_{k}^{v}}$ corresponds to the topic distributions from the LDA estimation on the train set. We re-sample $\\hat{\\theta_{d}^{k}}$ for each of the 10 samples and all the documents in the test set using 100 iterations of Gibbs sampling. The final perplexity for the 10th fold is calculated by averaging over 10 samples.\n",
    "\n",
    "3. Repeat the second step 9 more times each time changing the fold that serves as a test set.\n",
    "\n",
    "4. Calculate the final perplexity as the average over 10 folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the pre-processed articles from the train set which were prepared in a prior step using the `Pre-processing (train data)` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'schalck', u'milliardenkredit', u'sichert', u'zahlungsfah', u'ex', u'ddr', u'darstell', u'devisenbeschaff', u'ehemaligen_ddr', u'schalck', u'golodkowski', u'josef_strau\\xdf', u'eingefadelt', u'milliardenkredit', u'erstmal', u'zahlungsfah', u'ddr', u'aufrechterhalt', u'interview', u'ard', u'abend', u'ausgestrahlt', u'schalck', u'angaben_des_senders', u'freies', u'sfb', u'damal', u'nichtsein', u'ddr', u'gegang', u'geword', u'damal', u'ddr', u'kooperation', u'bundesrepubl', u'uberleb', u'parteichef', u'erich_honecker', u'berat', u'konterrevolutionar', u'zuruckgewies', u'ablehn', u'sowjet', u'staatsprasident', u'michail_gorbatschows', u'beigetrag', u'lebenswicht', u'adern', u'sowjetunion', u'durchschnitt', u'angeregt', u'schalck', u'golodkowski', u'sfb', u'bedingungslos', u'syst', u'gedient', u'schalck', u'wies', u'interview', u'vorwurf', u'rahm', u'jahrzehntelang', u'leiter_der_abteilung', u'kommerziell', u'koordinier', u'koko', u'kriminell', u'tatig', u'begang', u'schalck', u'sold', u'staatssicherheitsdien', u'stand', u'nannt', u'flucht', u'angst', u'fuhrungsspitz', u'politburos', u'gehabt', u'fallengelass', u'kurze_zeit', u'selbstmord', u'entschloss', u'beitrag', u'vorab', u'redaktioneller_fassung', u'ubermittelt']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('stems_for_lda_train.pkl', 'rb') as f:\n",
    "    stems = pickle.load(f)\n",
    "    \n",
    "print stems[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split the data set into 10 equally-sized folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Instantiate a ShuffleSplit object.\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.10, random_state=0)\n",
    "\n",
    "# Apply the split method to the stemmed tokens and save the resulting indices. \n",
    "# These indices will be used to split the stemmed tokens into training and test datasets.\n",
    "indices = list(ss.split(stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two arrays `train` and `test`. Each of the 10 elements of the `train` array is a list of pre-processed documents that serve as a train set in the current iteration of the cross-validation algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize empty lists for training and test sets\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# Loop over all splits (total of 10 as defined in ShuffleSplit)\n",
    "for s in range(10):\n",
    "    # For each split, select the stems corresponding to the training indices (indices[s][0]) \n",
    "    train.append(np.array(stems)[indices[s][0]])\n",
    "    \n",
    "    # Similarly, select the stems corresponding to the test indices (indices[s][1]) \n",
    "    # and append this subset of stems to the test list.\n",
    "    test.append(np.array(stems)[indices[s][1]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cross-validation algorithm for K in $\\{10,50,100\\}$.\n",
    "\n",
    "In this notebook, we run a cross-validation algorithm for the number of topics (K) in the set $\\{10,50,100\\}$. To manage the computational requirements effectively and circumvent memory constraints, we split the execution of the cross-validation algorithm across two separate notebooks. This allows us to run each notebook on a different computer simultaneously, enhancing our computational efficiency. The other notebook is dedicated to running the algorithm for K in the set $\\{150, 200, 250\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 days, 11:47:45.370000\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Define the number of cores to use for parallel processing\n",
    "NUM_CORE = 30\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "# Import the module 'perplexity_fold'\n",
    "import perplexity_fold\n",
    "\n",
    "# Define the range of topic numbers to try in cross-validation\n",
    "k_values = [10, 50, 100]  \n",
    "\n",
    "# Create a list of tuples, each containing a train set, a test set, and a number of topics (k)\n",
    "train_test_data = [(train_subset, test_subset, k) for k in k_values for train_subset, test_subset in zip(train, test)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    \n",
    "    # Use the pool to apply the perplexity_fold function to every item in train_test_data, in parallel\n",
    "    perplexity_test = pool.map(perplexity_fold.perplexity_fold, train_test_data) \n",
    "    \n",
    "    # For each k value, compute the mean perplexity over all 10 splits\n",
    "    perplexity_k_10_100 = [np.mean(perplexity_test[i:i+10]) for i in range(0, len(perplexity_test), 10)]\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the calculated perplexity values for each number of topics k={10,50,100} into a CSV file\n",
    "np.savetxt(\"perplexity_k_10_100.csv\", perplexity_k_10_100, delimiter = \".\", fmt='%10.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Javascript\n",
    "display(Javascript('IPython.notebook.save_checkpoint();'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
