{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0cfcea",
   "metadata": {},
   "source": [
    "# Sentiment-Adjusted Topic Series (BPW)\n",
    "\n",
    "In this notebook, I construct sentiment-adjusted topic series. For sentiment extraction, I use the German translation by [Bannier, Pauls, and Walter (BPW)](https://link.springer.com/article/10.1007/s11573-018-0914-8) of the widely recognized dictionary by [Loughran and McDonald (2011)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.2010.01625.x). \n",
    "\n",
    "The process involves identifying the 10 articles with the highest proportion of each topic for each day. I then calculate the average sentiment measure for these articles, which serves as the sentiment value for the topic on that day. Finally, I adjust the daily topic values by multiplying them with the corresponding sentiment measure.\n",
    "\n",
    "To begin, I load the datasets from Handelsblatt, SZ, Welt, and dpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d2dd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3336299\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sperrfrist 1. Januar 1000 HBV fordert umfassen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schamir weist Nahost-Äußerungen des neuen EG-P...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  day  month  year\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...    1      1  1991\n",
       "1  Welajati: Iran bleibt bei einem Krieg am Golf ...    1      1  1991\n",
       "2  Bush will offenbar seinen Außenminister erneut...    1      1  1991\n",
       "3  Sperrfrist 1. Januar 1000 HBV fordert umfassen...    1      1  1991\n",
       "4  Schamir weist Nahost-Äußerungen des neuen EG-P...    1      1  1991"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Set the path variable to point to the 'newspaper_data_processing' directory.\n",
    "path = os.getcwd().replace('\\\\newspaper_analysis\\\\topics', '\\\\newspaper_data_processing')\n",
    "\n",
    "# Load pre-processed 'dpa' dataset from a CSV file.\n",
    "dpa = pd.read_csv(path + '\\\\dpa\\\\' + 'dpa_prepro_final.csv', encoding = 'utf-8', sep=';', index_col = 0,  keep_default_na=False,\n",
    "                   dtype = {'rubrics': 'str', \n",
    "                            'source': 'str',\n",
    "                            'keywords': 'str',\n",
    "                            'title': 'str',\n",
    "                            'city': 'str',\n",
    "                            'genre': 'str',\n",
    "                            'wordcount': 'str'},\n",
    "                  converters = {'paragraphs': literal_eval})\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "dpa = dpa[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'SZ' dataset from a CSV file.\n",
    "sz = pd.read_csv(path + '\\\\SZ\\\\' + 'sz_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'newspaper_2': 'str',\n",
    "                                                                                                 'quelle_texts': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "sz.page = sz.page.fillna('')\n",
    "sz.newspaper = sz.newspaper.fillna('')\n",
    "sz.newspaper_2 = sz.newspaper_2.fillna('')\n",
    "sz.rubrics = sz.rubrics.fillna('')\n",
    "sz.quelle_texts = sz.quelle_texts.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "sz = sz[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Handelsblatt' dataset from a CSV file.\n",
    "hb = pd.read_csv(path + '\\\\Handelsblatt\\\\' + 'hb_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'kicker': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'series_title': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "hb.page = hb.page.fillna('')\n",
    "hb.series_title = hb.series_title.fillna('')\n",
    "hb.kicker = hb.kicker.fillna('')\n",
    "hb.rubrics = hb.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "hb = hb[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Welt' dataset from a CSV file.\n",
    "welt = pd.read_csv(path + '\\\\Welt\\\\' + 'welt_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'rubrics': 'str',\n",
    "                                                                                                 'title': 'str'})\n",
    "welt.title = welt.title.fillna('')\n",
    "welt.rubrics = welt.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "welt = welt[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Concatenate the 'dpa', 'sz', 'hb', and 'welt' DataFrames into a single DataFrame 'data'.\n",
    "data = pd.concat([dpa, sz, hb, welt])\n",
    "\n",
    "# The number of articles in the final dataset.\n",
    "print(len(data))\n",
    "\n",
    "# Sort the data in chronological order.\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "# Reset the index of the DataFrame\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394952b7",
   "metadata": {},
   "source": [
    "Next, I import sentiment scores, previously computed using an LSTM model for each article in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4f5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "\n",
    "# Set the path variable to point to the 'sentiment' directory\n",
    "path = os.getcwd().replace('\\\\topics', '') + '\\\\sentiment'\n",
    "\n",
    "with codecs.open(path + \"\\\\scores_lstm.csv\", \"r\", encoding='utf-8-sig') as f:\n",
    "    reader = csv.reader(f)\n",
    "    scores = [None if row[0] == '' else float(row[0]) for row in reader]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ffc01",
   "metadata": {},
   "source": [
    "I add sentiment scores as a new column to the `data` DataFrame and discard any rows with missing sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5298d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the sentiment scores as a new column in the data DataFrame\n",
    "data['scores'] = scores\n",
    "\n",
    "# Remove any rows in the DataFrame where a sentiment score is missing (NaN). In this context, \n",
    "# NaN corresponds to the model's inability to predict sentiment for certain \n",
    "# articles due to formatting issues or because the article is too short (less than 20 tokens).\n",
    "data = data.dropna(subset=['scores'])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69b80d",
   "metadata": {},
   "source": [
    "Next, I calculate the sentiment measure for each article as the difference between the number of positive and negative words, divided by the total number of words in the article.\n",
    "\n",
    "To do this, I first load the BPW dictionary and create two lists: one containing negative terms and another containing positive terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b68382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbau', 'abbauen', 'abbauend', 'abbauende', 'abbauendem']\n",
      "['adäquat', 'adäquate', 'adäquatem', 'adäquaten', 'adäquater']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read an Excel file, transform an output into a list\n",
    "bpw_neg = list(pd.read_excel('BPW_Dictionary.xlsx', sheet_name='NEG_BPW', header=None).iloc[:,0]) \n",
    "bpw_pos = list(pd.read_excel('BPW_Dictionary.xlsx', sheet_name='POS_BPW', header=None).iloc[:,0])\n",
    "\n",
    "# Convert boolean value back to its intended string form\n",
    "bpw_neg = ['falsch' if word is False else word for word in bpw_neg]\n",
    "\n",
    "print(bpw_neg[:5])\n",
    "print(bpw_pos[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5abec",
   "metadata": {},
   "source": [
    "I then calculate the number of negative words in each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e278958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:10.295570\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import count_words_chunk\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Number of cores to use\n",
    "NUM_CORE = mp.cpu_count() - 4 \n",
    "\n",
    "# Split data into chunks for parallel processing\n",
    "chunk_size = len(data.texts) // NUM_CORE + 1 \n",
    "text_chunks = [data.texts[i:i + chunk_size] for i in range(0, len(data.texts), chunk_size)]\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "    # Process each chunk in parallel\n",
    "    nw_results = pool.starmap(count_words_chunk.count_words_chunk, [(chunk, bpw_neg) for chunk in text_chunks])\n",
    "\n",
    "    # Close and join the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine results from all chunks\n",
    "    negative_counts = np.concatenate(nw_results)\n",
    "\n",
    "print(datetime.now() - startTime)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766919f",
   "metadata": {},
   "source": [
    "I proceed to calculate the number of positive words in each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf15b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:09.139934\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "    # Process each chunk in parallel\n",
    "    pw_results = pool.starmap(count_words_chunk.count_words_chunk, [(chunk, bpw_pos) for chunk in text_chunks])\n",
    "\n",
    "    # Close and join the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine results from all chunks\n",
    "    positive_counts = np.concatenate(pw_results)\n",
    "\n",
    "print(datetime.now() - startTime)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d022bb3",
   "metadata": {},
   "source": [
    "The final statistic required for calculating sentiment is the total number of words in each article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286c7082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:18.499805\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() \n",
    "\n",
    "# Import the function calculating the number of words in a text\n",
    "import count_words_mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955718f9",
   "metadata": {},
   "source": [
    "Finally, I calculate the sentiment for each article as the proportion of positive words minus the proportion of negative words, relative to the total word count. This value is then added as a new column to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2aeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert count_results to a NumPy array for element-wise operations\n",
    "count_results = np.array(count_results)\n",
    "\n",
    "# Calculate sentiment: (positive_counts - negative_counts) / count_results\n",
    "sentiment_measure = np.divide(\n",
    "    positive_counts - negative_counts, \n",
    "    count_results, \n",
    "    out=np.zeros_like(negative_counts, dtype=float), \n",
    "    where=count_results != 0\n",
    ")\n",
    "\n",
    "# Add the sentiment to the DataFrame\n",
    "data['sentiment'] = sentiment_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9666cf",
   "metadata": {},
   "source": [
    "Now, I incorporate the topic distributions for each article, which were previously computed using the Latent Dirichlet Allocation (LDA) algorithm in the notebook titled `Topic Model Estimation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b0fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the article topics from a CSV file\n",
    "article_topics = pd.read_csv('article_topic.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "# Merge the `data` DataFrame with the `article_topics` DataFrame\n",
    "data = pd.concat([data, article_topics], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30adb1",
   "metadata": {},
   "source": [
    "I define a function, `get_average_sentiment`, which calculates the average sentiment measure for each topic on a given day. This function selects a specified number of articles with the highest proportions for each topic and computes the average sentiment across these articles. The `calculate_average_sentiment` function applies this calculation to the entire dataset by grouping the data by date and processing each group in parallel using multiprocessing. The results are then combined into a single pandas DataFrame, with the topics as columns and the dates as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "602ec4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_average_sentiment_BPW import get_average_sentiment\n",
    "\n",
    "# Convert 'year', 'month', 'day' to datetime\n",
    "data['date'] = pd.to_datetime(data[['year', 'month', 'day']])\n",
    "\n",
    "def calculate_average_sentiment(data, n_articles):\n",
    "    \"\"\"\n",
    "    Function to calculate average sentiment for topics based on a specified number of articles.\n",
    "    \"\"\"\n",
    "    # Group data by 'date' to ensure each day stays together\n",
    "    grouped_by_date = [group for _, group in data.groupby('date')]\n",
    "    \n",
    "    # Prepare arguments for starmap (pair each group with the value of n_articles)\n",
    "    args = [(group, n_articles) for group in grouped_by_date]\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        # Create a multiprocessing pool\n",
    "        pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "        # Process each group (one day of data) in parallel\n",
    "        results = pool.starmap(get_average_sentiment, args)\n",
    "\n",
    "        # Concatenate the results into a single DataFrame\n",
    "        daily_average_sentiment = pd.concat(results)\n",
    "\n",
    "        # Close and join the pool\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    return daily_average_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c2eb792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:13.451334\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() \n",
    "\n",
    "# Generate average sentiment for 10 articles\n",
    "daily_average_sentiment_10 = calculate_average_sentiment(data, n_articles=10)\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cda1b8",
   "metadata": {},
   "source": [
    "Now I am going to load the daily topics and adjust them using the DataFrame `daily_average_sentiment_10`. The result is a dataframe where topic distributions are multiplied with the average sentiment of each topic for a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78c1c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the daily topics from a CSV file\n",
    "daily_topics = pd.read_csv('daily_topics.csv', encoding='utf-8')\n",
    "\n",
    "# Convert year, month, and day into a single date column\n",
    "daily_topics['date'] = pd.to_datetime(daily_topics[['year','month','day']])\n",
    "daily_topics.drop(columns=['year', 'month', 'day'], inplace=True)\n",
    "\n",
    "# Now, set 'date' as index\n",
    "daily_topics.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac1e7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment adjustment to the daily topics\n",
    "sentiment_adjusted_daily_topics = daily_topics.multiply(daily_average_sentiment_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472e6d7",
   "metadata": {},
   "source": [
    "I iterate over each topic and generate a graph comparing the original and sentiment-adjusted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c72ca519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "\n",
    "# Create a directory to save the plots\n",
    "os.makedirs('topics_BPW_plots', exist_ok=True)\n",
    "\n",
    "# Define the shaded areas for recessions\n",
    "recessions = [\n",
    "    (\"1992-01-01\", \"1993-12-31\"),  # Post-reunification recession\n",
    "    (\"2001-01-01\", \"2001-12-31\"),  # Dot-com recession\n",
    "    (\"2008-01-01\", \"2009-12-31\"),  # Great Recession\n",
    "    (\"2011-01-01\", \"2013-12-31\")   # European sovereign debt crisis\n",
    "]\n",
    "\n",
    "# Calculate the 180-day rolling mean for each series\n",
    "daily_topics_rm = daily_topics.rolling(window=180).mean()\n",
    "sentiment_adjusted_daily_topics_10_rm = sentiment_adjusted_daily_topics.rolling(window=180).mean()\n",
    "\n",
    "# Iterate over each topic\n",
    "for i in range(daily_topics.shape[1]):\n",
    "    # Generate the plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original topics on the primary y-axis\n",
    "    ax1.plot(daily_topics_rm.index, daily_topics_rm.iloc[:, i], label='Original Topic', color='black')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Original Topic Proportion', color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Add shaded areas for recessions\n",
    "    for start, end in recessions:\n",
    "        ax1.axvspan(pd.to_datetime(start), pd.to_datetime(end), color='grey', alpha=0.3)\n",
    "    \n",
    "    # Create a secondary y-axis for the sentiment-adjusted topics\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(sentiment_adjusted_daily_topics_10_rm.index, sentiment_adjusted_daily_topics_10_rm.iloc[:, i], label='Sentiment-Adjusted Topic', linestyle='--')\n",
    "    ax2.set_ylabel('Sentiment-Adjusted Topic')\n",
    "    ax2.tick_params(axis='y')\n",
    "    \n",
    "    # Add title and legends\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Format the x-axis to show every year\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Save the plot in the 'topics_plots' directory\n",
    "    plt.savefig('topics_BPW_plots/Topic_' + str(i) + '.png')\n",
    "    \n",
    "    # Clear the current figure to free memory\n",
    "    plt.clf()\n",
    "    \n",
    "    # Close the current figure to free memory\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49a48cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "sentiment_adjusted_daily_topics = sentiment_adjusted_daily_topics.reset_index()\n",
    "\n",
    "# Create 'year', 'month', and 'day' columns\n",
    "sentiment_adjusted_daily_topics['year'] = sentiment_adjusted_daily_topics['date'].dt.year\n",
    "sentiment_adjusted_daily_topics['month'] = sentiment_adjusted_daily_topics['date'].dt.month\n",
    "sentiment_adjusted_daily_topics['day'] = sentiment_adjusted_daily_topics['date'].dt.day\n",
    "\n",
    "# Drop the old 'index' column which holds the date\n",
    "sentiment_adjusted_daily_topics = sentiment_adjusted_daily_topics.drop(columns=['date'])\n",
    "\n",
    "# Reorder the columns to have 'year', 'month', 'day' as the first three columns\n",
    "cols = ['year', 'month', 'day'] + [col for col in sentiment_adjusted_daily_topics if col not in ['year', 'month', 'day']]\n",
    "sentiment_adjusted_daily_topics_format = sentiment_adjusted_daily_topics[cols]\n",
    "\n",
    "# Save sentiment-adjusted topics to a CSV file\n",
    "sentiment_adjusted_daily_topics_format.to_csv('BPW_adjusted_daily_topics.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7752fa",
   "metadata": {},
   "source": [
    "Finally, I load the BCC-adjusted topics, compute their 180-day rolling mean and then, for each of the selected topics, plot the original topic, the BPW-adjusted and the BCC-adjusted series together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b02535fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BCC-adjusted topics\n",
    "bcc = pd.read_csv('sign_adjusted_daily_topics_format.csv', encoding='utf-8')\n",
    "# Reconstruct date\n",
    "bcc['date'] = pd.to_datetime(bcc[['year','month','day']])\n",
    "bcc = bcc.set_index('date').drop(columns=['year','month','day'])\n",
    "\n",
    "# 180-day rolling means\n",
    "bcc_rm = bcc.rolling(window=180).mean()\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs('selected_topics_plots_BPW_BCC', exist_ok=True)\n",
    "\n",
    "# Recessions shading\n",
    "recessions = [\n",
    "    (\"1992-01-01\", \"1993-12-31\"),\n",
    "    (\"2001-01-01\", \"2001-12-31\"),\n",
    "    (\"2008-01-01\", \"2009-12-31\"),\n",
    "    (\"2011-01-01\", \"2013-12-31\")\n",
    "]\n",
    "\n",
    "# Selected topics\n",
    "selected_topics = [11, 27, 52, 127, 81, 77, 74, 131, 138, 100]\n",
    "\n",
    "for idx in selected_topics:\n",
    "    fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "    \n",
    "    # Plot both original and sign-adjusted topic on left axis\n",
    "    line1, = ax1.plot(daily_topics_rm.index, daily_topics_rm.iloc[:, idx], label='Original topic', color='black')\n",
    "    line2, = ax1.plot(bcc_rm.index, bcc_rm.iloc[:, idx], label='Sign-adjusted topic (BCC)', color='tab:blue')\n",
    "    \n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Topic Proportion', color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Add shaded areas for recessions\n",
    "    for start, end in recessions:\n",
    "        ax1.axvspan(pd.to_datetime(start), pd.to_datetime(end), color='grey', alpha=0.3)\n",
    "    \n",
    "    # Secondary axis for sentiment‐adjusted series (BPW)\n",
    "    ax2 = ax1.twinx()\n",
    "    line3, = ax2.plot(sentiment_adjusted_daily_topics_10_rm.index, sentiment_adjusted_daily_topics_10_rm.iloc[:, idx],\n",
    "        label='Sentiment-adjusted topic (BPW)',\n",
    "        color='tab:orange'\n",
    "    )\n",
    "    ax2.set_ylabel('Sentiment-Adjusted Topic')\n",
    "    ax2.tick_params(axis='y')\n",
    "    \n",
    "    # Legend\n",
    "    ax1.legend(handles=[line1, line2, line3], loc='upper right')\n",
    "    \n",
    "    # X-axis formatting\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Save\n",
    "    fig.savefig(f'selected_topics_plots_BPW_BCC/Topic_{idx}.png',\n",
    "                bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env_gpu",
   "language": "python",
   "name": "py3_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
