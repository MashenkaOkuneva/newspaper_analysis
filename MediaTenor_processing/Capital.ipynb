{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a27c014",
   "metadata": {},
   "source": [
    "In this notebook, we perform the following steps:\n",
    "\n",
    "1. **Load and Match Articles from Factiva**: \n",
    "   - We begin by downloading 198 RTF files of Capital articles from Factiva. These RTF files are then converted to TXT format. After the conversion, we load the articles from the TXT files and match them with their metadata.\n",
    "\n",
    "2. **Load and Match Articles from LexisNexis**: \n",
    "   - Similarly, we download 164 RTF files of Capital articles from LexisNexis. These RTF files are converted to TXT format, and the articles are then loaded and matched with their sentiment annotations.\n",
    "\n",
    "3. **Combine All Articles**: \n",
    "   - Finally, we combine all these articles into one dataset and save it as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc00e",
   "metadata": {},
   "source": [
    "## Media Tenor dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d800b6",
   "metadata": {},
   "source": [
    "To match the articles downloaded from Factiva and LexisNexis with their metadata from the Media Tenor dataset, we first need to load the Media Tenor dataset. We only retain articles with non-empty titles, as it is not possible to identify and download articles without titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008d2d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2014</td>\n",
       "      <td>201401</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Koalition</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>FAS</td>\n",
       "      <td>Habt bloß keine Angst vor China !</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Wir leben in einer Zeit der Wohlstands-Halluzi...</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.02.2015</td>\n",
       "      <td>201502</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Teheran ruft</td>\n",
       "      <td>Wettbewerbsfähigkeit/Nachfrage</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Geht es und wirklich so gut, wie es uns Merkel...</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month medium  \\\n",
       "0  01.01.2014  201401   WamS   \n",
       "1  01.01.2017  201701    FAS   \n",
       "2  01.01.2017  201701   BamS   \n",
       "3  01.02.2015  201502   WamS   \n",
       "4  01.01.2017  201701   BamS   \n",
       "\n",
       "                                               title  \\\n",
       "0                                          Koalition   \n",
       "1                  Habt bloß keine Angst vor China !   \n",
       "2  Wir leben in einer Zeit der Wohlstands-Halluzi...   \n",
       "3                                       Teheran ruft   \n",
       "4  Geht es und wirklich so gut, wie es uns Merkel...   \n",
       "\n",
       "                       topicgroup  negative  no_clear_tone  positive  \\\n",
       "0                      Konjunktur         0              1         0   \n",
       "1       Internationale Wirtschaft         0              0         1   \n",
       "2                      Konjunktur         0              0         1   \n",
       "3  Wettbewerbsfähigkeit/Nachfrage         1              3         0   \n",
       "4       Internationale Wirtschaft         0              1         0   \n",
       "\n",
       "   Number_of_reports AverageRating  \n",
       "0                  1             0  \n",
       "1                  1           100  \n",
       "2                  1           100  \n",
       "3                  4           -25  \n",
       "4                  1             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset acquired from Media Tenor\n",
    "sentiment_data = pd.read_csv('Daten_Wirtschaftliche_Lage.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "# Filter out rows with empty titles, as we cannot identify and download the articles without titles\n",
    "sentiment_data = sentiment_data[sentiment_data['title'].notnull()]\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569cec3b",
   "metadata": {},
   "source": [
    "The titles in the Media Tenor dataset were manually entered, leading to potential inconsistencies in punctuation and spacing. To address this issue and ensure accurate matching with the titles of the articles we download from databases, we normalize the titles in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e56319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2014</td>\n",
       "      <td>201401</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Koalition</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>koalition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>FAS</td>\n",
       "      <td>Habt bloß keine Angst vor China !</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>habt bloß keine angst vor china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Wir leben in einer Zeit der Wohlstands-Halluzi...</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>wir leben in einer zeit der wohlstands halluzi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.02.2015</td>\n",
       "      <td>201502</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Teheran ruft</td>\n",
       "      <td>Wettbewerbsfähigkeit/Nachfrage</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-25</td>\n",
       "      <td>teheran ruft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Geht es und wirklich so gut, wie es uns Merkel...</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>geht es und wirklich so gut wie es uns merkel ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month medium  \\\n",
       "0  01.01.2014  201401   WamS   \n",
       "1  01.01.2017  201701    FAS   \n",
       "2  01.01.2017  201701   BamS   \n",
       "3  01.02.2015  201502   WamS   \n",
       "4  01.01.2017  201701   BamS   \n",
       "\n",
       "                                               title  \\\n",
       "0                                          Koalition   \n",
       "1                  Habt bloß keine Angst vor China !   \n",
       "2  Wir leben in einer Zeit der Wohlstands-Halluzi...   \n",
       "3                                       Teheran ruft   \n",
       "4  Geht es und wirklich so gut, wie es uns Merkel...   \n",
       "\n",
       "                       topicgroup  negative  no_clear_tone  positive  \\\n",
       "0                      Konjunktur         0              1         0   \n",
       "1       Internationale Wirtschaft         0              0         1   \n",
       "2                      Konjunktur         0              0         1   \n",
       "3  Wettbewerbsfähigkeit/Nachfrage         1              3         0   \n",
       "4       Internationale Wirtschaft         0              1         0   \n",
       "\n",
       "   Number_of_reports AverageRating  \\\n",
       "0                  1             0   \n",
       "1                  1           100   \n",
       "2                  1           100   \n",
       "3                  4           -25   \n",
       "4                  1             0   \n",
       "\n",
       "                                         title_clean  \n",
       "0                                          koalition  \n",
       "1                    habt bloß keine angst vor china  \n",
       "2  wir leben in einer zeit der wohlstands halluzi...  \n",
       "3                                       teheran ruft  \n",
       "4  geht es und wirklich so gut wie es uns merkel ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Normalize class from the normalize module\n",
    "from normalize import Normalize\n",
    "\n",
    "# Initialize the Normalize class with the titles from the 'sentiment_data' DataFrame\n",
    "normalizer = Normalize(sentiment_data.title)\n",
    "\n",
    "# Apply the normalization process to the titles\n",
    "normalized_titles = normalizer.normalized()\n",
    "\n",
    "# Add the normalized titles to the sentiment_data DataFrame as a new column 'title_clean'\n",
    "sentiment_data['title_clean'] = normalized_titles\n",
    "\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a366e41",
   "metadata": {},
   "source": [
    "We need to focus on annotated articles from Capital related to business cycle conditions, as these are the specific articles we downloaded from the databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7289274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include only articles from Capital\n",
    "sentiment_data = sentiment_data[sentiment_data['medium'] == 'Capital']\n",
    "\n",
    "# Reset the index of the DataFrame and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Further filter the dataset to include only articles related to the business cycle conditions (Konjunktur)\n",
    "sentiment_data = sentiment_data[sentiment_data['topicgroup'] == 'Konjunktur']\n",
    "\n",
    "# Reset the index of the DataFrame again and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe656d",
   "metadata": {},
   "source": [
    "We filter the Media Tenor dataset to only keep articles where there was agreement between annotators on sentiment. Articles without annotator agreement (i.e., where `sentiment` is `NaN`) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67cc6cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.12.2017</td>\n",
       "      <td>201712</td>\n",
       "      <td>Capital</td>\n",
       "      <td>Man kann mit großen Hoffnungen</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>man kann mit großen hoffnungen</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.12.2017</td>\n",
       "      <td>201712</td>\n",
       "      <td>Capital</td>\n",
       "      <td>SPONSOREN MEIDEN FIFA</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-100</td>\n",
       "      <td>sponsoren meiden fifa</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.12.2017</td>\n",
       "      <td>201712</td>\n",
       "      <td>Capital</td>\n",
       "      <td>ZUKUNFT IST VERGANGENHEIT</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>zukunft ist vergangenheit</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.02.2018</td>\n",
       "      <td>201802</td>\n",
       "      <td>Capital</td>\n",
       "      <td>JOES TAUSCHBÖRSE</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>joes tauschbörse</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.02.2018</td>\n",
       "      <td>201802</td>\n",
       "      <td>Capital</td>\n",
       "      <td>Notenbanker sind Gefangene der Inflation</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>-50</td>\n",
       "      <td>notenbanker sind gefangene der inflation</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month   medium                                     title  \\\n",
       "0  14.12.2017  201712  Capital            Man kann mit großen Hoffnungen   \n",
       "1  14.12.2017  201712  Capital                     SPONSOREN MEIDEN FIFA   \n",
       "2  14.12.2017  201712  Capital                 ZUKUNFT IST VERGANGENHEIT   \n",
       "3  15.02.2018  201802  Capital                          JOES TAUSCHBÖRSE   \n",
       "4  15.02.2018  201802  Capital  Notenbanker sind Gefangene der Inflation   \n",
       "\n",
       "   topicgroup  negative  no_clear_tone  positive  Number_of_reports  \\\n",
       "0  Konjunktur         0              1         0                  1   \n",
       "1  Konjunktur         1              0         0                  1   \n",
       "2  Konjunktur         0              0         4                  4   \n",
       "3  Konjunktur         0              0         1                  1   \n",
       "4  Konjunktur         4              1         1                  6   \n",
       "\n",
       "  AverageRating                               title_clean  sentiment  \n",
       "0             0            man kann mit großen hoffnungen        0.0  \n",
       "1          -100                     sponsoren meiden fifa       -1.0  \n",
       "2           100                 zukunft ist vergangenheit        1.0  \n",
       "3           100                          joes tauschbörse        1.0  \n",
       "4           -50  notenbanker sind gefangene der inflation       -1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentiment import sentiment\n",
    "\n",
    "# Apply the 'sentiment' function to each row of the DataFrame and create a new 'sentiment' column\n",
    "sentiment_data['sentiment'] = sentiment_data.apply(lambda row: sentiment(row), axis=1)\n",
    "\n",
    "# Remove articles where there is no annotator agreement (i.e., sentiment is NaN)\n",
    "sentiment_data = sentiment_data[sentiment_data['sentiment'].notnull()]\n",
    "\n",
    "# Reset the index of the DataFrame again and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame to verify the results\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9a370",
   "metadata": {},
   "source": [
    "## Load and Match Articles from Factiva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05bd38f",
   "metadata": {},
   "source": [
    "Next, we focus on loading Capital articles downloaded from Factiva and matching them with their metadata. In our first step, we convert the RTF files into TXT format. All the RTF files are stored in `MediaTenor_LexisNexis_Factiva/Capital_Konjunktur_Factiva_rtf`. The converted TXT files are stored in `MediaTenor_LexisNexis_Factiva/Capital_Konjunktur_Factiva_txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141ef7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Import the function for converting RTF to TXT\n",
    "from convert_rtf_to_txt import convert_rtf_to_txt\n",
    "\n",
    "# Define paths for Capital RTF and TXT directories\n",
    "capital_rtf_path = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Capital_Konjunktur_Factiva_rtf')\n",
    "capital_txt_path = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Capital_Konjunktur_Factiva_txt')\n",
    "\n",
    "# Convert RTF files to TXT format for Capital\n",
    "convert_rtf_to_txt(capital_rtf_path, capital_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541dea5",
   "metadata": {},
   "source": [
    "As soon as the RTF files were transformed into TXT format, we made a few changes to the TXT files. Specifically, we corrected several titles to ensure accurate spelling and punctuation, which is important for matching them with the metadata from the Media Tenor dataset. For example, a title \"Gut **möglich ,**\" was corrected to \"Gut **möglich,**\".\n",
    "\n",
    "Additionally, a few articles were compilations of multiple pieces. In such cases, we manually selected the annotated article from the compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02895d09",
   "metadata": {},
   "source": [
    "Once the TXT files were ready, we used the function `extract_article_data_capital_factiva` to load the text of the articles along with the journal's name, date of publication, title, and file name into a dictionary called `article_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eaf4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_article_data_capital_factiva\n",
    "\n",
    "# Read and extract relevant information from TXT files in Capital directory.\n",
    "article_data = extract_article_data_capital_factiva.extract_article_data_capital_factiva(capital_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893d671",
   "metadata": {},
   "source": [
    "We use the `article_data` dictionary to create a DataFrame `capital_factiva` that includes columns for the journal's name, publication date (day, month, and year), article title, text, and file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898528a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capital</td>\n",
       "      <td>20</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2020</td>\n",
       "      <td>LASS ANGELN GEHEN</td>\n",
       "      <td>Aktien Trotz schlechter Wirtschaftsdaten feier...</td>\n",
       "      <td>Factiva-20200811-1029.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capital</td>\n",
       "      <td>20</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2020</td>\n",
       "      <td>\" Frau Wagenknecht ,  wird das eine Krise des ...</td>\n",
       "      <td>Der Neustart \"Bisher hat der Kapitalismus all ...</td>\n",
       "      <td>Factiva-20200811-1034.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capital</td>\n",
       "      <td>20</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2020</td>\n",
       "      <td>\" UNSICHERHEIT IM QUADRAT \"</td>\n",
       "      <td>Interview Der US-Starinvestor und Chefberater ...</td>\n",
       "      <td>Factiva-20200811-1035 (1).txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capital</td>\n",
       "      <td>20</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2020</td>\n",
       "      <td>BENKOS BEBEN</td>\n",
       "      <td>Signa Lange ging es für Warenhauskönig René Be...</td>\n",
       "      <td>Factiva-20200811-1035.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capital</td>\n",
       "      <td>20</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2020</td>\n",
       "      <td>WESTERN VON GESTERN</td>\n",
       "      <td>Die Wirtschaft ist voller Skandale, Fehden und...</td>\n",
       "      <td>Factiva-20200811-1038.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   journal day month  year                                              title  \\\n",
       "0  Capital  20   Mai  2020                                  LASS ANGELN GEHEN   \n",
       "1  Capital  20   Mai  2020  \" Frau Wagenknecht ,  wird das eine Krise des ...   \n",
       "2  Capital  20   Mai  2020                        \" UNSICHERHEIT IM QUADRAT \"   \n",
       "3  Capital  20   Mai  2020                                       BENKOS BEBEN   \n",
       "4  Capital  20   Mai  2020                                WESTERN VON GESTERN   \n",
       "\n",
       "                                                text  \\\n",
       "0  Aktien Trotz schlechter Wirtschaftsdaten feier...   \n",
       "1  Der Neustart \"Bisher hat der Kapitalismus all ...   \n",
       "2  Interview Der US-Starinvestor und Chefberater ...   \n",
       "3  Signa Lange ging es für Warenhauskönig René Be...   \n",
       "4  Die Wirtschaft ist voller Skandale, Fehden und...   \n",
       "\n",
       "                            file  \n",
       "0      Factiva-20200811-1029.txt  \n",
       "1      Factiva-20200811-1034.txt  \n",
       "2  Factiva-20200811-1035 (1).txt  \n",
       "3      Factiva-20200811-1035.txt  \n",
       "4      Factiva-20200811-1038.txt  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the collected data\n",
    "capital_factiva = pd.DataFrame({\n",
    "    'journal': article_data['journal'],\n",
    "    'day': article_data['day'],\n",
    "    'month': article_data['month'],\n",
    "    'year': article_data['year'],\n",
    "    'title': article_data['title'],\n",
    "    'text': article_data['text'],\n",
    "    'file': article_data['file']\n",
    "})\n",
    "\n",
    "capital_factiva.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7cf3d",
   "metadata": {},
   "source": [
    "To match the full texts of the loaded articles with their sentiment annotations from the Media Tenor dataset, we follow several key steps. First, we create a date in the same format as in the `sentiment_data` DataFrame. Next, we normalize the titles to ensure accurate matching. We also remove any duplicate articles that were mistakenly downloaded twice. After pre-processing, we merge the articles loaded from Factiva with their sentiment annotations from the Media Tenor dataset. We then sort the final DataFrame `data_match_factiva` in chronological order and retain only the relevant columns. Through this process, we successfully matched **198** Capital articles from Factiva with their sentiment annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465e9225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles from Factiva: 198\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>file</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capital</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>LASST UNS DIE ZUKUNFT BAUEN !</td>\n",
       "      <td>Konjunktur Die Deutschen genießen ihren Aufsch...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Factiva-20200811-1348 (2).txt</td>\n",
       "      <td>lasst uns die zukunft bauen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capital</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>HINTERTÜR IM FERNEN OSTEN</td>\n",
       "      <td>Unsere Sanktionen gegen Russland werden wirkun...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Factiva-20200811-1349.txt</td>\n",
       "      <td>hintertür im fernen osten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capital</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>Gute Sparer ,  schlechte Anleger</td>\n",
       "      <td>Wenn es um Vermögensbildung geht, sind die Deu...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Factiva-20200811-1351.txt</td>\n",
       "      <td>gute sparer schlechte anleger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capital</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>\" Im Osten muss sich das Unternehmertum erst w...</td>\n",
       "      <td>Deutsche Einheit Kann man die Lehren aus der d...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Factiva-20200811-1344 (2).txt</td>\n",
       "      <td>im osten muss sich das unternehmertum erst wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capital</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>WIRTSCHAFTSKARTE</td>\n",
       "      <td>KONJUNKTUR RUND UM DEN GLOBUS Nach einem guten...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Factiva-20200811-1345.txt</td>\n",
       "      <td>wirtschaftskarte</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   journal  day  month  year  \\\n",
       "0  Capital   18      9  2014   \n",
       "1  Capital   18      9  2014   \n",
       "2  Capital   18      9  2014   \n",
       "3  Capital   23     10  2014   \n",
       "4  Capital   23     10  2014   \n",
       "\n",
       "                                               title  \\\n",
       "0                      LASST UNS DIE ZUKUNFT BAUEN !   \n",
       "1                          HINTERTÜR IM FERNEN OSTEN   \n",
       "2                   Gute Sparer ,  schlechte Anleger   \n",
       "3  \" Im Osten muss sich das Unternehmertum erst w...   \n",
       "4                                   WIRTSCHAFTSKARTE   \n",
       "\n",
       "                                                text  sentiment  \\\n",
       "0  Konjunktur Die Deutschen genießen ihren Aufsch...       -1.0   \n",
       "1  Unsere Sanktionen gegen Russland werden wirkun...        1.0   \n",
       "2  Wenn es um Vermögensbildung geht, sind die Deu...       -1.0   \n",
       "3  Deutsche Einheit Kann man die Lehren aus der d...       -1.0   \n",
       "4  KONJUNKTUR RUND UM DEN GLOBUS Nach einem guten...       -1.0   \n",
       "\n",
       "                            file  \\\n",
       "0  Factiva-20200811-1348 (2).txt   \n",
       "1      Factiva-20200811-1349.txt   \n",
       "2      Factiva-20200811-1351.txt   \n",
       "3  Factiva-20200811-1344 (2).txt   \n",
       "4      Factiva-20200811-1345.txt   \n",
       "\n",
       "                                         title_clean  \n",
       "0                        lasst uns die zukunft bauen  \n",
       "1                          hintertür im fernen osten  \n",
       "2                      gute sparer schlechte anleger  \n",
       "3  im osten muss sich das unternehmertum erst wie...  \n",
       "4                                   wirtschaftskarte  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary to transform month name into month number\n",
    "name_to_number = {\n",
    "    u'Januar': '01', u'Februar': '02', u'M\\xe4rz': '03', u'April': '04', u'Mai': '05',\n",
    "    u'Juni': '06', u'Juli': '07', u'August': '08', u'September': '09', u'Oktober': '10',\n",
    "    u'November': '11', u'Dezember': '12'\n",
    "}\n",
    "\n",
    "# Transform month names into month numbers\n",
    "capital_factiva['month_num'] = capital_factiva['month'].map(name_to_number)\n",
    "\n",
    "# Create dictionary to transform single-digit day numbers\n",
    "day_transform = {u'1': '01', u'2': '02', u'3': '03', u'4': '04', u'5': '05', u'6': '06', u'7': '07', u'8': '08', u'9': '09'}\n",
    "\n",
    "# Transform single-digit day numbers into two-digit format\n",
    "capital_factiva['day'] = capital_factiva['day'].map(lambda d: day_transform.get(d, d))\n",
    "\n",
    "# Combine day, month, and year into a date string\n",
    "capital_factiva['date'] = capital_factiva.apply(lambda row: f\"{row['day']}.{row['month_num']}.{row['year']}\", axis=1)\n",
    "\n",
    "# Drop duplicated articles, keeping the first occurrence\n",
    "# We have duplicates because we mistakenly downloaded the same article twice\n",
    "capital_factiva = capital_factiva.drop_duplicates(['text', 'year', 'month', 'day'], keep='first')\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "capital_factiva = capital_factiva.reset_index(drop=True)\n",
    "\n",
    "# Initialize the Normalize class with the titles from the 'capital_factiva' DataFrame\n",
    "normalizer = Normalize(capital_factiva.title)\n",
    "\n",
    "# Apply the normalization process to the titles\n",
    "normalized_titles = normalizer.normalized()\n",
    "\n",
    "# Add the normalized titles to the 'capital_factiva' DataFrame as a new column 'title_clean'\n",
    "capital_factiva['title_clean'] = normalized_titles\n",
    "\n",
    "# Merge with sentiment_data on title_clean and date\n",
    "data_match_factiva = pd.merge(sentiment_data, capital_factiva, how='inner', on=['title_clean', 'date'])\n",
    "\n",
    "# Rename the 'month_num' column to 'month'\n",
    "data_match_factiva = data_match_factiva.rename(columns={'month_num': 'month'})\n",
    "\n",
    "# Rename the 'year_y' column to 'year'\n",
    "data_match_factiva = data_match_factiva.rename(columns={'year_y': 'year'})\n",
    "\n",
    "# Convert year, month, and day to integers\n",
    "data_match_factiva['year'] = data_match_factiva['year'].astype(int)\n",
    "data_match_factiva['month'] = data_match_factiva['month'].astype(int)\n",
    "data_match_factiva['day'] = data_match_factiva['day'].astype(int)\n",
    "\n",
    "# Sort the data in chronological order\n",
    "data_match_factiva = data_match_factiva.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "data_match_factiva = data_match_factiva.reset_index(drop=True)\n",
    "\n",
    "# Rename the 'title_y' column to 'title' to reflect the title from the Factiva dataset\n",
    "data_match_factiva = data_match_factiva.rename(columns={'title_y': 'title'})\n",
    "\n",
    "# Select only the required columns\n",
    "data_match_factiva = data_match_factiva[['journal', 'day', 'month', 'year', 'title', 'text', 'sentiment', 'file', 'title_clean']]\n",
    "\n",
    "# Print the number of articles from Factiva\n",
    "num_factiva_articles = len(data_match_factiva)\n",
    "\n",
    "print(f\"Number of articles from Factiva: {num_factiva_articles}\")\n",
    "\n",
    "# Display the first few rows of the final matched dataset\n",
    "data_match_factiva.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8debf2",
   "metadata": {},
   "source": [
    "## Load and Match Articles from LexisNexis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c100a",
   "metadata": {},
   "source": [
    "In this section, we aim to load articles from Capital that were downloaded from LexisNexis and match them with their sentiment annotations. We begin by converting the RTF files into TXT format. The original RTF files are located in `MediaTenor_LexisNexis_Factiva/Capital_Konjunktur_LexisNexis_rtf`, and the resulting TXT files are stored in `MediaTenor_LexisNexis_Factiva/Capital_Konjunktur_LexisNexis_txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc7192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for Capital RTF and TXT directories\n",
    "capital_lexisnexis_rtf_path = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Capital_Konjunktur_LexisNexis_rtf')\n",
    "capital_lexisnexis_txt_path = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Capital_Konjunktur_LexisNexis_txt')\n",
    "\n",
    "# Convert RTF files to TXT format for Capital\n",
    "convert_rtf_to_txt(capital_lexisnexis_rtf_path, capital_lexisnexis_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8bc119",
   "metadata": {},
   "source": [
    "Once the RTF files were converted to TXT format, we made some adjustments. Specifically, we corrected a few lead-ins in the articles due to punctuation issues. For example, a lead-in starting from \"**Eon/RWERWE-Chef** Peter Terium verteidigt\" was corrected to \"**Eon/RWE. RWE-Chef** Peter Terium verteidigt\" to ensure accurate formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5af252",
   "metadata": {},
   "source": [
    "After preparing the TXT files, we used the `extract_article_data_capital_lexisnexis` function to load the articles' text, along with the journal name, publication date, title, and file name, into a dictionary called a`article_data_lexisnexis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3020ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_article_data_capital_lexisnexis\n",
    "\n",
    "# Read and extract relevant information from TXT files in Capital directory.\n",
    "article_data_lexisnexis = extract_article_data_capital_lexisnexis.extract_article_data_capital_lexisnexis(capital_lexisnexis_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e4484",
   "metadata": {},
   "source": [
    "We use the `article_data_lexisnexis` dictionary to create a DataFrame `capital_lexisnexis` that includes columns for the journal's name, publication date (day, month, and year), article title, text, and file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bccb7887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capital</td>\n",
       "      <td>24</td>\n",
       "      <td>October</td>\n",
       "      <td>2013</td>\n",
       "      <td>\"20 Rivalen\"</td>\n",
       "      <td>G20. \"Wir verpflichten uns zum automatischen D...</td>\n",
       "      <td>20 RIVALEN. EIN JAHR. EIN SATZ. EINE SENSATION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capital</td>\n",
       "      <td>18</td>\n",
       "      <td>June</td>\n",
       "      <td>2014</td>\n",
       "      <td>\"Hier geht's ab!\"</td>\n",
       "      <td>DYNAMISCHSTE STÄDTE. Die Wirtschaft brummt in ...</td>\n",
       "      <td>23 Hier geht_s ab_.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capital</td>\n",
       "      <td>18</td>\n",
       "      <td>June</td>\n",
       "      <td>2014</td>\n",
       "      <td>\"Mister 14 Prozent\"</td>\n",
       "      <td>BESTER INVESTOR. Frank Hansen ist Spezialist f...</td>\n",
       "      <td>27 MISTER 14 PROZENT.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capital</td>\n",
       "      <td>15</td>\n",
       "      <td>December</td>\n",
       "      <td>2011</td>\n",
       "      <td>\"Abgesoffen\"</td>\n",
       "      <td>Computerindustrie. Weltweit explodieren die Pr...</td>\n",
       "      <td>Abgesoffen.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capital</td>\n",
       "      <td>19</td>\n",
       "      <td>July</td>\n",
       "      <td>2012</td>\n",
       "      <td>\"Aktionsplan Schiene\"</td>\n",
       "      <td>Deutsche Bahn. Der Konzern verdient mit der Be...</td>\n",
       "      <td>Aktionsplan Schiene.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   journal day     month  year                  title  \\\n",
       "0  Capital  24   October  2013           \"20 Rivalen\"   \n",
       "1  Capital  18      June  2014      \"Hier geht's ab!\"   \n",
       "2  Capital  18      June  2014    \"Mister 14 Prozent\"   \n",
       "3  Capital  15  December  2011           \"Abgesoffen\"   \n",
       "4  Capital  19      July  2012  \"Aktionsplan Schiene\"   \n",
       "\n",
       "                                                text  \\\n",
       "0  G20. \"Wir verpflichten uns zum automatischen D...   \n",
       "1  DYNAMISCHSTE STÄDTE. Die Wirtschaft brummt in ...   \n",
       "2  BESTER INVESTOR. Frank Hansen ist Spezialist f...   \n",
       "3  Computerindustrie. Weltweit explodieren die Pr...   \n",
       "4  Deutsche Bahn. Der Konzern verdient mit der Be...   \n",
       "\n",
       "                                                file  \n",
       "0  20 RIVALEN. EIN JAHR. EIN SATZ. EINE SENSATION...  \n",
       "1                             23 Hier geht_s ab_.txt  \n",
       "2                           27 MISTER 14 PROZENT.txt  \n",
       "3                                     Abgesoffen.txt  \n",
       "4                            Aktionsplan Schiene.txt  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the collected data\n",
    "capital_lexisnexis = pd.DataFrame({\n",
    "    'journal': article_data_lexisnexis['journal'],\n",
    "    'day': article_data_lexisnexis['day'],\n",
    "    'month': article_data_lexisnexis['month'],\n",
    "    'year': article_data_lexisnexis['year'],\n",
    "    'title': article_data_lexisnexis['title'],\n",
    "    'text': article_data_lexisnexis['text'],\n",
    "    'file': article_data_lexisnexis['file']\n",
    "})\n",
    "\n",
    "capital_lexisnexis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2546e",
   "metadata": {},
   "source": [
    "To match the full texts of the loaded articles with their sentiment annotations from the Media Tenor dataset, we follow several key steps. First, we create a date in the same format as in the `sentiment_data` DataFrame. Next, we normalize the titles to ensure accurate matching. We also verify that there are no duplicate articles. After pre-processing, we merge the articles loaded from LexisNexis with their sentiment annotations from the Media Tenor dataset. We then sort the final DataFrame `data_match_lexisnexis` in chronological order and retain only the relevant columns. Through this process, we successfully matched **164** Capital articles from LexisNexis with their sentiment annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "681fffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles from LexisNexis: 164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>file</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Capital</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>\"Der Mythos vom Staat, der nur stört\"</td>\n",
       "      <td>ESSAY. Wer etwas riskiert, erntet die Früchte,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DER MYTHOS VOM STAAT_ DER NUR ST_RT.txt</td>\n",
       "      <td>der mythos vom staat der nur stört</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Capital</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>\"Die mächtigste Zahl der wirtschaft\"</td>\n",
       "      <td>Das Bruttoinlandsprodukt ist der Puls unserer ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DIE M_CHTIGSTE ZAHL DER WIRTSCHAFT.txt</td>\n",
       "      <td>die mächtigste zahl der wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Capital</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>\"Die verdrängte Schuldenkrise\"</td>\n",
       "      <td>WELT DER WIRTSCHAFT. Seit Jahren reden alle üb...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>DIE VERDR_NGTE SCHULDENKRISE.txt</td>\n",
       "      <td>die verdrängte schuldenkrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Capital</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>2014</td>\n",
       "      <td>\"Hier schlägt das herz der französischen Wirts...</td>\n",
       "      <td>FRANKREICHS WIRTSCHAFT LAHMT. DOCH EIN PROVINZ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Hier schl_gt das Herz der franz_sischen Wirtsc...</td>\n",
       "      <td>hier schlägt das herz der französischen wirtsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Capital</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>2014</td>\n",
       "      <td>\"Das Desaster der Master\"</td>\n",
       "      <td>Banken im Umbruch: GeschäftsmodellDie Finanzwe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DAS DESASTER DER MASTER.txt</td>\n",
       "      <td>das desaster der master</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     journal  day  month  year  \\\n",
       "159  Capital   24      7  2014   \n",
       "160  Capital   24      7  2014   \n",
       "161  Capital   24      7  2014   \n",
       "162  Capital   21      8  2014   \n",
       "163  Capital   21      8  2014   \n",
       "\n",
       "                                                 title  \\\n",
       "159              \"Der Mythos vom Staat, der nur stört\"   \n",
       "160               \"Die mächtigste Zahl der wirtschaft\"   \n",
       "161                     \"Die verdrängte Schuldenkrise\"   \n",
       "162  \"Hier schlägt das herz der französischen Wirts...   \n",
       "163                          \"Das Desaster der Master\"   \n",
       "\n",
       "                                                  text  sentiment  \\\n",
       "159  ESSAY. Wer etwas riskiert, erntet die Früchte,...        1.0   \n",
       "160  Das Bruttoinlandsprodukt ist der Puls unserer ...        1.0   \n",
       "161  WELT DER WIRTSCHAFT. Seit Jahren reden alle üb...       -1.0   \n",
       "162  FRANKREICHS WIRTSCHAFT LAHMT. DOCH EIN PROVINZ...       -1.0   \n",
       "163  Banken im Umbruch: GeschäftsmodellDie Finanzwe...        0.0   \n",
       "\n",
       "                                                  file  \\\n",
       "159            DER MYTHOS VOM STAAT_ DER NUR ST_RT.txt   \n",
       "160             DIE M_CHTIGSTE ZAHL DER WIRTSCHAFT.txt   \n",
       "161                   DIE VERDR_NGTE SCHULDENKRISE.txt   \n",
       "162  Hier schl_gt das Herz der franz_sischen Wirtsc...   \n",
       "163                        DAS DESASTER DER MASTER.txt   \n",
       "\n",
       "                                           title_clean  \n",
       "159                 der mythos vom staat der nur stört  \n",
       "160                 die mächtigste zahl der wirtschaft  \n",
       "161                       die verdrängte schuldenkrise  \n",
       "162  hier schlägt das herz der französischen wirtsc...  \n",
       "163                            das desaster der master  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary to transform month name into month number\n",
    "name_to_number = {\n",
    "    u'Januar': '01', u'Februar': '02', u'M\\xe4rz': '03', u'April': '04', u'Mai': '05',\n",
    "    u'Juni': '06', u'Juli': '07', u'August': '08', u'September': '09', u'Oktober': '10',\n",
    "    u'November': '11', u'Dezember': '12',\n",
    "    u'January': '01', u'February': '02', u'March': '03',\n",
    "    u'May': '05', u'June': '06', u'July': '07',\n",
    "    u'October': '10', u'December': '12'\n",
    "}\n",
    "\n",
    "# Transform month names into month numbers\n",
    "capital_lexisnexis['month_num'] = capital_lexisnexis['month'].map(name_to_number)\n",
    "\n",
    "# Transform single-digit day numbers into two-digit format\n",
    "capital_lexisnexis['day'] = capital_lexisnexis['day'].map(lambda d: day_transform.get(d, d))\n",
    "\n",
    "# Combine day, month, and year into a date string\n",
    "capital_lexisnexis['date'] = capital_lexisnexis.apply(lambda row: f\"{row['day']}.{row['month_num']}.{row['year']}\", axis=1)\n",
    "\n",
    "# Initialize the Normalize class with the titles from the 'capital_lexisnexis' DataFrame\n",
    "normalizer = Normalize(capital_lexisnexis.title)\n",
    "\n",
    "# Apply the normalization process to the titles\n",
    "normalized_titles = normalizer.normalized()\n",
    "\n",
    "# Add the normalized titles to the 'capital_lexisnexis' DataFrame as a new column 'title_clean'\n",
    "capital_lexisnexis['title_clean'] = normalized_titles\n",
    "\n",
    "# Merge with sentiment_data on title_clean and date\n",
    "data_match_lexisnexis = pd.merge(sentiment_data, capital_lexisnexis, how='inner', on=['title_clean', 'date'])\n",
    "\n",
    "# Rename the 'month_num' column to 'month'\n",
    "data_match_lexisnexis = data_match_lexisnexis.rename(columns={'month_num': 'month'})\n",
    "\n",
    "# Rename the 'year_y' column to 'year'\n",
    "data_match_lexisnexis = data_match_lexisnexis.rename(columns={'year_y': 'year'})\n",
    "\n",
    "# Convert year, month, and day to integers\n",
    "data_match_lexisnexis['year'] = data_match_lexisnexis['year'].astype(int)\n",
    "data_match_lexisnexis['month'] = data_match_lexisnexis['month'].astype(int)\n",
    "data_match_lexisnexis['day'] = data_match_lexisnexis['day'].astype(int)\n",
    "\n",
    "# Sort the data in chronological order\n",
    "data_match_lexisnexis = data_match_lexisnexis.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "data_match_lexisnexis = data_match_lexisnexis.reset_index(drop=True)\n",
    "\n",
    "# Rename the 'title_y' column to 'title' to reflect the title from the LexisNexis dataset\n",
    "data_match_lexisnexis = data_match_lexisnexis.rename(columns={'title_y': 'title'})\n",
    "\n",
    "# Select only the required columns\n",
    "data_match_lexisnexis = data_match_lexisnexis[['journal', 'day', 'month', 'year', 'title', 'text', 'sentiment', 'file', 'title_clean']]\n",
    "\n",
    "# Print the number of articles from LexisNexis\n",
    "num_lexisnexis_articles = len(data_match_lexisnexis)\n",
    "\n",
    "print(f\"Number of articles from LexisNexis: {num_lexisnexis_articles}\")\n",
    "\n",
    "# Display the last few rows of the final matched dataset\n",
    "data_match_lexisnexis.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01a68d",
   "metadata": {},
   "source": [
    "## Combine All Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dafc76",
   "metadata": {},
   "source": [
    "As the final step, we combine all Capital articles, specifically those downloaded from Factiva and LexisNexis, into a single DataFrame called `capital_all`. This combined DataFrame is then saved as a CSV file named `capital.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "492e95cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles: 362\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>file</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capital</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>LASST UNS DIE ZUKUNFT BAUEN !</td>\n",
       "      <td>Konjunktur Die Deutschen genießen ihren Aufsch...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Factiva-20200811-1348 (2).txt</td>\n",
       "      <td>lasst uns die zukunft bauen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capital</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>HINTERTÜR IM FERNEN OSTEN</td>\n",
       "      <td>Unsere Sanktionen gegen Russland werden wirkun...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Factiva-20200811-1349.txt</td>\n",
       "      <td>hintertür im fernen osten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capital</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>Gute Sparer ,  schlechte Anleger</td>\n",
       "      <td>Wenn es um Vermögensbildung geht, sind die Deu...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Factiva-20200811-1351.txt</td>\n",
       "      <td>gute sparer schlechte anleger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capital</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>\" Im Osten muss sich das Unternehmertum erst w...</td>\n",
       "      <td>Deutsche Einheit Kann man die Lehren aus der d...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Factiva-20200811-1344 (2).txt</td>\n",
       "      <td>im osten muss sich das unternehmertum erst wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capital</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>WIRTSCHAFTSKARTE</td>\n",
       "      <td>KONJUNKTUR RUND UM DEN GLOBUS Nach einem guten...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Factiva-20200811-1345.txt</td>\n",
       "      <td>wirtschaftskarte</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   journal  day  month  year  \\\n",
       "0  Capital   18      9  2014   \n",
       "1  Capital   18      9  2014   \n",
       "2  Capital   18      9  2014   \n",
       "3  Capital   23     10  2014   \n",
       "4  Capital   23     10  2014   \n",
       "\n",
       "                                               title  \\\n",
       "0                      LASST UNS DIE ZUKUNFT BAUEN !   \n",
       "1                          HINTERTÜR IM FERNEN OSTEN   \n",
       "2                   Gute Sparer ,  schlechte Anleger   \n",
       "3  \" Im Osten muss sich das Unternehmertum erst w...   \n",
       "4                                   WIRTSCHAFTSKARTE   \n",
       "\n",
       "                                                text  sentiment  \\\n",
       "0  Konjunktur Die Deutschen genießen ihren Aufsch...       -1.0   \n",
       "1  Unsere Sanktionen gegen Russland werden wirkun...        1.0   \n",
       "2  Wenn es um Vermögensbildung geht, sind die Deu...       -1.0   \n",
       "3  Deutsche Einheit Kann man die Lehren aus der d...       -1.0   \n",
       "4  KONJUNKTUR RUND UM DEN GLOBUS Nach einem guten...       -1.0   \n",
       "\n",
       "                            file  \\\n",
       "0  Factiva-20200811-1348 (2).txt   \n",
       "1      Factiva-20200811-1349.txt   \n",
       "2      Factiva-20200811-1351.txt   \n",
       "3  Factiva-20200811-1344 (2).txt   \n",
       "4      Factiva-20200811-1345.txt   \n",
       "\n",
       "                                         title_clean  \n",
       "0                        lasst uns die zukunft bauen  \n",
       "1                          hintertür im fernen osten  \n",
       "2                      gute sparer schlechte anleger  \n",
       "3  im osten muss sich das unternehmertum erst wie...  \n",
       "4                                   wirtschaftskarte  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all articles from Factiva and LexisNexis datasets into a single DataFrame\n",
    "capital_all = pd.concat([data_match_factiva, data_match_lexisnexis], sort=False)\n",
    "\n",
    "# Reset the index of the combined DataFrame\n",
    "capital_all = capital_all.reset_index(drop=True)\n",
    "\n",
    "# Print the total number of articles in the combined DataFrame\n",
    "total_articles = len(capital_all)\n",
    "print(f\"Total number of articles: {total_articles}\")\n",
    "\n",
    "# Display the first few rows of the combined DataFrame to verify the merge\n",
    "capital_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20c69087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the combined DataFrame in chronological order\n",
    "capital_all = capital_all.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "capital_all = capital_all.reset_index(drop=True)\n",
    "\n",
    "# Drop the 'title_clean' column as it is no longer needed\n",
    "capital_all = capital_all.drop(columns=['title_clean'])\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "capital_all.to_csv('capital.csv', encoding='utf-8-sig', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
