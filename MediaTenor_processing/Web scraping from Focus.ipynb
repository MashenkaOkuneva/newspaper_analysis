{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884ea676",
   "metadata": {},
   "source": [
    "Given that 18% of the annotated articles (730 annotations) came from Focus, we decided to speed up the downloading process by web scraping 223 of these articles (31% of all annotations) directly from the Focus website. Articles published between 2011 and 2019 are freely accessible without a subscription."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76789869",
   "metadata": {},
   "source": [
    "## Media Tenor dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d58d6",
   "metadata": {},
   "source": [
    "First, we need to load the dataset provided by Media Tenor. This step is important because we will be scraping articles from Focus for the period 2011-2019, but only if their titles at least partially match those in the Media Tenor dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b1b7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2014</td>\n",
       "      <td>201401</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Koalition</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>FAS</td>\n",
       "      <td>Habt bloß keine Angst vor China !</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Wir leben in einer Zeit der Wohlstands-Halluzi...</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.02.2015</td>\n",
       "      <td>201502</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Teheran ruft</td>\n",
       "      <td>Wettbewerbsfähigkeit/Nachfrage</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Geht es und wirklich so gut, wie es uns Merkel...</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month medium  \\\n",
       "0  01.01.2014  201401   WamS   \n",
       "1  01.01.2017  201701    FAS   \n",
       "2  01.01.2017  201701   BamS   \n",
       "3  01.02.2015  201502   WamS   \n",
       "4  01.01.2017  201701   BamS   \n",
       "\n",
       "                                               title  \\\n",
       "0                                          Koalition   \n",
       "1                  Habt bloß keine Angst vor China !   \n",
       "2  Wir leben in einer Zeit der Wohlstands-Halluzi...   \n",
       "3                                       Teheran ruft   \n",
       "4  Geht es und wirklich so gut, wie es uns Merkel...   \n",
       "\n",
       "                       topicgroup  negative  no_clear_tone  positive  \\\n",
       "0                      Konjunktur         0              1         0   \n",
       "1       Internationale Wirtschaft         0              0         1   \n",
       "2                      Konjunktur         0              0         1   \n",
       "3  Wettbewerbsfähigkeit/Nachfrage         1              3         0   \n",
       "4       Internationale Wirtschaft         0              1         0   \n",
       "\n",
       "   Number_of_reports AverageRating  \n",
       "0                  1             0  \n",
       "1                  1           100  \n",
       "2                  1           100  \n",
       "3                  4           -25  \n",
       "4                  1             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset acquired from Media Tenor\n",
    "sentiment_data = pd.read_csv('Daten_Wirtschaftliche_Lage.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "# Filter out rows with empty titles, as we cannot identify and download the articles without titles\n",
    "sentiment_data = sentiment_data[sentiment_data['title'].notnull()]\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773d0ee",
   "metadata": {},
   "source": [
    "Due to the manual entry of titles in the Media Tenor dataset, there are potential inconsistencies in punctuation and spacing. To resolve this and achieve precise matching with the titles of the articles we scrape from the website, we normalize the titles in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cf2424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['koalition', 'habt bloß keine angst vor china', 'wir leben in einer zeit der wohlstands halluzination']\n"
     ]
    }
   ],
   "source": [
    "# Import the Normalize class from the normalize module\n",
    "from normalize import Normalize\n",
    "\n",
    "# Initialize the Normalize class with the titles from the sentiment_data DataFrame\n",
    "normalizer = Normalize(sentiment_data.title)\n",
    "\n",
    "# Apply the normalization process to the titles\n",
    "normalized_titles = normalizer.normalized()\n",
    "\n",
    "# Print the first three normalized titles to verify the results\n",
    "print(normalized_titles[:3])\n",
    "\n",
    "# Add the normalized titles to the sentiment_data DataFrame as a new column 'title_clean'\n",
    "sentiment_data['title_clean'] = normalized_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14437232",
   "metadata": {},
   "source": [
    "Our focus is on annotated articles from Focus that pertain to business cycle conditions, as these are the specific articles we intend to scrape from the website, depending on their availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85784aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.04.2017</td>\n",
       "      <td>201704</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Die Kündigung</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-50</td>\n",
       "      <td>die kündigung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.07.2017</td>\n",
       "      <td>201707</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Läuft.</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>läuft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.04.2017</td>\n",
       "      <td>201704</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Anlegen in Zeiten von Trump</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>anlegen in zeiten von trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.07.2013</td>\n",
       "      <td>201307</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Gewinne trotz Wackelbörse</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>gewinne trotz wackelbörse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.10.2011</td>\n",
       "      <td>201110</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Macht Europa nicht kaputt</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>macht europa nicht kaputt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month medium                        title  topicgroup  \\\n",
       "0  01.04.2017  201704  Focus                Die Kündigung  Konjunktur   \n",
       "1  01.07.2017  201707  Focus                       Läuft.  Konjunktur   \n",
       "2  01.04.2017  201704  Focus  Anlegen in Zeiten von Trump  Konjunktur   \n",
       "3  01.07.2013  201307  Focus    Gewinne trotz Wackelbörse  Konjunktur   \n",
       "4  01.10.2011  201110  Focus    Macht Europa nicht kaputt  Konjunktur   \n",
       "\n",
       "   negative  no_clear_tone  positive  Number_of_reports AverageRating  \\\n",
       "0         1              1         0                  2           -50   \n",
       "1         0              0         1                  1           100   \n",
       "2         0              0         2                  2           100   \n",
       "3         0              0         3                  3           100   \n",
       "4         0              1         0                  1             0   \n",
       "\n",
       "                   title_clean  \n",
       "0                die kündigung  \n",
       "1                        läuft  \n",
       "2  anlegen in zeiten von trump  \n",
       "3    gewinne trotz wackelbörse  \n",
       "4    macht europa nicht kaputt  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the dataset to include only articles from Focus\n",
    "sentiment_data = sentiment_data[sentiment_data['medium'] == 'Focus']\n",
    "\n",
    "# Reset the index of the DataFrame and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Further filter the dataset to include only articles related to the business cycle conditions (Konjunktur)\n",
    "sentiment_data = sentiment_data[sentiment_data['topicgroup'] == 'Konjunktur']\n",
    "\n",
    "# Reset the index of the DataFrame again and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame to verify the results\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e6533",
   "metadata": {},
   "source": [
    "## Web scraping: example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b396a2",
   "metadata": {},
   "source": [
    "The following code illustrates the process of scraping articles from a specific issue of Focus (19th issue of 2019) that have titles at least partially matching those listed in the Media Tenor dataset. These articles are publicly accessible without a subscription. In our example, we demonstrate how to extract article URLs from the issue's index page, access each link, and retrieve the article's title, publication date, and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961cf087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Content Preview:\n",
      " <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n",
      "<html itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\" lang=\"de\" xml:lang=\"de\" xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:fb=\"http://ogp.me/ns/fb#\" xmlns:og=\"http://opengraphprotocol.org/schema/\">\n",
      " <head>\n",
      "  <title>\n",
      "   FOCUS Magazin Heft-Archiv - Alle Jahrgänge\n",
      "  </title>\n",
      "  <meta content=\"de\" http-equiv=\"content-language\"/>\n",
      "  <meta content=\"Im FOCUS-Archiv auf FOCUS Online finden S\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL for a specific issue of Focus from 2019\n",
    "focus_issue_url = \"https://www.focus.de/magazin/archiv/jahrgang_2019/ausgabe_19/\"\n",
    "\n",
    "# Open the URL and create a BeautifulSoup object\n",
    "focus_issue = urlopen(focus_issue_url)\n",
    "focus_issue_soup = BeautifulSoup(focus_issue, 'html.parser')\n",
    "\n",
    "# Print the first 500 characters of the HTML content for a brief overview\n",
    "print(\"HTML Content Preview:\\n\", focus_issue_soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5039e26",
   "metadata": {},
   "source": [
    "Here, we show how to extract the issue number and year from the URL of a Focus issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ddbad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue Number: 19, Year: 2019\n"
     ]
    }
   ],
   "source": [
    "# Extract the issue number from the URL\n",
    "issue_number = int(focus_issue_url.split('/')[-2].replace(\"ausgabe_\", ''))\n",
    "\n",
    "# Extract the year from the URL\n",
    "year = int(focus_issue_url.split('/')[-3].replace(\"jahrgang_\", ''))\n",
    "\n",
    "# Display the extracted issue number and year\n",
    "print(f\"Issue Number: {issue_number}, Year: {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ddd7e8",
   "metadata": {},
   "source": [
    "Next, we extract the links and titles of all the articles from a particular issue. We find all the `<a>` tags in the HTML document and check if they contain a valid link. We then filter these links based on specific conditions to ensure they point to actual articles, excluding irrelevant links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5129bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Links (first 10): ['https://www.focus.de/finanzen/boerse/automatische-geldanlage-mit-einem-roboadvisor-so-finden-sie-den-besten-digitalen-anlagehelfer-fuer-ihren-vermoegensaufbau_id_9142131.html', 'https://www.focus.de/finanzen/versicherungen/schutz-fuer-teure-schaetzchen-hausratversicherung_id_1743846.html', 'https://www.focus.de/finanzen/versicherungen/bein-ab-arm-dran-unfallversicherung_id_2262359.html', 'https://www.focus.de/magazin/archiv/politik-die-aufs-und-abs-der-woche_id_10664136.html', 'https://www.focus.de/magazin/archiv/politik-afd-will-fraktionsspitze-umbauen-jeder-zweite-soll-gehen_id_10664138.html', 'https://www.focus.de/magazin/archiv/der-auferstandene-zurueck-auf-der-bildflaeche_id_10664151.html', 'https://www.focus.de/magazin/archiv/der-newcomer-zurueck-auf-der-buehne_id_10664153.html', 'https://www.focus.de/magazin/archiv/power-paare-wer-mit-wem-wer-gegen-wen-ruestungsstopp-zerschiesst-bilanz_id_10664159.html', 'https://www.focus.de/magazin/archiv/der-absteiger-zurueck-ins-bodenlose_id_10664161.html', 'https://www.focus.de/magazin/archiv/power-paare-wer-mit-wem-wer-gegen-wen-angriff-ist-die-beste-verteidigung_id_10664166.html']\n",
      "Extracted Headings (first 10): ['Robo-Advisor', 'Hausratversicherung', 'Unfallversicherung', 'Die Aufs und  Abs der Woche', 'AfD will Fraktionsspitze umbauen: Jeder Zweite soll gehen', 'Zurück auf der Bildfläche', 'Zurück auf der  Bühne', 'Rüstungsstopp zerschießt Bilanz', 'Zurück ins  Bodenlose', 'Angriff ist die beste Verteidigung']\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store article links and headings\n",
    "links_journal = []\n",
    "headings = []\n",
    "\n",
    "# Find all <a> tags in the HTML document\n",
    "all_links_focus = focus_issue_soup.findAll(\"a\")\n",
    "\n",
    "# List of links to exclude\n",
    "exclude_links = [\n",
    "    'https://www.focus.de/politik/', 'https://www.focus.de/finanzen/', 'https://www.focus.de/kultur/',\n",
    "    'https://www.focus.de/finanzen/focus-online-kooperationen-services-vergleiche-rechner_id_8615608.html',\n",
    "    'https://www.focus.de/politik/deutschland/', 'https://www.focus.de/politik/sicherheitsreport/',\n",
    "    'https://www.focus.de/politik/gerichte-in-deutschland/', 'https://www.focus.de/politik/ausland/',\n",
    "    'https://www.focus.de/politik/videos/', 'https://www.focus.de/politik/experten/', \n",
    "    'https://www.focus.de/politik/praxistipps/', 'https://www.focus.de/politik/', \n",
    "    'https://www.focus.de/finanzen/boerse/', 'https://www.focus.de/finanzen/altersvorsorge/',\n",
    "    'https://www.focus.de/finanzen/news/', 'https://www.focus.de/finanzen/banken/', \n",
    "    'https://www.focus.de/finanzen/versicherungen/', 'https://www.focus.de/finanzen/recht/', \n",
    "    'https://www.focus.de/finanzen/karriere/', 'https://www.focus.de/finanzen/experten/', \n",
    "    'https://www.focus.de/finanzen/steuern/', 'https://www.focus.de/finanzen/videos/', \n",
    "    'https://www.focus.de/finanzen/', \n",
    "    'https://www.focus.de/finanzen/boersenbriefe/finanzen100-boersenbriefe-die-wichtigsten-infos-fuer-ihren-boersenerfolg_id_7150217.html',\n",
    "    'https://www.focus.de/finanzen/banken/waehrungsrechner-die-wechselkurse-am-bankschalter_aid_53741.html',\n",
    "    'https://www.focus.de/finanzen/banken/kredit/tid-8141/kreditrechner_aid_210205.html', \n",
    "    'https://www.focus.de/finanzen/versicherungen/krankenversicherung/der-grosse-kassenvergleich-krankenversicherung_id_1725907.html', \n",
    "    'https://www.focus.de/finanzen/banken/kreditkarten/kreditkarten-100-angebote-im-vergleich_id_2306740.html',\n",
    "    'https://www.focus.de/finanzen/steuern/gehaltsplaner/brutto-netto-rechner-was-ihnen-vom-gehalt-uebrig-bleibt_id_2297045.html', \n",
    "    'https://www.focus.de/finanzen/altersvorsorge/rente/tid-8425/rentenrechner_aid_68489.html'\n",
    "]\n",
    "\n",
    "# Extract the link and heading of each article\n",
    "for link in all_links_focus:\n",
    "    # Check if there is a link part in the tag\n",
    "    if link.get(\"href\") is not None:\n",
    "        href = link.get(\"href\")\n",
    "        # Check if the link contains any valid section and include only those links that point to actual articles\n",
    "        if (any(section in href for section in ['/magazin/archiv', '/politik/', '/kultur/', '/finanzen/', '/wissen/', \n",
    "                                    '/sport/', '/gesundheit/', '/auto/', '/reisen/']) and \n",
    "            'login/' not in href and \n",
    "            'rss.focus.de' not in href and \n",
    "            href != 'https://www.focus.de/magazin/archiv/' and \n",
    "            href not in exclude_links and \n",
    "            '.html' in href):\n",
    "            # Append the link to links_journal list\n",
    "            links_journal.append(href)\n",
    "            # Append the cleaned heading to headings list\n",
    "            remove = link.text.split(':')[0]\n",
    "            headings.append(link.text.replace(remove + ':', '').strip('\\xa0').replace('\\ufeff', '').replace(\":\\xa0\", \"\").lstrip())\n",
    "\n",
    "# Display the first 10 extracted links and headings for verification\n",
    "print(\"Extracted Links (first 10):\", links_journal[:10])\n",
    "print(\"Extracted Headings (first 10):\", headings[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707aec5e",
   "metadata": {},
   "source": [
    "We also demonstrate how to extract the publication date, annotation, and main text from a specific Focus article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1119962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication Date: 03.05.2019\n",
      "\n",
      "Annotation: \n",
      "Die Wahl des neuen Vorstands muss um drei Monate vorgezogen werden, fordern unzufriedene Abgeordnete\n",
      "\n",
      "Main Text: \n",
      "Die Wahl des neuen Vorstands muss um drei Monate vorgezogen werden, fordern unzufriedene Abgeordnete.  Weil es massive Kritik an ihrer Amtsführung gibt, stehen zahlreiche Mitglieder  der AfD -Fraktionsführung im Bundestag vor dem Aus. Mehrere Abgeordnete haben beantragt, die turnusgemäß für September geplante Neuwahl der Fraktionsführung auf den 4. Juni vorzuverlegen. Dabei könnten von den elf Vorstandsposten bis zu fünf neu besetzt werden, heißt es von AfD-Abgeordneten. „In der AfD gibt es keine Erbpacht auf Ämter und Funktionen“, sagt Fraktionschefin Alice Weidel. Deshalb gelte niemand vor einer Wahl als gesetzt. Allerdings können Weidel und Co-Chef Alexander Gauland bislang mit einer Wiederwahl rechnen. Dagegen droht zwei der fünf stellvertretenden Fraktionschefs die Abwahl: Peter Felser und Beatrix von Storch. Von Storch hatte schon bei der letzten Wahl 2017 ein sehr knappes Ergebnis und gilt als wenig medientauglich. Felser wird schlechte Organisation der internen Klausurtagungen vorgeworfen. Bei den vier Parlamentarischen Geschäftsführern (PGF), die den Alltag der Fraktion managen, stehen Michael Espendiller und Hansjörg Müller vor dem Aus. In Müllers Verantwortungsbereich liegt die Finanzbuchhaltung der Fraktion. Im vergangenen Oktober wurden Unregelmäßigkeiten bei den Reisekostenabrechnungen von Mitarbeitern entdeckt. Zur Zitterpartie könnte die Wahl für den für Medienarbeit verantwortlichen PGF Jürgen Braun werden. Viele sind unzufrieden mit dem AfD-Newsroom. Es werde „mehrere Veränderungen bei den Parlamentarischen Geschäftsführern und den stellvertretenden Fraktionsvorsitzenden geben“, sagt auch der AfD-Abgeordnete und Fraktionsjustiziar Stephan Brandner. Es sei daher wichtig, die Wahl auf Juni vorzuziehen, „damit die Personen, die dann Verantwortung tragen, nach der Sommerpause mit Vollgas starten können“. agr.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL for a specific article\n",
    "focus_article_url = links_journal[4]\n",
    "\n",
    "# Open the URL and create a BeautifulSoup object\n",
    "focus_article = urlopen(focus_article_url)\n",
    "focus_article_soup = BeautifulSoup(focus_article, 'html.parser')\n",
    "\n",
    "# Extract the publication date of the article\n",
    "date = focus_article_soup.find(\"div\", {\"class\": \"displayDate\"}).get_text().split(' ')[1].replace(',', '')\n",
    "\n",
    "# Extract the annotation of the article\n",
    "annotation = focus_article_soup.find(\"div\", {\"class\": \"leadIn\"}).get_text()\n",
    "\n",
    "# Extract the paragraphs of the main text of the article\n",
    "paragraphs = list(focus_article_soup.find(\"div\", {\"class\": \"textBlock\"}).children)\n",
    "\n",
    "# Initialize the text with the annotation\n",
    "text_new = ''\n",
    "if annotation.strip()[-1] not in ['.', '!', ':', ';', '?', '\"', \"'\", '...', '…']:\n",
    "    text_new = text_new + annotation.strip() + '. '\n",
    "else:\n",
    "    text_new = text_new + annotation.strip()\n",
    "\n",
    "# Process each paragraph of the main text\n",
    "for par in paragraphs:\n",
    "    if par.get_text().replace('\\n', ''):\n",
    "        if (par.get_text().strip()[-1] not in ['.', '!', ':', ';', '?', '\"', \"'\", '...', '…']):\n",
    "            text_new = text_new + ' ' + par.get_text(separator=' ').strip() + '.'\n",
    "        else:\n",
    "            text_new = text_new + ' ' + par.get_text(separator=' ').strip()\n",
    "\n",
    "# Clean up the final text\n",
    "text_new = text_new.strip()\n",
    "text_new = text_new.replace(\"\\n\", '')\n",
    "\n",
    "# Print the extracted publication date\n",
    "print(\"Publication Date:\", date)\n",
    "\n",
    "# Print the extracted annotation\n",
    "print(\"\\nAnnotation:\", \"\\n\" + annotation.strip())\n",
    "\n",
    "# Print the cleaned and formatted main text\n",
    "print(\"\\nMain Text:\", \"\\n\" + text_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632f50d",
   "metadata": {},
   "source": [
    "Finally, we demonstrate how to download the full texts, titles, and publication dates of articles from a specific issue of Focus that have titles at least partially matching those in the Media Tenor dataset. We process each article link, extract the necessary details, and save each article as a TXT file. Furthermore, we compile all the articles into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d531ebe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>titles</th>\n",
       "      <th>titles_clean</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03.05.2019</td>\n",
       "      <td>Wer nächste Woche wichtig wird</td>\n",
       "      <td>wer nächste woche wichtig wird</td>\n",
       "      <td>Wer nächste Woche wichtig wird. So. Papst Fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03.05.2019</td>\n",
       "      <td>Zeugnistag</td>\n",
       "      <td>zeugnistag</td>\n",
       "      <td>Zeugnistag. Schlapper Start Stuttgart Das Jahr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03.05.2019</td>\n",
       "      <td>Neues Milliardenloch</td>\n",
       "      <td>neues milliardenloch</td>\n",
       "      <td>Neues Milliardenloch. Der Bundesregierung droh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03.05.2019</td>\n",
       "      <td>Musik</td>\n",
       "      <td>musik</td>\n",
       "      <td>Musik. (Rang Vorwoche/Anzahl der Wochen).  1 V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03.05.2019</td>\n",
       "      <td>Zahlen, bitte</td>\n",
       "      <td>zahlen bitte</td>\n",
       "      <td>Zahlen, bitte. 31 Pensionskassen von insgesamt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dates                          titles                    titles_clean  \\\n",
       "0  03.05.2019  Wer nächste Woche wichtig wird  wer nächste woche wichtig wird   \n",
       "1  03.05.2019                      Zeugnistag                      zeugnistag   \n",
       "2  03.05.2019            Neues Milliardenloch            neues milliardenloch   \n",
       "3  03.05.2019                           Musik                           musik   \n",
       "4  03.05.2019                   Zahlen, bitte                    zahlen bitte   \n",
       "\n",
       "                                                text  \n",
       "0  Wer nächste Woche wichtig wird. So. Papst Fran...  \n",
       "1  Zeugnistag. Schlapper Start Stuttgart Das Jahr...  \n",
       "2  Neues Milliardenloch. Der Bundesregierung droh...  \n",
       "3  Musik. (Rang Vorwoche/Anzahl der Wochen).  1 V...  \n",
       "4  Zahlen, bitte. 31 Pensionskassen von insgesamt...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the set of punctuation characters to exclude\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Lists to store extracted data\n",
    "texts = []\n",
    "titles = []\n",
    "titles_clean = []\n",
    "dates = []\n",
    "\n",
    "# Identifier for the articles\n",
    "id = 0\n",
    "\n",
    "# Ensure the directory exists for saving articles\n",
    "save_dir = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Focus_scrape_example_txt')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the list of headings\n",
    "for heading in headings:\n",
    "    id += 1\n",
    "    id_fixed = str(id)\n",
    "    \n",
    "    # Retrieve the title of the current article from the headings list\n",
    "    title = headings[(id-1)]\n",
    "    title = title.strip().replace(\"\\n\", '')\n",
    "    # Normalize the title\n",
    "    # Remove specific punctuation, replace hyphens with spaces, convert to lowercase, and strip leading/trailing spaces\n",
    "    title_clean = ''.join(ch for ch in title.replace('-', ' ').lower() if (ch not in exclude) and (ch not in ['\"', '„', '“', '»', '«'])).strip()\n",
    "    # Standardize spaces to a single space\n",
    "    title_clean = \" \".join(title_clean.split())\n",
    "    \n",
    "    # Check if the normalized title of the current article at least partially matches \n",
    "    # any normalized title from the MediaTenor dataset\n",
    "    if any(title_clean in s for s in sentiment_data['title_clean']):\n",
    "        # Open the article link\n",
    "        try: \n",
    "            article = urlopen(links_journal[(id-1)])\n",
    "            # Create a BeautifulSoup object\n",
    "            article = BeautifulSoup(article, 'html.parser')\n",
    "            # Extract the paragraphs of the main text of the article\n",
    "            paragraphs = list(article.find(\"div\", {\"class\": \"textBlock\"}).children)\n",
    "            \n",
    "            # Proceed only if paragraphs are found\n",
    "            if paragraphs:\n",
    "                \n",
    "                # Extract the publication date of the article\n",
    "                date = article.find(\"div\", {\"class\": \"displayDate\"}).get_text().split(' ')[1].replace(',', '')\n",
    "                               \n",
    "                # Initialize the text with the annotation\n",
    "                text_new = ''                \n",
    "                if article.find(\"div\", {\"class\": \"leadIn\"})  is not None: \n",
    "                    # Extract the annotation of the article\n",
    "                    annotation = article.find(\"div\", {\"class\": \"leadIn\"}).get_text()\n",
    "                    \n",
    "                    if annotation.strip()[-1] not in ['.', '!', ':', ';', '?', '\"', \"'\", '...', '…']:\n",
    "                        text_new = text_new + annotation.strip() + '. '\n",
    "                    else:\n",
    "                        text_new = text_new + annotation.strip()\n",
    "\n",
    "                # Extract the text from the paragraphs\n",
    "                for par in paragraphs:\n",
    "                    if par.get_text().replace('\\n', ''):\n",
    "                        if (par.get_text().strip()[-1] not in ['.', '!', ':', ';', '?', '\"', \"'\", '...', '…']):\n",
    "                            text_new = text_new + ' ' + par.get_text(separator=' ').strip() + '.'\n",
    "                        else:\n",
    "                            text_new = text_new + ' ' + par.get_text(separator=' ').strip()\n",
    "                \n",
    "                # Clean up the final text\n",
    "                text_new = text_new.strip().replace(\"\\n\", '')\n",
    "                \n",
    "                if title[-1] not in ['.', '!', ':', ';','?', '\"']:\n",
    "                    text_new = title + '. ' + text_new\n",
    "                else:\n",
    "                    text_new = title + ' ' + text_new\n",
    "\n",
    "            else:\n",
    "                text_new = ''\n",
    "                title = ''\n",
    "                title_clean = ''\n",
    "                date = ''\n",
    "\n",
    "        # Handle any IOError (e.g., page not found)     \n",
    "        except IOError:\n",
    "            text_new = ''\n",
    "            title = ''\n",
    "            title_clean = ''\n",
    "            date = ''\n",
    "\n",
    "        # Add the extracted data to the lists if the title is valid and at least partially matches \n",
    "        # any title in sentiment_data\n",
    "        if any(title_clean in s for s in sentiment_data['title_clean']) and title:\n",
    "            texts.append(text_new)\n",
    "            titles.append(title)\n",
    "            titles_clean.append(title_clean)\n",
    "            dates.append(date)\n",
    "            \n",
    "            # Save the article as a .txt file\n",
    "            issue_string = str(issue_number)\n",
    "            year_string = str(year)\n",
    "            file_path = os.path.join(save_dir, f\"{year_string} {issue_string} {id_fixed}.txt\")\n",
    "            with codecs.open(file_path, \"w\", \"utf-8-sig\") as temp:\n",
    "                temp.write(text_new) \n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = pd.DataFrame({\n",
    "    'dates': dates,\n",
    "    'titles': titles,\n",
    "    'titles_clean': titles_clean,\n",
    "    'text': texts\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf621b",
   "metadata": {},
   "source": [
    "## Web scraping: issues 1-52 (53), 2011 - 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e784f56",
   "metadata": {},
   "source": [
    "We employed the following code to download the full texts of articles, including their titles and publication dates, from issues 1-53 for the years 2011-2019. We only scraped an article if its title at least partially matched any title in the Media Tenor dataset. For instance, if the title of the scraped article was \"Musik\" and the Media Tenor dataset contained a title \"Fahrrad, Musik & Cowboyhut,\" the article titled \"Musik\" would be scraped as a candidate. This method allowed us to collect candidate articles that potentially matched the Media Tenor metadata.\n",
    "\n",
    "We downloaded 1,746 candidate articles from 2011-2019. Subsequently, we matched these articles with the Media Tenor metadata based on both title and publication date. As a result, we successfully scraped 223 articles from the Media Tenor dataset. The scraped texts were matched with their metadata in the notebook `Focus.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e50c535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mokuneva.UNI-KIEL\\AppData\\Local\\Temp\\3\\ipykernel_20196\\2364081027.py:199: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  data_excel.save()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py:339: UserWarning: Calling close() on already closed file.\n",
      "  warn(\"Calling close() on already closed file.\")\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import os\n",
    "# Import codecs in order to save articles as .txt files\n",
    "import codecs\n",
    "from urllib.request import urlopen\n",
    "# Import the library that pulls out HTML data\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the set of punctuation characters to exclude\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Define the directory to save the Excel file\n",
    "save_dir = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva')\n",
    "\n",
    "# Define the path to save the Excel file\n",
    "excel_file_path = os.path.join(save_dir, 'focus_2011_2019.xlsx')\n",
    "\n",
    "# Excel file where we save the data\n",
    "data_excel = pd.ExcelWriter(excel_file_path)\n",
    "\n",
    "# Directory to save text files\n",
    "text_save_dir = os.path.join(save_dir, 'Focus_scrape_2011_2019_txt')\n",
    "os.makedirs(text_save_dir, exist_ok=True)\n",
    "\n",
    "# Common part in all the links\n",
    "common = \"https://www.focus.de/magazin/archiv/jahrgang_\"\n",
    "magazine_year = 2010\n",
    "issue = 0\n",
    "number_of_rows = 0\n",
    "\n",
    "# Iterate over the years (2011 to 2019)\n",
    "for i in range(0, 9):\n",
    "    magazine_year += 1\n",
    "    magazine_year_fixed = str(magazine_year)\n",
    "\n",
    "    # Iterate over the issues (1 to 53)\n",
    "    for i in range(0, 53):\n",
    "        issue += 1\n",
    "        # Convert issue number to a zero-padded string\n",
    "        issue_fixed = str(issue).zfill(2)\n",
    "        # Construct the URL for the issue\n",
    "        focus_issue_url = common + magazine_year_fixed + \"/ausgabe_\" + issue_fixed + \"/\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Open the URL and create a BeautifulSoup object\n",
    "            focus_issue = urlopen(focus_issue_url)\n",
    "            focus_issue_soup = BeautifulSoup(focus_issue, 'html.parser')\n",
    "                        \n",
    "            # Extract the issue number from the URL\n",
    "            issue_number = int(focus_issue_url.split('/')[-2].replace(\"ausgabe_\", ''))\n",
    "            # Extract the year from the URL\n",
    "            year = int(focus_issue_url.split('/')[-3].replace(\"jahrgang_\", ''))\n",
    "            \n",
    "            # Initialize lists to store article links and headings\n",
    "            links_journal = []\n",
    "            headings = []\n",
    "            # Find all <a> tags in the HTML document\n",
    "            all_links_focus = focus_issue_soup.findAll(\"a\")\n",
    "\n",
    "            # Extract the link and heading of each article\n",
    "            for link in all_links_focus:\n",
    "                # Check if there is a link part in the tag\n",
    "                if link.get(\"href\") is not None:\n",
    "                    href = link.get(\"href\")\n",
    "                    # Check if the link contains any valid section and include only those links that point to actual articles\n",
    "                    if (any(section in href for section in ['/magazin/archiv', '/politik/', '/kultur/', '/finanzen/', '/wissen/', \n",
    "                                    '/sport/', '/gesundheit/', '/auto/', '/reisen/']) and \n",
    "                        'login/' not in href and \n",
    "                        'rss.focus.de' not in href and \n",
    "                        href != 'https://www.focus.de/magazin/archiv/' and \n",
    "                        href not in exclude_links and \n",
    "                        '.html' in href):\n",
    "                        # Append the link to links_journal list\n",
    "                        links_journal.append(href)\n",
    "                        # Append the cleaned heading to headings list\n",
    "                        remove = link.text.split(':')[0]\n",
    "                        headings.append(link.text.replace(remove + ':', '').strip('\\xa0').replace('\\ufeff', '').replace(\":\\xa0\", \"\").lstrip())\n",
    "                    \n",
    "            texts = []\n",
    "            titles = []\n",
    "            titles_clean = []\n",
    "            dates = []            \n",
    "            # Identifier of the article in the name\n",
    "            id = 0                  \n",
    "            \n",
    "            # Iterate over each article link\n",
    "            for heading in headings:\n",
    "                id += 1\n",
    "                # Convert id to a string for use in the text document name\n",
    "                id_fixed = str(id)\n",
    "                \n",
    "                # Retrieve the title of the current article from the headings list\n",
    "                title = headings[(id-1)]\n",
    "                title = title.strip().replace(\"\\n\", '')\n",
    "                # Normalize the title\n",
    "                # Remove specific punctuation, replace hyphens with spaces, convert to lowercase, and strip leading/trailing spaces\n",
    "                title_clean = ''.join(ch for ch in title.replace('-', ' ').lower() if (ch not in exclude) and (ch not in ['\"', '„', '“', '»', '«'])).strip()\n",
    "                # Standardize spaces to a single space\n",
    "                title_clean = \" \".join(title_clean.split())\n",
    "            \n",
    "                # Check if the normalized title of the current article at least partially matches \n",
    "                # any normalized title from the MediaTenor dataset\n",
    "                if any(title_clean in s for s in sentiment_data['title_clean']):\n",
    "                    # Open the article link\n",
    "                    try:\n",
    "                        article = urlopen(links_journal[(id-1)])\n",
    "                        # Create a BeautifulSoup object\n",
    "                        article = BeautifulSoup(article, 'html.parser')\n",
    "                        # Extract the paragraphs of the main text of the article\n",
    "                        paragraphs = list(article.find(\"div\", {\"class\": \"textBlock\"}).children)\n",
    "                        \n",
    "                        # Proceed only if paragraphs are found\n",
    "                        if paragraphs:\n",
    "                            \n",
    "                            # Extract the publication date of the article\n",
    "                            date = article.find(\"div\", {\"class\": \"displayDate\"}).get_text().split(' ')[1].replace(',', '')\n",
    "                            \n",
    "                            # Initialize the text with the annotation\n",
    "                            text_new = ''\n",
    "                            if article.find(\"div\", {\"class\": \"leadIn\"})  is not None:\n",
    "                                # Extract the annotation of the article\n",
    "                                annotation = article.find(\"div\", {\"class\": \"leadIn\"}).get_text()\n",
    "                                \n",
    "                                if annotation.strip()[-1] not in ['.', '!', ':', ';', '?', '\"', \"'\", '...', '…']:\n",
    "                                    text_new = text_new + annotation.strip() + '. '\n",
    "                                else:\n",
    "                                    text_new = text_new + annotation.strip()\n",
    "                                    \n",
    "                            # Extract the text from the paragraphs\n",
    "                            for par in paragraphs:\n",
    "                                if par.get_text().replace('\\n', ''):\n",
    "                                    if (par.get_text().strip()[-1] not in ['.', '!', ':', ';', '?', '\"', \"'\", '...', '…']):\n",
    "                                        text_new = text_new + ' ' + par.get_text(separator=' ').strip() + '.'\n",
    "                                    else:\n",
    "                                        text_new = text_new + ' ' + par.get_text(separator=' ').strip()\n",
    "\n",
    "                            # Clean up the final text\n",
    "                            text_new = text_new.strip().replace(\"\\n\", '')\n",
    "\n",
    "                            if title[-1] not in ['.', '!', ':', ';','?', '\"']:\n",
    "                                text_new = title + '. ' + text_new\n",
    "                            else:\n",
    "                                text_new = title + ' ' + text_new\n",
    "                                    \n",
    "                        else:\n",
    "                            text_new = ''\n",
    "                            title = ''\n",
    "                            title_clean = ''\n",
    "                            date = ''\n",
    "\n",
    "                    # Handle any IOError (e.g., page not found)         \n",
    "                    except IOError:\n",
    "                        text_new = ''\n",
    "                        title = ''\n",
    "                        title_clean = ''\n",
    "                        date = ''\n",
    "                    \n",
    "                    # Add the extracted data to the lists if the title is valid and at least partially matches \n",
    "                    # any title in sentiment_data\n",
    "                    if any(title_clean in s for s in sentiment_data['title_clean']) and title:\n",
    "                        texts.append(text_new)\n",
    "                        titles.append(title)\n",
    "                        titles_clean.append(title_clean)\n",
    "                        dates.append(date)\n",
    "\n",
    "                        # Save the article as a .txt file\n",
    "                        issue_string = str(issue_number)\n",
    "                        year_string = str(year)\n",
    "                        file_path = os.path.join(text_save_dir, f\"{year_string} {issue_string} {id_fixed}.txt\")\n",
    "                        with codecs.open(file_path, \"w\", \"utf-8-sig\") as temp:\n",
    "                            temp.write(text_new) \n",
    "                \n",
    "            # Create a DataFrame from the extracted data               \n",
    "            data = pd.DataFrame({\n",
    "                'dates': dates,\n",
    "                'text': texts,\n",
    "                'titles': titles,\n",
    "                'titles_clean': titles_clean\n",
    "            })                \n",
    "\n",
    "            # Save the data from the current issue to the Excel file\n",
    "            data.to_excel(data_excel, 'Sheet1', header = False, startrow = number_of_rows)\n",
    "            # Update the row counter\n",
    "            number_of_rows = data.shape[0] + number_of_rows\n",
    "\n",
    "        except IOError:\n",
    "            pass\n",
    "        \n",
    "    # Reset the issue counter for the next year    \n",
    "    issue = 0\n",
    "    \n",
    "# Save the Excel file  \n",
    "data_excel.save()\n",
    "data_excel.close()                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
