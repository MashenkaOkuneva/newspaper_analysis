{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 25% of the annotated articles (1,020 annotations) came from Spiegel, we found a way to speed up the downloading process by web scraping 505 of these articles (50% of all annotations) directly from Spiegel's website. When we were scraping the articles, content up to and including 2016 was freely available. Now, only articles up to and including 2009 are available without a subscription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media Tenor dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the dataset we obtained from Media Tenor. This is essential because we will be scraping articles from Spiegel for the period 2011-2016, but only if their titles match those in our Media Tenor dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2014</td>\n",
       "      <td>201401</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Koalition</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>FAS</td>\n",
       "      <td>Habt bloß keine Angst vor China !</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Wir leben in einer Zeit der Wohlstands-Halluzi...</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.02.2015</td>\n",
       "      <td>201502</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Teheran ruft</td>\n",
       "      <td>Wettbewerbsfähigkeit/Nachfrage</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Geht es und wirklich so gut, wie es uns Merkel...</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month medium  \\\n",
       "0  01.01.2014  201401   WamS   \n",
       "1  01.01.2017  201701    FAS   \n",
       "2  01.01.2017  201701   BamS   \n",
       "3  01.02.2015  201502   WamS   \n",
       "4  01.01.2017  201701   BamS   \n",
       "\n",
       "                                               title  \\\n",
       "0                                          Koalition   \n",
       "1                  Habt bloß keine Angst vor China !   \n",
       "2  Wir leben in einer Zeit der Wohlstands-Halluzi...   \n",
       "3                                       Teheran ruft   \n",
       "4  Geht es und wirklich so gut, wie es uns Merkel...   \n",
       "\n",
       "                       topicgroup  negative  no_clear_tone  positive  \\\n",
       "0                      Konjunktur         0              1         0   \n",
       "1       Internationale Wirtschaft         0              0         1   \n",
       "2                      Konjunktur         0              0         1   \n",
       "3  Wettbewerbsfähigkeit/Nachfrage         1              3         0   \n",
       "4       Internationale Wirtschaft         0              1         0   \n",
       "\n",
       "   Number_of_reports AverageRating  \n",
       "0                  1             0  \n",
       "1                  1           100  \n",
       "2                  1           100  \n",
       "3                  4           -25  \n",
       "4                  1             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset acquired from Media Tenor\n",
    "sentiment_data = pd.read_csv('Daten_Wirtschaftliche_Lage.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "# Filter out rows with empty titles, as we cannot identify and download the articles without titles\n",
    "sentiment_data = sentiment_data[sentiment_data['title'].notnull()]\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The titles in the Media Tenor dataset were manually entered, leading to potential inconsistencies in punctuation and spacing. To address this issue and ensure accurate matching with the titles of the articles we scrape from the website, we normalize the titles in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['koalition', 'habt bloß keine angst vor china', 'wir leben in einer zeit der wohlstands halluzination']\n"
     ]
    }
   ],
   "source": [
    "# Import the Normalize class from the normalize module\n",
    "from normalize import Normalize\n",
    "\n",
    "# Initialize the Normalize class with the titles from the sentiment_data DataFrame\n",
    "normalizer = Normalize(sentiment_data.title)\n",
    "\n",
    "# Apply the normalization process to the titles\n",
    "normalized_titles = normalizer.normalized()\n",
    "\n",
    "# Print the first three normalized titles to verify the results\n",
    "print(normalized_titles[:3])\n",
    "\n",
    "# Add the normalized titles to the sentiment_data DataFrame as a new column 'title_clean'\n",
    "sentiment_data['title_clean'] = normalized_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to focus on annotated articles from Spiegel related to business cycle conditions, as these are the specific articles we aim to scrape from the website based on their availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.02.2020</td>\n",
       "      <td>202002</td>\n",
       "      <td>Spiegel</td>\n",
       "      <td>Keim der Angst</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-33,33</td>\n",
       "      <td>keim der angst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.02.2020</td>\n",
       "      <td>202002</td>\n",
       "      <td>Spiegel</td>\n",
       "      <td>»Du musst die Gesellschaft verändern wollen«</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-100</td>\n",
       "      <td>du musst die gesellschaft verändern wollen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.04.2017</td>\n",
       "      <td>201704</td>\n",
       "      <td>Spiegel</td>\n",
       "      <td>Eine Stunde Applaus. Und dann?</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>eine stunde applaus und dann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.08.2015</td>\n",
       "      <td>201508</td>\n",
       "      <td>Spiegel</td>\n",
       "      <td>Chinesische Heuschrecke</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-100</td>\n",
       "      <td>chinesische heuschrecke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.07.2013</td>\n",
       "      <td>201307</td>\n",
       "      <td>Spiegel</td>\n",
       "      <td>Spirale nach unten</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-100</td>\n",
       "      <td>spirale nach unten</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month   medium                                         title  \\\n",
       "0  01.02.2020  202002  Spiegel                                Keim der Angst   \n",
       "1  01.02.2020  202002  Spiegel  »Du musst die Gesellschaft verändern wollen«   \n",
       "2  01.04.2017  201704  Spiegel                Eine Stunde Applaus. Und dann?   \n",
       "3  01.08.2015  201508  Spiegel                       Chinesische Heuschrecke   \n",
       "4  01.07.2013  201307  Spiegel                            Spirale nach unten   \n",
       "\n",
       "   topicgroup  negative  no_clear_tone  positive  Number_of_reports  \\\n",
       "0  Konjunktur         1              2         0                  3   \n",
       "1  Konjunktur         1              0         0                  1   \n",
       "2  Konjunktur         0              1         0                  1   \n",
       "3  Konjunktur         1              0         0                  1   \n",
       "4  Konjunktur         1              0         0                  1   \n",
       "\n",
       "  AverageRating                                 title_clean  \n",
       "0        -33,33                              keim der angst  \n",
       "1          -100  du musst die gesellschaft verändern wollen  \n",
       "2             0                eine stunde applaus und dann  \n",
       "3          -100                     chinesische heuschrecke  \n",
       "4          -100                          spirale nach unten  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the dataset to include only articles from Spiegel\n",
    "sentiment_data = sentiment_data[sentiment_data['medium'] == 'Spiegel']\n",
    "\n",
    "# Reset the index of the DataFrame and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Further filter the dataset to include only articles related to the business cycle conditions (Konjunktur)\n",
    "sentiment_data = sentiment_data[sentiment_data['topicgroup'] == 'Konjunktur']\n",
    "\n",
    "# Reset the index of the DataFrame again and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame to verify the results\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping: example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates how to scrape a few articles from a specific issue of Spiegel. In the past, we used similar code to scrape articles from 2011-2016 when all these articles were publicly available without a subscription.\n",
    "Now, a similar approach can be used to scrape articles up to and including 2009, which are still available without a subscription.\n",
    "\n",
    "This example focuses on scraping the first few articles from the 46th issue of Spiegel in 2008. It involves extracting article links from the issue's index page, opening each link, and retrieving the article's title, publication date, and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Content Preview:\n",
      " <!DOCTYPE html>\n",
      "<html lang=\"de\">\n",
      " <head>\n",
      "  <title>\n",
      "   DER SPIEGEL 46/2008 - Inhaltsverzeichnis\n",
      "  </title>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width,initial-scale=1,user-scalable=yes\" name=\"viewport\"/>\n",
      "  <meta content=\"true\" name=\"MSSmartTagsPreventParsing\"/>\n",
      "  <meta content=\"no\" http-equiv=\"imagetoolbar\"/>\n",
      "  <meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n",
      "  <meta content=\"app-id=424881832\" name=\"apple-itunes-app\"/>\n",
      "  <link href=\"https://www.spiegel.de/public/spon/j\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL for a specific issue of Spiegel from 2008\n",
    "spiegel_issue_url = \"https://www.spiegel.de/spiegel/print/index-2008-46.html\"\n",
    "\n",
    "# Open the URL and create a BeautifulSoup object\n",
    "spiegel_issue = urlopen(spiegel_issue_url)\n",
    "spiegel_issue_soup = BeautifulSoup(spiegel_issue, 'html.parser')\n",
    "\n",
    "# Print the first 500 characters of the HTML content for a brief overview\n",
    "print(\"HTML Content Preview:\\n\", spiegel_issue_soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we demonstrate how to extract the issue number and year from the title of a Spiegel issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DER SPIEGEL 46/2008 - Inhaltsverzeichnis\n",
      "DER SPIEGEL\n",
      "46\n",
      "2008\n"
     ]
    }
   ],
   "source": [
    "# Extract the title of the issue\n",
    "title = spiegel_issue_soup.title.string\n",
    "print(title)\n",
    "\n",
    "# Parse details from the issue title\n",
    "journal_title = title.rsplit(' ', 3)[0]\n",
    "issue_number = int(title.rsplit(' ', 3)[1].rsplit('/', 1)[0])\n",
    "year = int(title.rsplit(' ', 3)[1].rsplit('/', 1)[1])\n",
    "\n",
    "# Display parsed details\n",
    "print(journal_title)\n",
    "print(issue_number)\n",
    "print(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the links and titles of all the articles from a particular issue. We find all the `<a>` tags in the HTML document and check if they contain a valid link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Links (first 10): ['https://www.spiegel.de/politik/das-projekt-obama-a-90d661ad-0002-0001-0000-000062013393?context=issue', 'https://www.spiegel.de/politik/das-projekt-obama-a-90d661ad-0002-0001-0000-000062013393?context=issue', 'https://www.spiegel.de/politik/im-gelobten-land-a-0eb2f40e-0002-0001-0000-000062013394?context=issue', 'https://www.spiegel.de/politik/ein-weltkrieg-ohne-krieg-a-79b5c498-0002-0001-0000-000062013395?context=issue', 'https://www.spiegel.de/politik/10-november-2008-titel-a-ea2be9dc-0002-0001-0000-000062013320?context=issue', 'https://www.spiegel.de/politik/10-november-2008-cayman-islands-a-9d2cd94a-0002-0001-0000-000062013321?context=issue', 'https://www.spiegel.de/politik/10-november-2008-eishockey-a-042588f0-0002-0001-0000-000062013322?context=issue', 'https://www.spiegel.de/politik/spaete-einheit-a-bc714e5a-0002-0001-0000-000062013340?context=issue', 'https://www.spiegel.de/politik/aufklaerer-versetzt-a-cb767ecc-0002-0001-0000-000062013341?context=issue', 'https://www.spiegel.de/politik/brandherd-in-schlafkabine-a-527f2f76-0002-0001-0000-000062013342?context=issue']\n",
      "Extracted Headings (first 10): ['Das Projekt Obama', 'Das Projekt Obama', 'Im Gelobten Land', '»Ein Weltkrieg ohne Krieg«', '10. November 2008 Titel', '10. November 2008 Cayman Islands', '10. November 2008 Eishockey', 'Späte Einheit', 'Aufklärer versetzt', 'Brandherd in Schlafkabine?']\n"
     ]
    }
   ],
   "source": [
    "# Extract all article links and their headings from the issue\n",
    "links_journal = []\n",
    "headings = []\n",
    "all_links_spiegel = spiegel_issue_soup.findAll(\"a\")\n",
    "\n",
    "for link in all_links_spiegel:\n",
    "    # Check if there is a link part in the tag\n",
    "    if link.get(\"href\") is not None:\n",
    "        # Check if the link contains 'context=issue'\n",
    "        if link.get(\"href\").find('context=issue') != -1:\n",
    "            # Append the link to links_journal list\n",
    "            links_journal.append(link.get(\"href\"))\n",
    "            # Append the heading to headings list\n",
    "            headings.append(link.get(\"title\"))\n",
    "\n",
    "# Display only the first 10 extracted links and headings for verification\n",
    "print(\"Extracted Links (first 10):\", links_journal[:10])\n",
    "print(\"Extracted Headings (first 10):\", headings[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we show how to download the full texts of articles, as well as their titles and publication dates, from a particular issue. We process each article link, extract the relevant details, and save each article as a TXT file. Additionally, we gather all the articles into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>titles</th>\n",
       "      <th>titles_clean</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09.11.2008</td>\n",
       "      <td>Das Projekt Obama</td>\n",
       "      <td>das projekt obama</td>\n",
       "      <td>Das Projekt Obama. Gewaltig sind die Herausfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09.11.2008</td>\n",
       "      <td>Das Projekt Obama</td>\n",
       "      <td>das projekt obama</td>\n",
       "      <td>Das Projekt Obama. Gewaltig sind die Herausfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09.11.2008</td>\n",
       "      <td>Im Gelobten Land</td>\n",
       "      <td>im gelobten land</td>\n",
       "      <td>Im Gelobten Land. Obamas Sieg ist der Triumph ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09.11.2008</td>\n",
       "      <td>»Ein Weltkrieg ohne Krieg«</td>\n",
       "      <td>ein weltkrieg ohne krieg</td>\n",
       "      <td>»Ein Weltkrieg ohne Krieg«. Der britische Hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09.11.2008</td>\n",
       "      <td>10. November 2008 Titel</td>\n",
       "      <td>10 november 2008 titel</td>\n",
       "      <td>10. November 2008 Titel. Es waren bewegende Sz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dates                      titles              titles_clean  \\\n",
       "0  09.11.2008           Das Projekt Obama         das projekt obama   \n",
       "1  09.11.2008           Das Projekt Obama         das projekt obama   \n",
       "2  09.11.2008            Im Gelobten Land          im gelobten land   \n",
       "3  09.11.2008  »Ein Weltkrieg ohne Krieg«  ein weltkrieg ohne krieg   \n",
       "4  09.11.2008     10. November 2008 Titel    10 november 2008 titel   \n",
       "\n",
       "                                                text  \n",
       "0  Das Projekt Obama. Gewaltig sind die Herausfor...  \n",
       "1  Das Projekt Obama. Gewaltig sind die Herausfor...  \n",
       "2  Im Gelobten Land. Obamas Sieg ist der Triumph ...  \n",
       "3  »Ein Weltkrieg ohne Krieg«. Der britische Hist...  \n",
       "4  10. November 2008 Titel. Es waren bewegende Sz...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the set of punctuation characters to exclude\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Lists to store extracted data\n",
    "texts = []\n",
    "titles = []\n",
    "titles_clean = []\n",
    "dates = []\n",
    "\n",
    "# Identifier for the articles\n",
    "id = 0\n",
    "\n",
    "# Ensure the directory exists for saving articles\n",
    "save_dir = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Spiegel_scrape_2008_txt')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Process each article link (here we limit to the first 5 for demonstration)\n",
    "for link in links_journal[:5]:\n",
    "    id += 1\n",
    "    id_fixed = str(id)\n",
    "    \n",
    "    # Open the article link\n",
    "    try: \n",
    "        article = urlopen(link)\n",
    "        # Create a BeautifulSoup object\n",
    "        article = BeautifulSoup(article, 'html.parser')\n",
    "        # Extract paragraphs with the class 'RichText'\n",
    "        paragraphs = article.findAll(\"div\", {\"class\": \"RichText\"})\n",
    "        \n",
    "        # Proceed only if paragraphs are found\n",
    "        if paragraphs:\n",
    "            # Extract the title\n",
    "            title = article.find(\"span\", {\"class\": \"font-brandUI font-extrabold lg:text-7xl md:text-5xl sm:text-4xl leading-tight\"}).find(\"span\", {\"class\": \"align-middle\"}).get_text()\n",
    "            title = title.strip().replace(\"\\n\", '')\n",
    "            # Normalize the title\n",
    "            # Remove specific punctuation, replace hyphens with spaces, convert to lowercase, and strip leading/trailing spaces\n",
    "            title_clean = ''.join(ch for ch in title.replace('-', ' ').lower() if (ch not in exclude) and (ch not in ['\"', '„', '“', '»', '«'])).strip()\n",
    "            # Standardize spaces to a single space\n",
    "            title_clean = \" \".join(title_clean.split())\n",
    "          \n",
    "            # Extract the publication date\n",
    "            date = article.find(\"time\", {\"class\": \"timeformat\"}).get_text().split(',')[0]\n",
    "\n",
    "            # Extract the text from the paragraphs\n",
    "            text_new = ''\n",
    "\n",
    "            for par in paragraphs:\n",
    "                text_new = text_new + par.get_text()\n",
    "\n",
    "            text_new = text_new.strip().replace(\"\\n\", ' ')\n",
    "\n",
    "            if title[-1] not in ['.', '!', ':', ';','?', '\"']:\n",
    "                text_new = title + '. ' + text_new\n",
    "            else:\n",
    "                text_new = title + ' ' + text_new\n",
    "\n",
    "        else:\n",
    "            text_new = ''\n",
    "            title = ''\n",
    "            title_clean = ''\n",
    "            date = ''\n",
    "        \n",
    "    # Handle any IOError (e.g., page not found)   \n",
    "    except IOError:\n",
    "        text_new = ''\n",
    "        title = ''\n",
    "        title_clean = ''\n",
    "        date = ''\n",
    "        \n",
    "    # Add the extracted data to the lists if the title is found\n",
    "    if title:\n",
    "        texts.append(text_new)\n",
    "        titles.append(title)\n",
    "        titles_clean.append(title_clean)\n",
    "        dates.append(date)\n",
    "        \n",
    "        # Save the article as a .txt file\n",
    "        issue_string = str(issue_number)\n",
    "        year_string = str(year)\n",
    "        file_path = os.path.join(save_dir, f\"{year_string} {issue_string} {id_fixed}.txt\")\n",
    "        with codecs.open(file_path, \"w\", \"utf-8-sig\") as temp:\n",
    "            temp.write(text_new)\n",
    "            \n",
    "# Create a DataFrame from the extracted data\n",
    "data = pd.DataFrame({\n",
    "    'dates': dates,\n",
    "    'titles': titles,\n",
    "    'titles_clean': titles_clean,\n",
    "    'text': texts\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping: issues 1-52 (53), 2011 - 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following code to download the full texts of articles, along with their titles and publication dates, from issues 1-53 for the years 2011-2015. We only scraped an article if its title was present in the Media Tenor dataset. This approach allowed us to collect candidate articles that potentially matched the Media Tenor metadata.\n",
    "\n",
    "We downloaded 572 candidates from 2011-2015 and 182 candidates from 2016, totaling 754 articles. Later, we matched these articles with the Media Tenor metadata based on both title and publication date. As a result, we successfully scraped 505 articles from the Media Tenor dataset. We matched the scraped texts with metadata in the notebook `Spiegel.ipynb`. \n",
    "\n",
    "Although this code cannot be used now to scrape these articles due to the current restriction that only articles up to and including 2009 are available without a subscription, we publish this code to demonstrate our research process and to aid anyone needing articles published in 2009 or earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "# Import codecs in order to save articles as .txt files\n",
    "import codecs\n",
    "from urllib.request import urlopen\n",
    "# Import the library that pulls out HTML data\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the set of punctuation characters to exclude\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Define the directory to save the Excel file\n",
    "save_dir = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva')\n",
    "\n",
    "# Define the path to save the Excel file\n",
    "excel_file_path = os.path.join(save_dir, 'spiegel_2011_2015.xlsx')\n",
    "\n",
    "# Excel file where we save the data\n",
    "data_excel = pd.ExcelWriter(excel_file_path)\n",
    "\n",
    "# Directory to save text files\n",
    "text_save_dir = os.path.join(save_dir, 'Spiegel_scrape_2011_2015_txt')\n",
    "os.makedirs(text_save_dir, exist_ok=True)\n",
    "\n",
    "# Common part in all the links\n",
    "common = \"http://www.spiegel.de/spiegel/print/index-\"\n",
    "magazine_year = 2010\n",
    "issue = 0\n",
    "number_of_rows = 0\n",
    "\n",
    "# Iterate over the years (2011 to 2015)\n",
    "for i in range(0, 5):\n",
    "    magazine_year += 1\n",
    "    magazine_year_fixed = str(magazine_year)\n",
    "    \n",
    "    # Iterate over the issues (1 to 53)\n",
    "    for i in range(0, 53):\n",
    "        issue += 1\n",
    "        # Convert issue number to a zero-padded string\n",
    "        issue_fixed = str(issue).zfill(2)\n",
    "        # Construct the URL for the issue\n",
    "        spiegel_issue = common + magazine_year_fixed + \"-\" + issue_fixed + \".html\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Open the URL and create a BeautifulSoup object\n",
    "            spiegel_issue = urlopen(spiegel_issue)\n",
    "            spiegel_issue_soup = BeautifulSoup(spiegel_issue, 'html.parser')\n",
    "            \n",
    "            # Extract the title of the issue\n",
    "            title = spiegel_issue_soup.title.string\n",
    "            # Extract issue number and year from the title\n",
    "            issue_number = int(title.rsplit(' ', 3)[1].rsplit('/', 1)[0])\n",
    "            year = int(title.rsplit(' ', 3)[1].rsplit('/', 1)[1])\n",
    "            \n",
    "            links_journal = []\n",
    "            # Find all links in the HTML document\n",
    "            all_links_spiegel = spiegel_issue_soup.findAll(\"a\")\n",
    "\n",
    "            # Extract the link of each article and the heading of each article\n",
    "            for link in all_links_spiegel:\n",
    "                # Check if there is a link part in the tag\n",
    "                if link.get(\"href\") is not None:\n",
    "                    # Check if the link contains '/print/d-'\n",
    "                    if link.get(\"href\").find('/print/d-') != -1:\n",
    "                        # Append the link to links_journal list\n",
    "                        links_journal.append(link.get(\"href\"))\n",
    "\n",
    "            texts = []\n",
    "            titles = []\n",
    "            titles_clean = []\n",
    "            dates = []\n",
    "            # Identifier of the article in the name\n",
    "            id = 0\n",
    "            \n",
    "            # Iterate over each article link\n",
    "            for link in links_journal:\n",
    "                id += 1\n",
    "                # Convert id to a string for use in the text document name\n",
    "                id_fixed = str(id)\n",
    "                \n",
    "                try:\n",
    "                    # Open the article URL and create a BeautifulSoup object\n",
    "                    article = urlopen(link)\n",
    "                    article = BeautifulSoup(article, 'html.parser')\n",
    "                    \n",
    "                    # Extract the text\n",
    "                    paragraphs = article.findAll(\"div\", {\"class\": \"RichText\"})\n",
    "                    if paragraphs != []:\n",
    "                        title = article.find(\"span\", {\"class\": \"align-middle\"}).get_text()\n",
    "                        title = title.strip().replace(\"\\n\", '')\n",
    "                        # Normalize the title\n",
    "                        # Remove specific punctuation, replace hyphens with spaces, convert to lowercase, and strip leading/trailing spaces\n",
    "                        title_clean = ''.join(ch for ch in title.replace('-', ' ').lower() if (ch not in exclude) and (ch not in ['\"', '„', '“', '»', '«'])).strip()\n",
    "                        # Standardize spaces to a single space\n",
    "                        title_clean = \" \".join(title_clean.split())\n",
    "                                              \n",
    "                        # Check if the normalized title of the current article is among the normalized titles of articles \n",
    "                        # from the MediaTenor dataset\n",
    "                        if any(title_clean in s for s in sentiment_data['title_clean']):\n",
    "                            \n",
    "                            # Extract the publication date\n",
    "                            date = article.find(\"time\", {\"class\": \"timeformat\"}).get_text()\n",
    "                            \n",
    "                            # Extract the text from the paragraphs\n",
    "                            text_new = ''\n",
    "                            for par in paragraphs:\n",
    "                                text_new = text_new + par.get_text()\n",
    "                            text_new = text_new.strip().replace(\"\\n\", ' ')\n",
    "                            if title[-1] not in ['.', '!', ':', ';','?', '\"']:\n",
    "                                text_new = title + '. ' + text_new\n",
    "                            else:\n",
    "                                text_new = title + ' ' + text_new\n",
    "                                \n",
    "                    else:\n",
    "                        text_new = ''\n",
    "                        title = ''\n",
    "                        title_clean = ''\n",
    "                        date = ''\n",
    "\n",
    "                # Handle errors (e.g., if the page doesn't exist)\n",
    "                except IOError:\n",
    "                    text_new = ''\n",
    "                    title = ''\n",
    "                    title_clean = ''\n",
    "                    date = ''\n",
    "\n",
    "                # Add the extracted data to the lists if the title is valid and exists in sentiment_data\n",
    "                if any(title_clean in s for s in sentiment_data['title_clean']) and (title != ''):\n",
    "                    texts.append(text_new)\n",
    "                    titles.append(title)\n",
    "                    titles_clean.append(title_clean)\n",
    "                    dates.append(date)\n",
    "                    \n",
    "                    # Save the article as a .txt file\n",
    "                    issue_string = str(issue_number)\n",
    "                    year_string = str(year)\n",
    "                    file_path = os.path.join(text_save_dir, f\"{year_string} {issue_string} {id_fixed}.txt\")\n",
    "                    with codecs.open(file_path, \"w\", \"utf-8-sig\") as temp:\n",
    "                        temp.write(text_new) \n",
    "                        \n",
    "            # Create a DataFrame from the extracted data               \n",
    "            data = pd.DataFrame({\n",
    "                'dates': dates,\n",
    "                'text': texts,\n",
    "                'titles': titles,\n",
    "                'titles_clean': titles_clean\n",
    "            })            \n",
    "\n",
    "            # Save the data from the current issue to the Excel file\n",
    "            data.to_excel(data_excel, 'Sheet1', header = False, startrow = number_of_rows)\n",
    "            # Update the row counter\n",
    "            number_of_rows = data.shape[0] + number_of_rows\n",
    "            \n",
    "        except IOError:\n",
    "            pass\n",
    "        \n",
    "    # Reset the issue counter for the next year    \n",
    "    issue = 0\n",
    "    \n",
    "# Save the Excel file  \n",
    "data_excel.save()\n",
    "data_excel.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping: issues 1-52 (53), 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2016, an article without a title caused an error during scraping. To handle this, we added a condition to skip articles without titles, ensuring that only articles with both text and titles are processed. This approach prevents errors and ensures that the articles can be matched with the Media Tenor dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the Excel file\n",
    "save_dir = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva')\n",
    "\n",
    "# Define the path to save the Excel file\n",
    "excel_file_path = os.path.join(save_dir, 'spiegel_2016.xlsx')\n",
    "\n",
    "# Excel file where we save the data\n",
    "data_excel_2016 = pd.ExcelWriter(excel_file_path)\n",
    "\n",
    "# Directory to save text files\n",
    "text_save_dir = os.path.join(save_dir, 'Spiegel_scrape_2016_txt')\n",
    "os.makedirs(text_save_dir, exist_ok=True)\n",
    "\n",
    "# Common part in all the links\n",
    "common = \"http://www.spiegel.de/spiegel/print/index-\"\n",
    "magazine_year = 2015\n",
    "issue = 0\n",
    "number_of_rows = 0\n",
    "\n",
    "# Iterate over the years (2016)\n",
    "for i in range(0, 1):\n",
    "    magazine_year += 1\n",
    "    magazine_year_fixed = str(magazine_year)\n",
    "    \n",
    "    # Iterate over the issues (1 to 53)\n",
    "    for i in range(0, 53):\n",
    "        issue += 1\n",
    "        # Convert issue number to a zero-padded string\n",
    "        issue_fixed = str(issue).zfill(2)\n",
    "        # Construct the URL for the issue\n",
    "        spiegel_issue = common + magazine_year_fixed + \"-\" + issue_fixed + \".html\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Open the URL and create a BeautifulSoup object\n",
    "            spiegel_issue = urlopen(spiegel_issue)\n",
    "            spiegel_issue_soup = BeautifulSoup(spiegel_issue, 'html.parser')\n",
    "            \n",
    "            # Extract the title of the issue\n",
    "            title = spiegel_issue_soup.title.string\n",
    "            # Extract issue number and year from the title\n",
    "            issue_number = int(title.rsplit(' ', 3)[1].rsplit('/', 1)[0])\n",
    "            year = int(title.rsplit(' ', 3)[1].rsplit('/', 1)[1])\n",
    "            \n",
    "            links_journal = []\n",
    "            # Find all links in the HTML document\n",
    "            all_links_spiegel = spiegel_issue_soup.findAll(\"a\")\n",
    "\n",
    "            # Extract the link of each article and the heading of each article\n",
    "            for link in all_links_spiegel:\n",
    "                # Check if there is a link part in the tag\n",
    "                if link.get(\"href\") is not None:\n",
    "                    # Check if the link contains '/print/d-'\n",
    "                    if link.get(\"href\").find('/print/d-') != -1:\n",
    "                        # Append the link to links_journal list\n",
    "                        links_journal.append(link.get(\"href\"))\n",
    "                        \n",
    "            texts = []\n",
    "            titles = []\n",
    "            titles_clean = []\n",
    "            dates = []\n",
    "            # Identifier of the article in the name\n",
    "            id = 0\n",
    "            \n",
    "            # Iterate over each article link\n",
    "            for link in links_journal:\n",
    "                id += 1\n",
    "                # Convert id to a string for use in the text document name\n",
    "                id_fixed = str(id)\n",
    "                \n",
    "                try:\n",
    "                    # Open the article URL and create a BeautifulSoup object\n",
    "                    article = urlopen(link)\n",
    "                    article = BeautifulSoup(article, 'html.parser')\n",
    "                    \n",
    "                    # Extract the text\n",
    "                    paragraphs = article.findAll(\"div\", {\"class\": \"RichText\"})\n",
    "                    # NEW CONDITION\n",
    "                    # Check if the paragraphs list is not empty and the article has a title element with the class \"align-middle\"\n",
    "                    if (paragraphs != []) and (article.find(\"span\", {\"class\": \"align-middle\"}) is not None):\n",
    "\n",
    "                        title = article.find(\"span\", {\"class\": \"align-middle\"}).get_text()\n",
    "                        title = title.strip().replace(\"\\n\", '')\n",
    "                        # Normalize the title\n",
    "                        # Remove specific punctuation, replace hyphens with spaces, convert to lowercase, and strip leading/trailing spaces\n",
    "                        title_clean = ''.join(ch for ch in title.replace('-', ' ').lower() if (ch not in exclude) and (ch not in ['\"', '„', '“', '»', '«'])).strip()\n",
    "                        # Standardize spaces to a single space\n",
    "                        title_clean = \" \".join(title_clean.split())\n",
    "                        \n",
    "                        # Check if the normalized title of the current article is among the normalized titles of articles \n",
    "                        # from the MediaTenor dataset     \n",
    "                        if any(title_clean in s for s in sentiment_data['title_clean']):\n",
    "                            \n",
    "                             # Extract the publication date\n",
    "                            date = article.find(\"time\", {\"class\": \"timeformat\"}).get_text()\n",
    "                            \n",
    "                            # Extract the text from the paragraphs\n",
    "                            text_new = ''\n",
    "                            for par in paragraphs:\n",
    "                                text_new = text_new + par.get_text()\n",
    "                            text_new = text_new.strip().replace(\"\\n\", ' ')\n",
    "                            if title[-1] not in ['.', '!', ':', ';','?', '\"']:\n",
    "                                text_new = title + '. ' + text_new\n",
    "                            else:\n",
    "                                text_new = title + ' ' + text_new\n",
    "\n",
    "                    else:\n",
    "                        text_new = ''\n",
    "                        title = ''\n",
    "                        title_clean = ''\n",
    "                        date = ''\n",
    "\n",
    "                # Handle errors (e.g., if the page doesn't exist)   \n",
    "                except IOError:\n",
    "                    text_new = ''\n",
    "                    title = ''\n",
    "                    title_clean = ''\n",
    "                    date = ''\n",
    "\n",
    "                # Add the extracted data to the lists if the title is valid and exists in sentiment_data\n",
    "                if any(title_clean in s for s in sentiment_data['title_clean']) and (title != ''):\n",
    "                    texts.append(text_new)\n",
    "                    titles.append(title)\n",
    "                    titles_clean.append(title_clean)\n",
    "                    dates.append(date)\n",
    "                    \n",
    "                    # Save the article as a .txt file\n",
    "                    issue_string = str(issue_number)\n",
    "                    year_string = str(year)\n",
    "                    file_path = os.path.join(text_save_dir, f\"{year_string} {issue_string} {id_fixed}.txt\")\n",
    "                    with codecs.open(file_path, \"w\", \"utf-8-sig\") as temp:\n",
    "                        temp.write(text_new) \n",
    "                        \n",
    "            # Create a DataFrame from the extracted data                 \n",
    "            data = pd.DataFrame({\n",
    "                'dates': dates,\n",
    "                'text': texts,\n",
    "                'titles': titles,\n",
    "                'titles_clean': titles_clean\n",
    "            })       \n",
    "\n",
    "            # Save the data from the current issue to the Excel file\n",
    "            data.to_excel(data_excel_2016, 'Sheet1', header = False, startrow = number_of_rows)\n",
    "            # Update the row counter\n",
    "            number_of_rows = data.shape[0] + number_of_rows\n",
    "            \n",
    "        except IOError:\n",
    "            pass\n",
    "    \n",
    "    # Reset the issue counter for the next year\n",
    "    issue = 0\n",
    "    \n",
    "# Save the Excel file\n",
    "data_excel_2016.save()\n",
    "data_excel_2016.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
