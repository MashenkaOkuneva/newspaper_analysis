{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58734ee6",
   "metadata": {},
   "source": [
    "In this notebook, we perform the following steps:\n",
    "\n",
    "1. **Match Articles from Focus website**: \n",
    "   - Initially, we match 223 articles scraped from the Focus website, as detailed in the notebook **Web scraping from Focus.ipynb**, with their corresponding metadata from the Media Tenor dataset, including sentiment annotations.\n",
    "   \n",
    "2. **Identify and Download Missing Articles**:\n",
    "   - We identify 507 articles published between 2011-2019 that were annotated by Media Tenor but were not available online and could only be found in the print version of the journal. We attempted to download them from Factiva and LexisNexis depending on their availability.\n",
    "\n",
    "3. **Load and Match Articles from Factiva**: \n",
    "   - We begin by downloading 390 RTF files of Focus articles from Factiva. These RTF files are then converted to TXT format. After the conversion, we load the articles from the TXT files and match them with their metadata.\n",
    "\n",
    "4. **Load and Match Articles from LexisNexis**: \n",
    "   - Similarly, we download 106 RTF files of Focus articles from LexisNexis. These RTF files are converted to TXT format, and the articles are then loaded and matched with their sentiment annotations.\n",
    "\n",
    "5. **Combine All Articles**: \n",
    "   - Finally, we combine all these articles into one dataset and save it as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f92bde",
   "metadata": {},
   "source": [
    "## Media Tenor dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccfa320",
   "metadata": {},
   "source": [
    "To match the articles scraped from Focus website or downloaded from Factiva and LexisNexis with their metadata from the Media Tenor dataset, we first need to load the Media Tenor dataset. We only retain articles with non-empty titles, as it is not possible to identify and download articles without titles. Additionally, we remove one article published in 2002, which was included in the file by mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47f276c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2014</td>\n",
       "      <td>201401</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Koalition</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>FAS</td>\n",
       "      <td>Habt bloß keine Angst vor China !</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Wir leben in einer Zeit der Wohlstands-Halluzi...</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.02.2015</td>\n",
       "      <td>201502</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Teheran ruft</td>\n",
       "      <td>Wettbewerbsfähigkeit/Nachfrage</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-25</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Geht es und wirklich so gut, wie es uns Merkel...</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month medium  \\\n",
       "0  01.01.2014  201401   WamS   \n",
       "1  01.01.2017  201701    FAS   \n",
       "2  01.01.2017  201701   BamS   \n",
       "3  01.02.2015  201502   WamS   \n",
       "4  01.01.2017  201701   BamS   \n",
       "\n",
       "                                               title  \\\n",
       "0                                          Koalition   \n",
       "1                  Habt bloß keine Angst vor China !   \n",
       "2  Wir leben in einer Zeit der Wohlstands-Halluzi...   \n",
       "3                                       Teheran ruft   \n",
       "4  Geht es und wirklich so gut, wie es uns Merkel...   \n",
       "\n",
       "                       topicgroup  negative  no_clear_tone  positive  \\\n",
       "0                      Konjunktur         0              1         0   \n",
       "1       Internationale Wirtschaft         0              0         1   \n",
       "2                      Konjunktur         0              0         1   \n",
       "3  Wettbewerbsfähigkeit/Nachfrage         1              3         0   \n",
       "4       Internationale Wirtschaft         0              1         0   \n",
       "\n",
       "   Number_of_reports AverageRating  year  \n",
       "0                  1             0  2014  \n",
       "1                  1           100  2017  \n",
       "2                  1           100  2017  \n",
       "3                  4           -25  2015  \n",
       "4                  1             0  2017  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset acquired from Media Tenor\n",
    "sentiment_data = pd.read_csv('Daten_Wirtschaftliche_Lage.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "# Filter out rows with empty titles, as we cannot identify and download the articles without titles\n",
    "sentiment_data = sentiment_data[sentiment_data['title'].notnull()]\n",
    "\n",
    "def year(row):\n",
    "    '''\n",
    "    Extract the year from the 'date' column.\n",
    "    '''\n",
    "    # Split the date string by '.' and take the third part, then convert it to an integer\n",
    "    return int(row['date'].split('.')[2])\n",
    "\n",
    "# Apply the 'year' function to each row of the DataFrame and create a new 'year' column\n",
    "# The 'axis=1' parameter indicates that the function is applied to each row\n",
    "sentiment_data['year'] = sentiment_data.apply(lambda row: year(row), axis=1)\n",
    "\n",
    "# Remove articles from the 'sentiment_data' DataFrame where the year is 2002\n",
    "sentiment_data = sentiment_data[sentiment_data['year'] != 2002]\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd93d1",
   "metadata": {},
   "source": [
    "The titles in the Media Tenor dataset were manually entered, leading to potential inconsistencies in punctuation and spacing. To address this issue and ensure accurate matching with the titles of the articles we scrape from the website or download from databases, we normalize the titles in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8701c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "      <th>year</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2014</td>\n",
       "      <td>201401</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Koalition</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>koalition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>FAS</td>\n",
       "      <td>Habt bloß keine Angst vor China !</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2017</td>\n",
       "      <td>habt bloß keine angst vor china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Wir leben in einer Zeit der Wohlstands-Halluzi...</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2017</td>\n",
       "      <td>wir leben in einer zeit der wohlstands halluzi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.02.2015</td>\n",
       "      <td>201502</td>\n",
       "      <td>WamS</td>\n",
       "      <td>Teheran ruft</td>\n",
       "      <td>Wettbewerbsfähigkeit/Nachfrage</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-25</td>\n",
       "      <td>2015</td>\n",
       "      <td>teheran ruft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2017</td>\n",
       "      <td>201701</td>\n",
       "      <td>BamS</td>\n",
       "      <td>Geht es und wirklich so gut, wie es uns Merkel...</td>\n",
       "      <td>Internationale Wirtschaft</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>geht es und wirklich so gut wie es uns merkel ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month medium  \\\n",
       "0  01.01.2014  201401   WamS   \n",
       "1  01.01.2017  201701    FAS   \n",
       "2  01.01.2017  201701   BamS   \n",
       "3  01.02.2015  201502   WamS   \n",
       "4  01.01.2017  201701   BamS   \n",
       "\n",
       "                                               title  \\\n",
       "0                                          Koalition   \n",
       "1                  Habt bloß keine Angst vor China !   \n",
       "2  Wir leben in einer Zeit der Wohlstands-Halluzi...   \n",
       "3                                       Teheran ruft   \n",
       "4  Geht es und wirklich so gut, wie es uns Merkel...   \n",
       "\n",
       "                       topicgroup  negative  no_clear_tone  positive  \\\n",
       "0                      Konjunktur         0              1         0   \n",
       "1       Internationale Wirtschaft         0              0         1   \n",
       "2                      Konjunktur         0              0         1   \n",
       "3  Wettbewerbsfähigkeit/Nachfrage         1              3         0   \n",
       "4       Internationale Wirtschaft         0              1         0   \n",
       "\n",
       "   Number_of_reports AverageRating  year  \\\n",
       "0                  1             0  2014   \n",
       "1                  1           100  2017   \n",
       "2                  1           100  2017   \n",
       "3                  4           -25  2015   \n",
       "4                  1             0  2017   \n",
       "\n",
       "                                         title_clean  \n",
       "0                                          koalition  \n",
       "1                    habt bloß keine angst vor china  \n",
       "2  wir leben in einer zeit der wohlstands halluzi...  \n",
       "3                                       teheran ruft  \n",
       "4  geht es und wirklich so gut wie es uns merkel ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Normalize class from the normalize module\n",
    "from normalize import Normalize\n",
    "\n",
    "# Initialize the Normalize class with the titles from the sentiment_data DataFrame\n",
    "normalizer = Normalize(sentiment_data.title)\n",
    "\n",
    "# Apply the normalization process to the titles\n",
    "normalized_titles = normalizer.normalized()\n",
    "\n",
    "# Add the normalized titles to the sentiment_data DataFrame as a new column 'title_clean'\n",
    "sentiment_data['title_clean'] = normalized_titles\n",
    "\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c012e5e",
   "metadata": {},
   "source": [
    "We need to focus on annotated articles from Focus related to business cycle conditions, as these are the specific articles we scraped from the website or downloaded from the databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9640593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include only articles from Focus\n",
    "sentiment_data = sentiment_data[sentiment_data['medium'] == 'Focus']\n",
    "\n",
    "# Reset the index of the DataFrame and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Further filter the dataset to include only articles related to the business cycle conditions (Konjunktur)\n",
    "sentiment_data = sentiment_data[sentiment_data['topicgroup'] == 'Konjunktur']\n",
    "\n",
    "# Reset the index of the DataFrame again and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908862dd",
   "metadata": {},
   "source": [
    "We filter the Media Tenor dataset to only keep articles where there was agreement between annotators on sentiment. Articles without annotator agreement (i.e., where `sentiment` is `NaN`) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1020545c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "      <th>year</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.07.2017</td>\n",
       "      <td>201707</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Läuft.</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2017</td>\n",
       "      <td>läuft</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.04.2017</td>\n",
       "      <td>201704</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Anlegen in Zeiten von Trump</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>2017</td>\n",
       "      <td>anlegen in zeiten von trump</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.07.2013</td>\n",
       "      <td>201307</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Gewinne trotz Wackelbörse</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>2013</td>\n",
       "      <td>gewinne trotz wackelbörse</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.10.2011</td>\n",
       "      <td>201110</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Macht Europa nicht kaputt</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>macht europa nicht kaputt</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.10.2012</td>\n",
       "      <td>201210</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Der Kandidat sucht aus</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>der kandidat sucht aus</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   month medium                        title  topicgroup  \\\n",
       "0  01.07.2017  201707  Focus                       Läuft.  Konjunktur   \n",
       "1  01.04.2017  201704  Focus  Anlegen in Zeiten von Trump  Konjunktur   \n",
       "2  01.07.2013  201307  Focus    Gewinne trotz Wackelbörse  Konjunktur   \n",
       "3  01.10.2011  201110  Focus    Macht Europa nicht kaputt  Konjunktur   \n",
       "4  01.10.2012  201210  Focus       Der Kandidat sucht aus  Konjunktur   \n",
       "\n",
       "   negative  no_clear_tone  positive  Number_of_reports AverageRating  year  \\\n",
       "0         0              0         1                  1           100  2017   \n",
       "1         0              0         2                  2           100  2017   \n",
       "2         0              0         3                  3           100  2013   \n",
       "3         0              1         0                  1             0  2011   \n",
       "4         0              1         0                  1             0  2012   \n",
       "\n",
       "                   title_clean  sentiment  \n",
       "0                        läuft        1.0  \n",
       "1  anlegen in zeiten von trump        1.0  \n",
       "2    gewinne trotz wackelbörse        1.0  \n",
       "3    macht europa nicht kaputt        0.0  \n",
       "4       der kandidat sucht aus        0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentiment import sentiment\n",
    "\n",
    "# Apply the 'sentiment' function to each row of the DataFrame and create a new 'sentiment' column\n",
    "sentiment_data['sentiment'] = sentiment_data.apply(lambda row: sentiment(row), axis=1)\n",
    "\n",
    "# Remove articles where there is no annotator agreement (i.e., sentiment is NaN)\n",
    "sentiment_data = sentiment_data[sentiment_data['sentiment'].notnull()]\n",
    "\n",
    "# Reset the index of the DataFrame again and remove the old index column\n",
    "sentiment_data = sentiment_data.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame to verify the results\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9e04",
   "metadata": {},
   "source": [
    "## Match Articles from Focus website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea2293",
   "metadata": {},
   "source": [
    "Next, we load the articles that we scraped from Focus website and saved in the file `focus_2011_2019.csv`. These articles are read into a DataFrame named `focus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1124d896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.11.2013</td>\n",
       "      <td>NOTIZEN AUS DER WIRTSCHAFT. Der Favoritenwechs...</td>\n",
       "      <td>NOTIZEN AUS DER WIRTSCHAFT</td>\n",
       "      <td>notizen aus der wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.11.2013</td>\n",
       "      <td>MONTAG IST ZEUGNISTAG. WOLFGANG MAYRHUBER Mill...</td>\n",
       "      <td>MONTAG IST ZEUGNISTAG</td>\n",
       "      <td>montag ist zeugnistag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.11.2013</td>\n",
       "      <td>NOTIZEN AUS DER WIRTSCHAFT. Die Grafit-Aktie ....</td>\n",
       "      <td>NOTIZEN AUS DER WIRTSCHAFT</td>\n",
       "      <td>notizen aus der wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.11.2013</td>\n",
       "      <td>NOTIZEN AUS DER WIRTSCHAFT. Der japanische Yen...</td>\n",
       "      <td>NOTIZEN AUS DER WIRTSCHAFT</td>\n",
       "      <td>notizen aus der wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.11.2013</td>\n",
       "      <td>MONTAG IST ZEUGNISTAG. BIRGIT FISCHER Insiderw...</td>\n",
       "      <td>MONTAG IST ZEUGNISTAG</td>\n",
       "      <td>montag ist zeugnistag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               text  \\\n",
       "0  15.11.2013  NOTIZEN AUS DER WIRTSCHAFT. Der Favoritenwechs...   \n",
       "1  15.11.2013  MONTAG IST ZEUGNISTAG. WOLFGANG MAYRHUBER Mill...   \n",
       "2  15.11.2013  NOTIZEN AUS DER WIRTSCHAFT. Die Grafit-Aktie ....   \n",
       "3  15.11.2013  NOTIZEN AUS DER WIRTSCHAFT. Der japanische Yen...   \n",
       "4  15.11.2013  MONTAG IST ZEUGNISTAG. BIRGIT FISCHER Insiderw...   \n",
       "\n",
       "                        title                 title_clean  \n",
       "0  NOTIZEN AUS DER WIRTSCHAFT  notizen aus der wirtschaft  \n",
       "1       MONTAG IST ZEUGNISTAG       montag ist zeugnistag  \n",
       "2  NOTIZEN AUS DER WIRTSCHAFT  notizen aus der wirtschaft  \n",
       "3  NOTIZEN AUS DER WIRTSCHAFT  notizen aus der wirtschaft  \n",
       "4       MONTAG IST ZEUGNISTAG       montag ist zeugnistag  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path for a CSV file\n",
    "path_focus = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'focus_2011_2019.csv')\n",
    "\n",
    "# Load the articles scraped from Focus website\n",
    "focus = pd.read_csv(path_focus, encoding='utf-8', sep=';', names=[\"date\", \"text\", \"title\", \"title_clean\"])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "focus = focus.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "focus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4213163",
   "metadata": {},
   "source": [
    "Now we merge these articles with their metadata from the Media Tenor dataset. We start by performing an initial merge based on the exact `title_clean` and `date`. If an exact match is not found, we adjust the publication date of the scraped articles by one or two days forward and backward, and attempt to match again. This adjustment is necessary because Media Tenor annotated articles from the print version of Focus, while the online version may have the same content but slightly different publication dates. By accounting for this possible discrepancy, we ensure accurate matching. The final merged DataFrame includes columns for the journal's name, publication date (day, month, and year), article title, text, sentiment, and file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833c3980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>file</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Focus</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>Die große Koalition der Geldausgeber</td>\n",
       "      <td>Die große Koalition der Geldausgeber. Mehr Job...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>die große koalition der geldausgeber</td>\n",
       "      <td>18.11.2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Focus</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zahlen aus der Wirtschaft</td>\n",
       "      <td>Zahlen aus der Wirtschaft. Auf 1,20 US-Dollar ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>zahlen aus der wirtschaft</td>\n",
       "      <td>18.11.2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Focus</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>\"Das erinnert mich an Weihnachten\"</td>\n",
       "      <td>\"Das erinnert mich an Weihnachten\" Der Wirtsch...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>das erinnert mich an weihnachten</td>\n",
       "      <td>18.11.2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>\"Abgerechnet wird am Schluss\"</td>\n",
       "      <td>\"Abgerechnet wird am Schluss\" Der stellvertret...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>abgerechnet wird am schluss</td>\n",
       "      <td>25.11.2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>Vor neuem Höhenflug?</td>\n",
       "      <td>Vor neuem Höhenflug? Der japanische Nikkei-Ind...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>vor neuem höhenflug</td>\n",
       "      <td>25.11.2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  journal  day  month  year                                 title  \\\n",
       "0   Focus   18     11  2013  Die große Koalition der Geldausgeber   \n",
       "1   Focus   18     11  2013             Zahlen aus der Wirtschaft   \n",
       "2   Focus   18     11  2013    \"Das erinnert mich an Weihnachten\"   \n",
       "3   Focus   25     11  2013         \"Abgerechnet wird am Schluss\"   \n",
       "4   Focus   25     11  2013                  Vor neuem Höhenflug?   \n",
       "\n",
       "                                                text  sentiment  \\\n",
       "0  Die große Koalition der Geldausgeber. Mehr Job...        1.0   \n",
       "1  Zahlen aus der Wirtschaft. Auf 1,20 US-Dollar ...        1.0   \n",
       "2  \"Das erinnert mich an Weihnachten\" Der Wirtsch...        1.0   \n",
       "3  \"Abgerechnet wird am Schluss\" Der stellvertret...        1.0   \n",
       "4  Vor neuem Höhenflug? Der japanische Nikkei-Ind...        1.0   \n",
       "\n",
       "                  file                           title_clean        date  \n",
       "0  focus_2011_2019.csv  die große koalition der geldausgeber  18.11.2013  \n",
       "1  focus_2011_2019.csv             zahlen aus der wirtschaft  18.11.2013  \n",
       "2  focus_2011_2019.csv      das erinnert mich an weihnachten  18.11.2013  \n",
       "3  focus_2011_2019.csv           abgerechnet wird am schluss  25.11.2013  \n",
       "4  focus_2011_2019.csv                   vor neuem höhenflug  25.11.2013  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def adjust_date(date_str, delta):\n",
    "    \n",
    "    '''Function to adjust the publication date'''\n",
    "    \n",
    "    # Convert the date string into a datetime object\n",
    "    date = datetime.strptime(date_str, '%d.%m.%Y')\n",
    "    # Adjust the date by the specified number of days (delta)\n",
    "    adjusted_date = date + pd.Timedelta(days=delta)\n",
    "    \n",
    "    # Convert the adjusted datetime object back into a string in the original format\n",
    "    return adjusted_date.strftime('%d.%m.%Y')\n",
    "\n",
    "# Initial matching based on exact date\n",
    "data_match_scraped = pd.merge(sentiment_data, focus, how='inner', on=['title_clean', 'date'])\n",
    "\n",
    "# Date adjustments: iterate through the delta values\n",
    "for delta in [1, -1, 2, -2]:\n",
    "    # Adjust dates in the 'focus' DataFrame\n",
    "    focus['adjusted_date'] = focus['date'].apply(lambda date: adjust_date(date, delta))\n",
    "    # Perform the merge with the adjusted dates\n",
    "    matched_data = pd.merge(sentiment_data, focus, how='inner', left_on=['title_clean', 'date'], right_on=['title_clean', 'adjusted_date'])\n",
    "    # Rename the 'date_x' column to 'date'\n",
    "    matched_data = matched_data.rename(columns={'date_x': 'date'})\n",
    "    # Drop the 'date_y' and 'adjusted_date' columns\n",
    "    matched_data = matched_data.drop(columns=['date_y', 'adjusted_date'])\n",
    "    # Append the new matches to the main DataFrame\n",
    "    data_match_scraped = pd.concat([data_match_scraped, matched_data], ignore_index=True)\n",
    "    \n",
    "# Remove duplicates\n",
    "data_match_scraped = data_match_scraped.drop_duplicates(['date', 'title_clean'])\n",
    "\n",
    "# Rename columns and split date\n",
    "# Rename the 'medium' column to 'journal'\n",
    "data_match_scraped = data_match_scraped.rename(columns={'medium': 'journal'})\n",
    "# Split 'date' into 'day', 'month', and 'year'\n",
    "data_match_scraped['date'] = pd.to_datetime(data_match_scraped['date'], format='%d.%m.%Y')\n",
    "data_match_scraped['day'] = data_match_scraped['date'].dt.day\n",
    "data_match_scraped['month'] = data_match_scraped['date'].dt.month\n",
    "data_match_scraped['year'] = data_match_scraped['date'].dt.year\n",
    "# Create 'file' column that contains the name of the CSV file\n",
    "data_match_scraped['file'] = 'focus_2011_2019.csv'\n",
    "# Rename the 'title_y' column to 'title' to reflect the title from the Focus dataset\n",
    "data_match_scraped = data_match_scraped.rename(columns={'title_y': 'title'})\n",
    "# Convert the datetime object back into a string in the original format\n",
    "data_match_scraped['date'] = data_match_scraped['date'].dt.strftime('%d.%m.%Y')\n",
    "# Reorder columns\n",
    "columns_order = ['journal', 'day', 'month', 'year', 'title', 'text', 'sentiment', 'file', 'title_clean', 'date']\n",
    "data_match_scraped = data_match_scraped[columns_order]\n",
    "\n",
    "# Sort data chronologically and reset index\n",
    "data_match_scraped = data_match_scraped.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "data_match_scraped = data_match_scraped.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows\n",
    "data_match_scraped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acaf6f4",
   "metadata": {},
   "source": [
    "Next, we remove 14 matched articles with publication dates and titles listed in `pairs_to_remove` as they were accidentally downloaded twice. These articles were both scraped from the website and downloaded from LexisNexis or Factiva. Since the versions from the databases generally have better formatting, we prefer them. Therefore, we drop the 14 duplicates that were scraped from the website. As a final result, we successfully scraped **223** articles from the Focus website and matched them with their sentiment annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7373f38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matched articles: 223\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>file</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Focus</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>Die große Koalition der Geldausgeber</td>\n",
       "      <td>Die große Koalition der Geldausgeber. Mehr Job...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>die große koalition der geldausgeber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Focus</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zahlen aus der Wirtschaft</td>\n",
       "      <td>Zahlen aus der Wirtschaft. Auf 1,20 US-Dollar ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>zahlen aus der wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Focus</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>\"Das erinnert mich an Weihnachten\"</td>\n",
       "      <td>\"Das erinnert mich an Weihnachten\" Der Wirtsch...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>das erinnert mich an weihnachten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>\"Abgerechnet wird am Schluss\"</td>\n",
       "      <td>\"Abgerechnet wird am Schluss\" Der stellvertret...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>abgerechnet wird am schluss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>Vor neuem Höhenflug?</td>\n",
       "      <td>Vor neuem Höhenflug? Der japanische Nikkei-Ind...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>vor neuem höhenflug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  journal  day  month  year                                 title  \\\n",
       "0   Focus   18     11  2013  Die große Koalition der Geldausgeber   \n",
       "1   Focus   18     11  2013             Zahlen aus der Wirtschaft   \n",
       "2   Focus   18     11  2013    \"Das erinnert mich an Weihnachten\"   \n",
       "3   Focus   25     11  2013         \"Abgerechnet wird am Schluss\"   \n",
       "4   Focus   25     11  2013                  Vor neuem Höhenflug?   \n",
       "\n",
       "                                                text  sentiment  \\\n",
       "0  Die große Koalition der Geldausgeber. Mehr Job...        1.0   \n",
       "1  Zahlen aus der Wirtschaft. Auf 1,20 US-Dollar ...        1.0   \n",
       "2  \"Das erinnert mich an Weihnachten\" Der Wirtsch...        1.0   \n",
       "3  \"Abgerechnet wird am Schluss\" Der stellvertret...        1.0   \n",
       "4  Vor neuem Höhenflug? Der japanische Nikkei-Ind...        1.0   \n",
       "\n",
       "                  file                           title_clean  \n",
       "0  focus_2011_2019.csv  die große koalition der geldausgeber  \n",
       "1  focus_2011_2019.csv             zahlen aus der wirtschaft  \n",
       "2  focus_2011_2019.csv      das erinnert mich an weihnachten  \n",
       "3  focus_2011_2019.csv           abgerechnet wird am schluss  \n",
       "4  focus_2011_2019.csv                   vor neuem höhenflug  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of specific title and date pairs to be removed\n",
    "pairs_to_remove = [\n",
    "    ('alle drei minuten ein angriff', '02.06.2014'),\n",
    "    ('eine moralische krise', '02.06.2014'),\n",
    "    ('die meister des idealen strahls', '02.06.2014'),\n",
    "    ('mehr können wir uns nicht leisten', '02.06.2014'),\n",
    "    ('was wenn die wirtschaft kaum noch wächst', '02.06.2014'),\n",
    "    ('wer gewinnt wenn sich europa erholt', '12.09.2015'),\n",
    "    ('die schönen aktien und ihre klugen händler', '02.04.2016'),\n",
    "    ('kurstreiber maschinenbau', '02.04.2016'),\n",
    "    ('europa hat harte zeiten vor sich', '17.06.2017'),\n",
    "    ('wer das bier bestellt soll es auch zahlen', '01.09.2018'),\n",
    "    ('veränderungen tun weh zittern ist nicht erlaubt', '01.12.2018'),\n",
    "    ('schutz gegen kursverluste', '01.12.2018'),\n",
    "    ('wir können dagegen ankämpfen', '25.05.2019'),\n",
    "    ('wer nächste woche wichtig wird', '25.05.2019')\n",
    "]\n",
    "\n",
    "# Convert list of pairs to a DataFrame\n",
    "pairs_df = pd.DataFrame(pairs_to_remove, columns=['title_clean', 'date'])\n",
    "\n",
    "# Merge the 'data_match_scraped' with 'pairs_df' to identify rows to be removed\n",
    "merged_df = data_match_scraped.merge(pairs_df, on=['title_clean', 'date'], how='left', indicator=True)\n",
    "\n",
    "# Keep rows that do not appear in the pairs_df\n",
    "data_match_scraped = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Drop the 'date' column\n",
    "data_match_scraped = data_match_scraped.drop(columns=['date'])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "data_match_scraped = data_match_scraped.reset_index(drop=True)\n",
    "\n",
    "# Sort data chronologically and reset index\n",
    "data_match_scraped = data_match_scraped.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "data_match_scraped = data_match_scraped.reset_index(drop=True)\n",
    "\n",
    "# Display the number of matched articles and the first few rows\n",
    "print(f\"Number of matched articles: {len(data_match_scraped)}\")\n",
    "data_match_scraped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461510e",
   "metadata": {},
   "source": [
    "## Identify and Download Missing Articles\n",
    "\n",
    "The following code identifies **507** articles published between 2011-2019 that were annotated by Media Tenor but were not available online and could only be found in the print version of the journal. Since we were unable to scrape these articles from the Focus website, we identified these missing articles and then attempted to download them from Factiva and LexisNexis depending on their availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "887468c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles to download: 507\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>medium</th>\n",
       "      <th>title</th>\n",
       "      <th>topicgroup</th>\n",
       "      <th>negative</th>\n",
       "      <th>no_clear_tone</th>\n",
       "      <th>positive</th>\n",
       "      <th>Number_of_reports</th>\n",
       "      <th>AverageRating</th>\n",
       "      <th>year</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>201101</td>\n",
       "      <td>Focus</td>\n",
       "      <td>7 Aktien- &amp; Fonds-Trends</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>2011</td>\n",
       "      <td>7 aktien fonds trends</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>201101</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Frisches Geld für Aktionäre</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2011</td>\n",
       "      <td>frisches geld für aktionäre</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-02-07</td>\n",
       "      <td>201102</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Das Inflationsgefühl täuscht nicht</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>das inflationsgefühl täuscht nicht</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-03-14</td>\n",
       "      <td>201103</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Japan erschüttert die Welt</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>japan erschüttert die welt</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-03-14</td>\n",
       "      <td>201103</td>\n",
       "      <td>Focus</td>\n",
       "      <td>keine 5 Zeilen (nur Tabellennennung)</td>\n",
       "      <td>Konjunktur</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2011</td>\n",
       "      <td>keine 5 zeilen nur tabellennennung</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   month medium                                 title  topicgroup  \\\n",
       "0 2011-01-03  201101  Focus              7 Aktien- & Fonds-Trends  Konjunktur   \n",
       "1 2011-01-17  201101  Focus           Frisches Geld für Aktionäre  Konjunktur   \n",
       "2 2011-02-07  201102  Focus    Das Inflationsgefühl täuscht nicht  Konjunktur   \n",
       "3 2011-03-14  201103  Focus            Japan erschüttert die Welt  Konjunktur   \n",
       "4 2011-03-14  201103  Focus  keine 5 Zeilen (nur Tabellennennung)  Konjunktur   \n",
       "\n",
       "   negative  no_clear_tone  positive  Number_of_reports AverageRating  year  \\\n",
       "0         0              0         9                  9           100  2011   \n",
       "1         0              0         1                  1           100  2011   \n",
       "2         0              2         0                  2             0  2011   \n",
       "3         0              3         0                  3             0  2011   \n",
       "4         0              0         1                  1           100  2011   \n",
       "\n",
       "                          title_clean  sentiment  \n",
       "0               7 aktien fonds trends        1.0  \n",
       "1         frisches geld für aktionäre        1.0  \n",
       "2  das inflationsgefühl täuscht nicht        0.0  \n",
       "3          japan erschüttert die welt        0.0  \n",
       "4  keine 5 zeilen nur tabellennennung        1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the date string in 'data_match_scraped' DataFrame\n",
    "data_match_scraped['day_str'] = data_match_scraped['day'].astype(str).str.zfill(2)\n",
    "data_match_scraped['month_str'] = data_match_scraped['month'].astype(str).str.zfill(2)\n",
    "data_match_scraped['year_str'] = data_match_scraped['year'].astype(str)\n",
    "data_match_scraped['date'] = data_match_scraped['day_str'] + '.' + data_match_scraped['month_str'] + '.' + data_match_scraped['year_str']\n",
    "\n",
    "# Create tuples of (title_clean, date) for comparison\n",
    "sentiment_data_tuples = sentiment_data[['title_clean', 'date']].apply(tuple, axis=1)\n",
    "data_match_scraped_tuples = data_match_scraped[['title_clean', 'date']].apply(tuple, axis=1)\n",
    "\n",
    "# Identify articles that still need to be downloaded\n",
    "to_download = sentiment_data[~sentiment_data_tuples.isin(data_match_scraped_tuples)]\n",
    "\n",
    "# Reset the index of the DataFrame and remove the old index column\n",
    "to_download = to_download.reset_index(drop=True)\n",
    "\n",
    "# Convert the 'date' column to datetime format for accurate sorting\n",
    "to_download['date'] = pd.to_datetime(to_download['date'], format='%d.%m.%Y')\n",
    "\n",
    "# Sort 'to_download' based on 'date'\n",
    "to_download = to_download.sort_values(by='date')\n",
    "\n",
    "# Reset the index of the DataFrame and remove the old index column\n",
    "to_download = to_download.reset_index(drop=True)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "to_download.to_csv('to_download_focus.csv', encoding='utf-8-sig', sep=',')\n",
    "\n",
    "# Drop the temporary columns\n",
    "data_match_scraped = data_match_scraped.drop(columns=['day_str', 'month_str', 'year_str', 'date'])\n",
    "\n",
    "print(f\"Number of articles to download: {len(to_download)}\")\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the result\n",
    "to_download.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7603ef4",
   "metadata": {},
   "source": [
    "## Load and Match Articles from Factiva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79341826",
   "metadata": {},
   "source": [
    "Next, we focus on loading Focus articles downloaded from Factiva and matching them with their metadata. In our first step, we convert the RTF files into TXT format. All the RTF files are stored in `MediaTenor_LexisNexis_Factiva/Focus_Konjunktur_Factiva_rtf`. The converted TXT files are stored in `MediaTenor_LexisNexis_Factiva/Focus_Konjunktur_Factiva_txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0d2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for converting RTF to TXT\n",
    "from convert_rtf_to_txt import convert_rtf_to_txt\n",
    "\n",
    "# Define paths for Focus RTF and TXT directories\n",
    "focus_rtf_path = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Focus_Konjunktur_Factiva_rtf')\n",
    "focus_txt_path = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Focus_Konjunktur_Factiva_txt')\n",
    "\n",
    "# Convert RTF files to TXT format for Focus\n",
    "convert_rtf_to_txt(focus_rtf_path, focus_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73b73f",
   "metadata": {},
   "source": [
    "As soon as the RTF files were transformed into TXT format, we made a few changes to the TXT files. Specifically, we corrected several titles to ensure accurate spelling and punctuation, which is important for matching them with the metadata from the Media Tenor dataset. For example:\n",
    "- \"Kalter Krieg **2 . 0**\" was corrected to \"Kalter Krieg **2.0**\"\n",
    "- \"Die **Rohstoff- Formel** fürs Depot\" was corrected to \"Die **Rohstoff-Formel** fürs Depot\"\n",
    "\n",
    "Additionally, a few articles were compilations of multiple pieces. In such cases, we manually selected the annotated article from the compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268ccac",
   "metadata": {},
   "source": [
    "Once the TXT files were ready, we used the function `extract_article_data_focus_factiva` to load the text of the articles along with the journal's name, date of publication, title, and file name into a dictionary called `article_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9629e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_article_data_focus_factiva\n",
    "\n",
    "# Read and extract relevant information from TXT files in Focus directory.\n",
    "article_data = extract_article_data_focus_factiva.extract_article_data_focus_factiva(focus_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b591d0",
   "metadata": {},
   "source": [
    "We use the `article_data` dictionary to create a DataFrame `focus_factiva` that includes columns for the journal's name, publication date (day, month, and year), article title, text, and file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "005c7ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Focus</td>\n",
       "      <td>15</td>\n",
       "      <td>Juni</td>\n",
       "      <td>2019</td>\n",
       "      <td>Die Klimakrise ist unser dritter Weltkrieg</td>\n",
       "      <td>Die USA können es sich nicht leisten, auf den ...</td>\n",
       "      <td>Factiva-20200827-1230.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2019</td>\n",
       "      <td>Wer nächste Woche wichtig wird</td>\n",
       "      <td>Der Terminkalender vom 26. bis 31. Mai. So. Eu...</td>\n",
       "      <td>Factiva-20200827-1234.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2019</td>\n",
       "      <td>Kalter Krieg 2.0</td>\n",
       "      <td>US-Präsident Donald Trump drängt Google, alle ...</td>\n",
       "      <td>Factiva-20200827-1237.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2019</td>\n",
       "      <td>Die Crashtest - Dummys der Wall Street</td>\n",
       "      <td>Es deutet wenig darauf hin, dass die Welt viel...</td>\n",
       "      <td>Factiva-20200827-1240.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Focus</td>\n",
       "      <td>11</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2019</td>\n",
       "      <td>Die Billionenkette</td>\n",
       "      <td>Das Blockchain-System wurde durch Kryptowährun...</td>\n",
       "      <td>Factiva-20200827-1241 (1).txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  journal day month  year                                       title  \\\n",
       "0   Focus  15  Juni  2019  Die Klimakrise ist unser dritter Weltkrieg   \n",
       "1   Focus  25   Mai  2019              Wer nächste Woche wichtig wird   \n",
       "2   Focus  25   Mai  2019                            Kalter Krieg 2.0   \n",
       "3   Focus  25   Mai  2019      Die Crashtest - Dummys der Wall Street   \n",
       "4   Focus  11   Mai  2019                          Die Billionenkette   \n",
       "\n",
       "                                                text  \\\n",
       "0  Die USA können es sich nicht leisten, auf den ...   \n",
       "1  Der Terminkalender vom 26. bis 31. Mai. So. Eu...   \n",
       "2  US-Präsident Donald Trump drängt Google, alle ...   \n",
       "3  Es deutet wenig darauf hin, dass die Welt viel...   \n",
       "4  Das Blockchain-System wurde durch Kryptowährun...   \n",
       "\n",
       "                            file  \n",
       "0      Factiva-20200827-1230.txt  \n",
       "1      Factiva-20200827-1234.txt  \n",
       "2      Factiva-20200827-1237.txt  \n",
       "3      Factiva-20200827-1240.txt  \n",
       "4  Factiva-20200827-1241 (1).txt  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the collected data\n",
    "focus_factiva = pd.DataFrame({\n",
    "    'journal': article_data['journal'],\n",
    "    'day': article_data['day'],\n",
    "    'month': article_data['month'],\n",
    "    'year': article_data['year'],\n",
    "    'title': article_data['title'],\n",
    "    'text': article_data['text'],\n",
    "    'file': article_data['file']\n",
    "})\n",
    "\n",
    "focus_factiva.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27997e",
   "metadata": {},
   "source": [
    "To match the full texts of the loaded articles with their sentiment annotations from the Media Tenor dataset, we follow several key steps. First, we create a date in the same format as in the `sentiment_data` DataFrame. Next, we normalize the titles to ensure accurate matching. We also verify that there are no duplicate articles. After pre-processing, we merge the articles loaded from Factiva with their sentiment annotations from the Media Tenor dataset. We then sort the final DataFrame `data_match_factiva` in chronological order and retain only the relevant columns. Through this process, we successfully matched **390** Focus articles from Factiva with their sentiment annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c09beab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles from Factiva: 390\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>file</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Focus</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>2012</td>\n",
       "      <td>Raus oder aus</td>\n",
       "      <td>Muss Griechenland aus dem Euro aussteigen? Geh...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Factiva-20200828-1232.txt</td>\n",
       "      <td>raus oder aus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Focus</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>2012</td>\n",
       "      <td>MONTAG IST ZEUGNISTAG</td>\n",
       "      <td>RÜDIGER GRUBE. Die Bahn rollt - und das besser...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Factiva-20200828-1231 (1).txt</td>\n",
       "      <td>montag ist zeugnistag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Focus</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2012</td>\n",
       "      <td>Kirchhof-Comeback</td>\n",
       "      <td>ALTSCHULDEN ABBAUEN Der frühere Verfassungsric...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Factiva-20200828-1231.txt</td>\n",
       "      <td>kirchhof comeback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Focus</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2012</td>\n",
       "      <td>» Ohne die FDP hätten wir längst Euro - Bonds «</td>\n",
       "      <td>Außenminister Guido Westerwelle (FDP) sieht zu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Factiva-20200828-1230 (1).txt</td>\n",
       "      <td>ohne die fdp hätten wir längst euro bonds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Focus</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2012</td>\n",
       "      <td>Der Traum vom ewigen Urlaub</td>\n",
       "      <td>Ob eine Stunde von zu Hause oder gleich Mallor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Factiva-20200828-1230.txt</td>\n",
       "      <td>der traum vom ewigen urlaub</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  journal  day  month  year                                            title  \\\n",
       "0   Focus   30      7  2012                                    Raus oder aus   \n",
       "1   Focus   30      7  2012                            MONTAG IST ZEUGNISTAG   \n",
       "2   Focus    6      8  2012                                Kirchhof-Comeback   \n",
       "3   Focus    6      8  2012  » Ohne die FDP hätten wir längst Euro - Bonds «   \n",
       "4   Focus    6      8  2012                      Der Traum vom ewigen Urlaub   \n",
       "\n",
       "                                                text  sentiment  \\\n",
       "0  Muss Griechenland aus dem Euro aussteigen? Geh...       -1.0   \n",
       "1  RÜDIGER GRUBE. Die Bahn rollt - und das besser...        0.0   \n",
       "2  ALTSCHULDEN ABBAUEN Der frühere Verfassungsric...        0.0   \n",
       "3  Außenminister Guido Westerwelle (FDP) sieht zu...        1.0   \n",
       "4  Ob eine Stunde von zu Hause oder gleich Mallor...        0.0   \n",
       "\n",
       "                            file                                title_clean  \n",
       "0      Factiva-20200828-1232.txt                              raus oder aus  \n",
       "1  Factiva-20200828-1231 (1).txt                      montag ist zeugnistag  \n",
       "2      Factiva-20200828-1231.txt                          kirchhof comeback  \n",
       "3  Factiva-20200828-1230 (1).txt  ohne die fdp hätten wir längst euro bonds  \n",
       "4      Factiva-20200828-1230.txt                der traum vom ewigen urlaub  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary to transform month name into month number\n",
    "name_to_number = {\n",
    "    u'Januar': '01', u'Februar': '02', u'M\\xe4rz': '03', u'April': '04', u'Mai': '05',\n",
    "    u'Juni': '06', u'Juli': '07', u'August': '08', u'September': '09', u'Oktober': '10',\n",
    "    u'November': '11', u'Dezember': '12'\n",
    "}\n",
    "\n",
    "# Transform month names into month numbers\n",
    "focus_factiva['month_num'] = focus_factiva['month'].map(name_to_number)\n",
    "\n",
    "# Create dictionary to transform single-digit day numbers\n",
    "day_transform = {u'1': '01', u'2': '02', u'3': '03', u'4': '04', u'5': '05', u'6': '06', u'7': '07', u'8': '08', u'9': '09'}\n",
    "\n",
    "# Transform single-digit day numbers into two-digit format\n",
    "focus_factiva['day'] = focus_factiva['day'].map(lambda d: day_transform.get(d, d))\n",
    "\n",
    "# Combine day, month, and year into a date string\n",
    "focus_factiva['date'] = focus_factiva.apply(lambda row: f\"{row['day']}.{row['month_num']}.{row['year']}\", axis=1)\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "focus_factiva = focus_factiva.reset_index(drop=True)\n",
    "\n",
    "# Initialize the Normalize class with the titles from the focus_factiva DataFrame\n",
    "normalizer = Normalize(focus_factiva.title)\n",
    "\n",
    "# Apply the normalization process to the titles\n",
    "normalized_titles = normalizer.normalized()\n",
    "\n",
    "# Add the normalized titles to the focus_factiva DataFrame as a new column 'title_clean'\n",
    "focus_factiva['title_clean'] = normalized_titles\n",
    "\n",
    "# Merge with sentiment_data on title_clean and date\n",
    "data_match_factiva = pd.merge(sentiment_data, focus_factiva, how='inner', on=['title_clean', 'date'])\n",
    "\n",
    "# Rename the 'month_num' column to 'month'\n",
    "data_match_factiva = data_match_factiva.rename(columns={'month_num': 'month'})\n",
    "\n",
    "# Rename the 'year_y' column to 'year'\n",
    "data_match_factiva = data_match_factiva.rename(columns={'year_y': 'year'})\n",
    "\n",
    "# Convert year, month, and day to integers\n",
    "data_match_factiva['year'] = data_match_factiva['year'].astype(int)\n",
    "data_match_factiva['month'] = data_match_factiva['month'].astype(int)\n",
    "data_match_factiva['day'] = data_match_factiva['day'].astype(int)\n",
    "\n",
    "# Sort the data in chronological order\n",
    "data_match_factiva = data_match_factiva.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "data_match_factiva = data_match_factiva.reset_index(drop=True)\n",
    "\n",
    "# Rename the 'title_y' column to 'title' to reflect the title from the Factiva dataset\n",
    "data_match_factiva = data_match_factiva.rename(columns={'title_y': 'title'})\n",
    "\n",
    "# Select only the required columns\n",
    "data_match_factiva = data_match_factiva[['journal', 'day', 'month', 'year', 'title', 'text', 'sentiment', 'file', 'title_clean']]\n",
    "\n",
    "# Print the number of articles from Factiva\n",
    "num_factiva_articles = len(data_match_factiva)\n",
    "\n",
    "print(f\"Number of articles from Factiva: {num_factiva_articles}\")\n",
    "\n",
    "# Display the first few rows of the final matched dataset\n",
    "data_match_factiva.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c41e02",
   "metadata": {},
   "source": [
    "## Load and Match Articles from LexisNexis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dff02b",
   "metadata": {},
   "source": [
    "In this section, we aim to load articles from Focus that were downloaded from LexisNexis and match them with their sentiment annotations. We begin by converting the RTF files into TXT format. The original RTF files are located in `MediaTenor_LexisNexis_Factiva/Focus_Konjunktur_LexisNexis_rtf`, and the resulting TXT files are stored in `MediaTenor_LexisNexis_Factiva/Focus_Konjunktur_LexisNexis_txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ac2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for Focus RTF and TXT directories\n",
    "focus_lexisnexis_rtf_path = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Focus_Konjunktur_LexisNexis_rtf')\n",
    "focus_lexisnexis_txt_path = os.path.join(os.getcwd(), 'MediaTenor_LexisNexis_Factiva', 'Focus_Konjunktur_LexisNexis_txt')\n",
    "\n",
    "# Convert RTF files to TXT format for Focus\n",
    "convert_rtf_to_txt(focus_lexisnexis_rtf_path, focus_lexisnexis_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1733b",
   "metadata": {},
   "source": [
    "Once the RTF files were converted to TXT format, we made some adjustments. Before extracting titles from the 'Search Terms' in our TXT files, we corrected a few titles to ensure they matched those in the Media Tenor dataset. For example, \"**´Krise?** Welche Krise?\" was corrected to \"**Krise?** Welche Krise?\" to ensure accurate matching.\n",
    "\n",
    "Additionally, for some documents, we removed content found at the end of the text that was not part of the main article, such as source information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47010a",
   "metadata": {},
   "source": [
    "After preparing the TXT files, we used the `extract_article_data_focus_lexisnexis` function to load the articles' text, along with the journal name, publication date, title, and file name, into a dictionary called a`article_data_lexisnexis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dc1ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_article_data_focus_lexisnexis\n",
    "\n",
    "# Read and extract relevant information from TXT files in Focus directory.\n",
    "article_data_lexisnexis = extract_article_data_focus_lexisnexis.extract_article_data_focus_lexisnexis(focus_lexisnexis_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7db180",
   "metadata": {},
   "source": [
    "We use the `article_data_lexisnexis` dictionary to create a DataFrame `focus_lexisnexis` that includes columns for the journal's name, publication date (day, month, and year), article title, text, and file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff71092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>Juni</td>\n",
       "      <td>2012</td>\n",
       "      <td>Solides Skandinavien</td>\n",
       "      <td>Zinsjäger können bereits mit überschaubaren Ei...</td>\n",
       "      <td>30 Tipps _ Anleihen und Anleihefonds_ Gesamtti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Focus</td>\n",
       "      <td>19</td>\n",
       "      <td>Dezember</td>\n",
       "      <td>2011</td>\n",
       "      <td>Abgrenzung von den linken Parteien</td>\n",
       "      <td>NEUAUSRICHTUNG Mit einem klaren politischen Ab...</td>\n",
       "      <td>Abgrenzung von den linken Parteien.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Focus</td>\n",
       "      <td>21</td>\n",
       "      <td>Mai</td>\n",
       "      <td>2012</td>\n",
       "      <td>Abrechnung mit dem Euro</td>\n",
       "      <td>Bestsellerautor Thilo Sarrazin analysiert die ...</td>\n",
       "      <td>Abrechnung mit dem Euro.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Focus</td>\n",
       "      <td>27</td>\n",
       "      <td>Dezember</td>\n",
       "      <td>2011</td>\n",
       "      <td>Boom statt kollaps?</td>\n",
       "      <td>Eine Grundschule im Berliner Westen. Generalpr...</td>\n",
       "      <td>Boom statt Kollaps_.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Focus</td>\n",
       "      <td>8</td>\n",
       "      <td>August</td>\n",
       "      <td>2011</td>\n",
       "      <td>Brasilien &amp; Co</td>\n",
       "      <td>SCHWELLENLÄNDER Aktienanleger brauchen derzeit...</td>\n",
       "      <td>Brasilien &amp; Co._ Chancen trotz Panik.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  journal day     month  year                               title  \\\n",
       "0   Focus  25      Juni  2012               Solides Skandinavien    \n",
       "1   Focus  19  Dezember  2011  Abgrenzung von den linken Parteien   \n",
       "2   Focus  21       Mai  2012             Abrechnung mit dem Euro   \n",
       "3   Focus  27  Dezember  2011                 Boom statt kollaps?   \n",
       "4   Focus   8    August  2011                      Brasilien & Co   \n",
       "\n",
       "                                                text  \\\n",
       "0  Zinsjäger können bereits mit überschaubaren Ei...   \n",
       "1  NEUAUSRICHTUNG Mit einem klaren politischen Ab...   \n",
       "2  Bestsellerautor Thilo Sarrazin analysiert die ...   \n",
       "3  Eine Grundschule im Berliner Westen. Generalpr...   \n",
       "4  SCHWELLENLÄNDER Aktienanleger brauchen derzeit...   \n",
       "\n",
       "                                                file  \n",
       "0  30 Tipps _ Anleihen und Anleihefonds_ Gesamtti...  \n",
       "1             Abgrenzung von den linken Parteien.txt  \n",
       "2                        Abrechnung mit dem Euro.txt  \n",
       "3                            Boom statt Kollaps_.txt  \n",
       "4           Brasilien & Co._ Chancen trotz Panik.txt  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the collected data\n",
    "focus_lexisnexis = pd.DataFrame({\n",
    "    'journal': article_data_lexisnexis['journal'],\n",
    "    'day': article_data_lexisnexis['day'],\n",
    "    'month': article_data_lexisnexis['month'],\n",
    "    'year': article_data_lexisnexis['year'],\n",
    "    'title': article_data_lexisnexis['title'],\n",
    "    'text': article_data_lexisnexis['text'],\n",
    "    'file': article_data_lexisnexis['file']\n",
    "})\n",
    "\n",
    "focus_lexisnexis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa80718",
   "metadata": {},
   "source": [
    "To match the full texts of the loaded articles with their sentiment annotations from the Media Tenor dataset, we follow several key steps. First, we create a date in the same format as in the `sentiment_data` DataFrame. Next, we normalize the titles to ensure accurate matching. We also verify that there are no duplicate articles. After pre-processing, we merge the articles loaded from LexisNexis with their sentiment annotations from the Media Tenor dataset. We then sort the final DataFrame `data_match_lexisnexis` in chronological order and retain only the relevant columns. Through this process, we successfully matched **106** Focus articles from LexisNexis with their sentiment annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2a075af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles from LexisNexis: 106\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>file</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Focus</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>Wo Deutschland am stärksten ist</td>\n",
       "      <td>Im Gesamtranking sichert sich der Landkreis Mü...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Wo Deutschland am st_rksten ist.txt</td>\n",
       "      <td>wo deutschland am stärksten ist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Focus</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>Die Verlierer-Regierer</td>\n",
       "      <td>Mit letzter Kraft schleppen sich Union und SPD...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Die Verlierer- Regierer.txt</td>\n",
       "      <td>die verlierer regierer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Focus</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>Ein Moment Ewigkeit</td>\n",
       "      <td>Eben noch flogen Beschimpfungen und Drohungen ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Panmunjom_ Ein Moment Ewigkeit.txt</td>\n",
       "      <td>ein moment ewigkeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Focus</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>Deutschland entscheidet, ob der Euro überlebt</td>\n",
       "      <td>Als Finanzminister Griechenlands lehnte Yanis ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Deutschland entscheidet_ ob der Euro _berlebt.txt</td>\n",
       "      <td>deutschland entscheidet ob der euro überlebt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Focus</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>Veränderungen tun weh, zittern ist nicht erlaubt</td>\n",
       "      <td>Von Robert Schneider, Chefredakteur. Liebe Les...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Ver_nderungen tun weh_ zittern ist nicht erlau...</td>\n",
       "      <td>veränderungen tun weh zittern ist nicht erlaubt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    journal  day  month  year  \\\n",
       "101   Focus   10      2  2018   \n",
       "102   Focus   10      2  2018   \n",
       "103   Focus    5      5  2018   \n",
       "104   Focus    9      6  2018   \n",
       "105   Focus    1     12  2018   \n",
       "\n",
       "                                                 title  \\\n",
       "101                    Wo Deutschland am stärksten ist   \n",
       "102                             Die Verlierer-Regierer   \n",
       "103                                Ein Moment Ewigkeit   \n",
       "104      Deutschland entscheidet, ob der Euro überlebt   \n",
       "105  Veränderungen tun weh, zittern ist nicht erlaubt    \n",
       "\n",
       "                                                  text  sentiment  \\\n",
       "101  Im Gesamtranking sichert sich der Landkreis Mü...        1.0   \n",
       "102  Mit letzter Kraft schleppen sich Union und SPD...        1.0   \n",
       "103  Eben noch flogen Beschimpfungen und Drohungen ...        1.0   \n",
       "104  Als Finanzminister Griechenlands lehnte Yanis ...       -1.0   \n",
       "105  Von Robert Schneider, Chefredakteur. Liebe Les...       -1.0   \n",
       "\n",
       "                                                  file  \\\n",
       "101                Wo Deutschland am st_rksten ist.txt   \n",
       "102                        Die Verlierer- Regierer.txt   \n",
       "103                 Panmunjom_ Ein Moment Ewigkeit.txt   \n",
       "104  Deutschland entscheidet_ ob der Euro _berlebt.txt   \n",
       "105  Ver_nderungen tun weh_ zittern ist nicht erlau...   \n",
       "\n",
       "                                         title_clean  \n",
       "101                  wo deutschland am stärksten ist  \n",
       "102                           die verlierer regierer  \n",
       "103                              ein moment ewigkeit  \n",
       "104     deutschland entscheidet ob der euro überlebt  \n",
       "105  veränderungen tun weh zittern ist nicht erlaubt  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform month names into month numbers\n",
    "focus_lexisnexis['month_num'] = focus_lexisnexis['month'].map(name_to_number)\n",
    "\n",
    "# Transform single-digit day numbers into two-digit format\n",
    "focus_lexisnexis['day'] = focus_lexisnexis['day'].map(lambda d: day_transform.get(d, d))\n",
    "\n",
    "# Combine day, month, and year into a date string\n",
    "focus_lexisnexis['date'] = focus_lexisnexis.apply(lambda row: f\"{row['day']}.{row['month_num']}.{row['year']}\", axis=1)\n",
    "\n",
    "# Initialize the Normalize class with the titles from the spiegel_lexisnexis DataFrame\n",
    "normalizer = Normalize(focus_lexisnexis.title)\n",
    "\n",
    "# Apply the normalization process to the titles\n",
    "normalized_titles = normalizer.normalized()\n",
    "\n",
    "# Add the normalized titles to the 'focus_lexisnexis' DataFrame as a new column 'title_clean'\n",
    "focus_lexisnexis['title_clean'] = normalized_titles\n",
    "\n",
    "# Merge with sentiment_data on title_clean and date\n",
    "data_match_lexisnexis = pd.merge(sentiment_data, focus_lexisnexis, how='inner', on=['title_clean', 'date'])\n",
    "\n",
    "# Rename the 'month_num' column to 'month'\n",
    "data_match_lexisnexis = data_match_lexisnexis.rename(columns={'month_num': 'month'})\n",
    "\n",
    "# Rename the 'year_y' column to 'year'\n",
    "data_match_lexisnexis = data_match_lexisnexis.rename(columns={'year_y': 'year'})\n",
    "\n",
    "# Convert year, month, and day to integers\n",
    "data_match_lexisnexis['year'] = data_match_lexisnexis['year'].astype(int)\n",
    "data_match_lexisnexis['month'] = data_match_lexisnexis['month'].astype(int)\n",
    "data_match_lexisnexis['day'] = data_match_lexisnexis['day'].astype(int)\n",
    "\n",
    "# Sort the data in chronological order\n",
    "data_match_lexisnexis = data_match_lexisnexis.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "data_match_lexisnexis = data_match_lexisnexis.reset_index(drop=True)\n",
    "\n",
    "# Rename the 'title_y' column to 'title' to reflect the title from the LexisNexis dataset\n",
    "data_match_lexisnexis = data_match_lexisnexis.rename(columns={'title_y': 'title'})\n",
    "\n",
    "# Select only the required columns\n",
    "data_match_lexisnexis = data_match_lexisnexis[['journal', 'day', 'month', 'year', 'title', 'text', 'sentiment', 'file', 'title_clean']]\n",
    "\n",
    "# Print the number of articles from LexisNexis\n",
    "num_lexisnexis_articles = len(data_match_lexisnexis)\n",
    "\n",
    "print(f\"Number of articles from LexisNexis: {num_lexisnexis_articles}\")\n",
    "\n",
    "# Display the last few rows of the final matched dataset\n",
    "data_match_lexisnexis.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e441e",
   "metadata": {},
   "source": [
    "## Combine All Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24cd3b",
   "metadata": {},
   "source": [
    "As the final step, we consolidate all Focus articles, including those that were scraped and those downloaded from Factiva and LexisNexis, into a single DataFrame called `focus_all`. This combined DataFrame is then saved as a CSV file named `focus.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2da75360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles: 719\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>file</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Focus</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>Die große Koalition der Geldausgeber</td>\n",
       "      <td>Die große Koalition der Geldausgeber. Mehr Job...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>die große koalition der geldausgeber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Focus</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zahlen aus der Wirtschaft</td>\n",
       "      <td>Zahlen aus der Wirtschaft. Auf 1,20 US-Dollar ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>zahlen aus der wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Focus</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>\"Das erinnert mich an Weihnachten\"</td>\n",
       "      <td>\"Das erinnert mich an Weihnachten\" Der Wirtsch...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>das erinnert mich an weihnachten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>\"Abgerechnet wird am Schluss\"</td>\n",
       "      <td>\"Abgerechnet wird am Schluss\" Der stellvertret...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>abgerechnet wird am schluss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Focus</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>Vor neuem Höhenflug?</td>\n",
       "      <td>Vor neuem Höhenflug? Der japanische Nikkei-Ind...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>focus_2011_2019.csv</td>\n",
       "      <td>vor neuem höhenflug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  journal  day  month  year                                 title  \\\n",
       "0   Focus   18     11  2013  Die große Koalition der Geldausgeber   \n",
       "1   Focus   18     11  2013             Zahlen aus der Wirtschaft   \n",
       "2   Focus   18     11  2013    \"Das erinnert mich an Weihnachten\"   \n",
       "3   Focus   25     11  2013         \"Abgerechnet wird am Schluss\"   \n",
       "4   Focus   25     11  2013                  Vor neuem Höhenflug?   \n",
       "\n",
       "                                                text  sentiment  \\\n",
       "0  Die große Koalition der Geldausgeber. Mehr Job...        1.0   \n",
       "1  Zahlen aus der Wirtschaft. Auf 1,20 US-Dollar ...        1.0   \n",
       "2  \"Das erinnert mich an Weihnachten\" Der Wirtsch...        1.0   \n",
       "3  \"Abgerechnet wird am Schluss\" Der stellvertret...        1.0   \n",
       "4  Vor neuem Höhenflug? Der japanische Nikkei-Ind...        1.0   \n",
       "\n",
       "                  file                           title_clean  \n",
       "0  focus_2011_2019.csv  die große koalition der geldausgeber  \n",
       "1  focus_2011_2019.csv             zahlen aus der wirtschaft  \n",
       "2  focus_2011_2019.csv      das erinnert mich an weihnachten  \n",
       "3  focus_2011_2019.csv           abgerechnet wird am schluss  \n",
       "4  focus_2011_2019.csv                   vor neuem höhenflug  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all articles from scraped, Factiva, and LexisNexis datasets into a single DataFrame\n",
    "focus_all = pd.concat([data_match_scraped, data_match_factiva, data_match_lexisnexis], sort=False)\n",
    "\n",
    "# Reset the index of the combined DataFrame\n",
    "focus_all = focus_all.reset_index(drop=True)\n",
    "\n",
    "# Print the total number of articles in the combined DataFrame\n",
    "total_articles = len(focus_all)\n",
    "print(f\"Total number of articles: {total_articles}\")\n",
    "\n",
    "# Display the first few rows of the combined DataFrame to verify the merge\n",
    "focus_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2cf1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the combined DataFrame in chronological order\n",
    "focus_all = focus_all.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "focus_all = focus_all.reset_index(drop=True)\n",
    "\n",
    "# Drop the 'title_clean' column as it is no longer needed\n",
    "focus_all = focus_all.drop(columns=['title_clean'])\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "focus_all.to_csv('focus.csv', encoding='utf-8-sig', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
