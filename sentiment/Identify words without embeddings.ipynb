{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de65314d",
   "metadata": {},
   "source": [
    "Before we apply the LSTM model, it's important to identify words in our MediaTenor training set (2011-2020) that do not have corresponding representations in our pre-trained embeddings. The embeddings were trained on a separate set of data from Süddeutsche Zeitung, Welt, dpa, and Handelsblatt from 1991 to 2010. The words that are not represented in the embeddings might fall into one of three categories:\n",
    "\n",
    "1. They could be rare words that seldom appeared in the text corpus used to train the embeddings.\n",
    "2. They could be words that entered common usage after 2010, so they did not appear in the corpus used for the embeddings.\n",
    "3. They could be misspelled or incorrect words in the MediaTenor training set, which is why they did not appear in the embeddings.\n",
    "\n",
    "We will create a list of these words and later exclude them from our texts. This is important to ensure the compatibility of our training set with the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e7f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Open and read articles from the 'articles.txt' file \n",
    "with open('MediaTenor_data/articles.txt', 'r', encoding = 'utf-8') as f:\n",
    "    articles = f.read()\n",
    "\n",
    "# Open and read labels from the 'labels_binary.txt' file    \n",
    "with open('MediaTenor_data/labels_binary.txt', 'r', encoding = 'utf-8') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91b2a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPIEGEL-Gespräch mit Expräsident Lula da Silva über Korruptionsvorwürfe und seine Nachfolgerin Dilma Rousseff. Der ehemalige brasilianische Präsident Luiz Inácio Lula da Silva, 70, spricht über die Krise in seinem Land, wehrt sich gegen Korruptionsvorwürfe und verteidigt seine Nachfolgerin Dilma Rousseff. SPIEGEL: Herr Präsident, in einem Monat beginnen in Rio de Janeiro die Olympischen Spiele, aber das Land befindet sich in einer schweren politischen und wirtschaftlichen Krise. Als Brasilien den Zuschlag bekam, galt es als kommender Star unter den Schwellenländern. Wie konnte es zu diesem Absturz kommen? Lula: Als ich die Olympischen Spiele und die Fußballweltmeisterschaft nach Brasilien geholt habe, glaubte ich tatsächlich, wir würden uns bis 2016 im Kreis der fünf oder sechs größten Wirtschaftsmächte etablieren. Aber wir leiden noch immer unter einer weltweiten Finanzkrise. Die USA haben sich nicht wirklich erholt, Europa steckt weiter in der Krise, Chinas Wirtschaftsleistung geht z\n",
      "\n",
      "negative\n",
      "positive\n",
      "ne\n"
     ]
    }
   ],
   "source": [
    "# Print the first 1000 characters of the articles content\n",
    "print(articles[:1000])\n",
    "print()\n",
    "\n",
    "# Print the first 20 characters of the labels content\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcd6e1",
   "metadata": {},
   "source": [
    "To correctly identify words in the MediaTenor training set that do not have a corresponding embedding representation, we must ensure that we pre-process MediaTenor data using the same steps as were applied to the main corpus during the embeddings training. These steps include transforming all characters to lowercase, removing URLs, removing all punctuation and non-alphabetic characters, consolidating multiple whitespaces into a single one, and eliminating single-letter tokens from the text. However, an additional step specific to MediaTenor data is the exclusion of metadata. These words or phrases are not part of the actual content we wish to analyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7be0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    \"\"\"\n",
    "    This function removes multiple spaces in a string. \n",
    "    It uses a regular expression to match 2 or more spaces and replaces them with a single space.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_short_words(text):\n",
    "    \"\"\"\n",
    "    This function removes words of length 1 from a string.\n",
    "    \"\"\"\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])\n",
    "    return text\n",
    "\n",
    "def remove_metadata(text, meta_list):\n",
    "    \"\"\"\n",
    "    This function removes metadata from a text.\n",
    "    Metadata is a list of phrases. If any of these phrases are found in the text,\n",
    "    everything from the phrase and onwards is cut off.\n",
    "    \"\"\"\n",
    "    for phrase in meta_list:\n",
    "        if phrase in text:\n",
    "            text = text.split('dokument', 1)[0]\n",
    "    return text\n",
    "\n",
    "# List of metadata phrases\n",
    "metadata_phrases = ['dokument bihann', 'dokument bid', 'dokument welt', 'dokument bberbr', 'dokument focus']\n",
    "\n",
    "# Convert articles to lowercase\n",
    "articles = articles.lower()\n",
    "\n",
    "# Remove URLs\n",
    "articles = re.sub(r'https\\S+|http\\S+|www.\\S+', '', articles)\n",
    "\n",
    "# Remove punctuation\n",
    "articles = articles.replace('.', ' ').replace('-', ' ').replace('/', ' ')\n",
    "articles = ''.join([c for c in articles if c not in punctuation and c not in ['»', '«']])\n",
    "\n",
    "# Remove non-alphabetic characters from the text\n",
    "articles = ''.join([c for c in articles if (c.isalpha() or c in [' ', '\\n'])])\n",
    "\n",
    "# Split articles by new lines\n",
    "articles_split = articles.split('\\n')\n",
    "\n",
    "# Remove multiple spaces, short words, and metadata\n",
    "articles_split = list(map(remove_multiple_spaces, articles_split))\n",
    "articles_split = list(map(remove_short_words, articles_split))\n",
    "articles_split = list(map(lambda text: remove_metadata(text, metadata_phrases), articles_split))\n",
    "\n",
    "# Join all articles into a single string\n",
    "all_text = ' '.join(articles_split)\n",
    "\n",
    "# Create a list of all words in the MediaTenor data\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c36b98",
   "metadata": {},
   "source": [
    "Next, we identify words in the vocabulary that don't have a pre-trained vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2dd1152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 116935 / 141222 pretrained vectors found.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Count the occurrences of each word in the articles\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Sort words by their count, in descending order\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "# Create a dictionary that maps each word to a unique integer\n",
    "vocab_to_int = {word: idx for idx, word in enumerate(sorted_vocab)}\n",
    "\n",
    "# Set the path variable to point to the 'word_embeddings' directory.\n",
    "path = os.getcwd().replace('\\\\sentiment', '') + '\\\\word_embeddings'\n",
    "\n",
    "# Identify words in the vocabulary that do have a pre-trained vector\n",
    "with open(path + '\\\\news_word2vec.txt', 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "    pretrained_vectors_found = 0\n",
    "    words_with_pretrained_vector = []\n",
    "    for line in f:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        word = tokens[0]\n",
    "        if word in vocab_to_int:\n",
    "            pretrained_vectors_found += 1\n",
    "            words_with_pretrained_vector.append(word)\n",
    "    print(f\"There are {pretrained_vectors_found} / {len(vocab_to_int)} pretrained vectors found.\")\n",
    "    \n",
    "# Create a list of words in the vocabulary that don't have a pre-trained vector\n",
    "words_without_pretrained_vector = list((Counter(list(vocab_to_int.keys())) - Counter(words_with_pretrained_vector)).elements())\n",
    "\n",
    "# Save words without pre-trained vector to a csv file\n",
    "with open(\"words_without_pretrained_vector.csv\", 'w', encoding='utf-8-sig') as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL,  delimiter='\\n')\n",
    "    writer.writerow(words_without_pretrained_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env_gpu",
   "language": "python",
   "name": "py3_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
