{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26046c67",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine (LSVM) Benchmark\n",
    "\n",
    "In this notebook, we use a Linear SVM (LSVM) model as a benchmark for the LSTM model. Both models are trained on the same training articles and tested on the same test articles. The hyperparameter `C` is tuned using cross-validation, where `C` controls the margin's hardness: a larger `C` enforces a stricter, hard margin, while a smaller `C` softens the margin, allowing some misclassifications. The goal is to compare the performance of the LSVM to the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42464146",
   "metadata": {},
   "source": [
    "We first load the MTI articles and their corresponding labels. The data is then split into training and test sets using predefined indices, ensuring they are identical to those used for the LSTM model. Finally, we combine the train and test articles and labels into two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f587b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Open and read articles from the 'articles.txt' file \n",
    "with open('MediaTenor_data/articles.txt', 'r', encoding = 'utf-8') as f:\n",
    "    articles = f.read().split('\\n')  # Splitting into a list of articles\n",
    "\n",
    "# Open and read labels from the 'labels_binary.txt' file    \n",
    "with open('MediaTenor_data/labels_binary.txt', 'r', encoding = 'utf-8') as f:\n",
    "    labels = f.read().split('\\n')  # Splitting into a list of labels\n",
    "    \n",
    "# Load the train indices from the CSV file\n",
    "with open('train_indices.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    train_indices = list(map(int, next(reader))) \n",
    "    \n",
    "# Load the test indices from the CSV file\n",
    "with open('test_indices.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    test_indices = list(map(int, next(reader))) \n",
    "\n",
    "# Filter articles and labels for the training set\n",
    "train_articles = [articles[i] for i in train_indices]\n",
    "train_labels = [labels[i] for i in train_indices]\n",
    "\n",
    "# Filter articles and labels for the test set\n",
    "test_articles = [articles[i] for i in test_indices]\n",
    "test_labels = [labels[i] for i in test_indices]\n",
    "\n",
    "# Combine train and test articles\n",
    "all_articles = train_articles + test_articles\n",
    "\n",
    "# Combine train and test labels\n",
    "all_labels = train_labels + test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696a315",
   "metadata": {},
   "source": [
    "We pre-process the articles by retaining only the sentences that contain at least one word related to business cycle conditions. Then, we perform the following normalization steps: we lowercase all articles, remove URLs, remove punctuation, and strip non-alphabetic characters. Additionally, we remove multiple spaces, single-letter tokens, stopwords, and metadata from the text. Finally, we lemmatize the articles to reduce words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500c868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mokuneva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:52.803050\n",
      "SPIEGEL: Herr Präsident, in einem Monat beginnen in Rio de Janeiro die Olympischen Spiele, aber das Land befindet sich in einer schweren politischen und wirtschaftlichen Krise. Die USA haben sich nicht wirklich erholt, Europa steckt weiter in der Krise, Chinas Wirtschaftsleistung geht zurück. Lula: Ich habe Dilma immer gesagt: Du redest zu viel mit Ökonomen, wir brauchen mehr Politik. Es ist möglich, dass viele von euch ihren Job verlieren, aber wenn ihr jetzt Angst habt und nicht mehr konsumiert, dann steigt die Arbeitslosigkeit erst recht. Lula: Ich denke, man muss die Dinge im Zusammenhang sehen: der wirtschaftliche Abschwung, der knappe Ausgang der letzten Wahlen, das vergiftete Klima in einer immer stärker polarisierten Gesellschaft. Viel mehr Sorgen bereitet mir, dass es in unserer Demokratie offenbar möglich ist, ein Opfer solcher Lügen zu werden.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import multiprocessing as mp \n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import keep_economy_related_sentences\n",
    "\n",
    "NUM_CORE = 60 # set the number of cores to use\n",
    "\n",
    "# Set the path variable to point to the 'word_embeddings' directory.\n",
    "path = os.getcwd().replace('\\\\sentiment', '') + '\\\\word_embeddings'\n",
    "\n",
    "# Load words related to 'Wirtschaft' and 'Konjunktur'\n",
    "konjunktur_words = keep_economy_related_sentences.load_words(path + '\\\\konjunktur_synonyms.txt')\n",
    "wirtschaft_words = keep_economy_related_sentences.load_words(path + '\\\\wirtschaft_synonyms.txt')\n",
    "\n",
    "# Combine the two lists\n",
    "economy_related_words = konjunktur_words + wirtschaft_words\n",
    "\n",
    "startTime = datetime.now() \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    inputs = zip(all_articles, [economy_related_words]*len(all_articles))\n",
    "    economy_related_sentences = pool.starmap(keep_economy_related_sentences.keep_economy_related_sentences, inputs) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "print(economy_related_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bce78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty string to store the segments of articles related to business cycle conditions\n",
    "articles = ''\n",
    "\n",
    "for article in economy_related_sentences:\n",
    "    articles = articles + article + ' \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd129a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import spacy\n",
    "from string import punctuation\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    \"\"\"\n",
    "    This function removes multiple spaces in a string. \n",
    "    It uses a regular expression to match 2 or more spaces and replaces them with a single space.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_short_words(text):\n",
    "    \"\"\"\n",
    "    This function removes words of length 1 from a string.\n",
    "    \"\"\"\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])\n",
    "    return text\n",
    "\n",
    "def remove_metadata(text, meta_list):\n",
    "    \"\"\"\n",
    "    This function removes metadata from a text.\n",
    "    Metadata is a list of phrases. If any of these phrases are found in the text,\n",
    "    everything from the phrase and onwards is cut off.\n",
    "    \"\"\"\n",
    "    for phrase in meta_list:\n",
    "        if phrase in text:\n",
    "            text = text.split('dokument', 1)[0]\n",
    "    return text\n",
    "\n",
    "# Initialize an empty list to hold stopwords\n",
    "stopwords = []\n",
    "\n",
    "# Read in a list of German stopwords from a text file\n",
    "with codecs.open('stopwords.txt', 'r', 'utf-8-sig') as input_data:\n",
    "    for line in input_data:\n",
    "        # Remove trailing whitespaces and convert to lowercase, then append the stopword\n",
    "        stopwords.append(line.strip().lower())\n",
    "        \n",
    "# Define stopwords that should be kept for sentiment analysis (e.g., negation words)\n",
    "sw_keep = ['kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'nicht', 'nichts', \n",
    "           'aber', 'doch', 'gegen', 'ohne', 'sondern', 'sonst']\n",
    "\n",
    "# Make sure that important stopwords like 'kein' and 'nicht' are not removed from the articles\n",
    "stopwords = [word for word in stopwords if word not in sw_keep]\n",
    "\n",
    "def remove_stopwords(text, stopwords=stopwords):\n",
    "    \"\"\"\n",
    "    Removes stopwords from the input text based on the defined stopwords list.\n",
    "    \"\"\"\n",
    "    # Split the text into words, filter out the stopwords, and join back into a string\n",
    "    cleaned_text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return cleaned_text\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "def lemmatization(text):\n",
    "    \"\"\"\n",
    "    Lemmatizes the input text using SpaCy's German language model.\n",
    "    \"\"\"\n",
    "    # Apply the SpaCy model to process the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Return the lemmatized text by joining lemmatized tokens\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "# List of metadata phrases\n",
    "metadata_phrases = ['dokument bihann', 'dokument bid', 'dokument welt', 'dokument bberbr', 'dokument focus']\n",
    "\n",
    "# Convert articles to lowercase\n",
    "articles = articles.lower()\n",
    "\n",
    "# Remove URLs\n",
    "articles = re.sub(r'https\\S+|http\\S+|www.\\S+', '', articles)\n",
    "\n",
    "# Remove punctuation\n",
    "articles = articles.replace('.', ' ').replace('-', ' ').replace('/', ' ')\n",
    "articles = ''.join([c for c in articles if c not in punctuation and c not in ['»', '«']])\n",
    "\n",
    "# Remove non-alphabetic characters from the text\n",
    "articles = ''.join([c for c in articles if (c.isalpha() or c in [' ', '\\n'])])\n",
    "\n",
    "# Split articles by new lines\n",
    "articles_split = articles.split('\\n')[:-1]\n",
    "\n",
    "# Remove multiple spaces, single-letter tokens, stopwords and metadata\n",
    "articles_split = list(map(remove_multiple_spaces, articles_split))\n",
    "articles_split = list(map(remove_short_words, articles_split))\n",
    "articles_split = list(map(remove_stopwords, articles_split))\n",
    "articles_split = list(map(lambda text: remove_metadata(text, metadata_phrases), articles_split))\n",
    "\n",
    "# Lemmatize each article\n",
    "articles_split = list(map(lemmatization, articles_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2714c8b",
   "metadata": {},
   "source": [
    "Next, we encode the labels into a binary format, where 'positive' (representing positive or no clear tone class) is mapped to 1, and 'negative' is mapped to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e78af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert labels to binary format: 1 for 'positive' and 0 for 'negative'\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in all_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb151e00",
   "metadata": {},
   "source": [
    "Next, we split the pre-processed articles and encoded labels into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e82e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the articles and labels into training and test sets\n",
    "X_train = articles_split[:len(train_articles)]\n",
    "X_test = articles_split[len(train_articles):]\n",
    "y_train = encoded_labels[:len(train_articles)]\n",
    "y_test = encoded_labels[len(train_articles):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d07d51",
   "metadata": {},
   "source": [
    "We will use TF-IDF vectorizer to convert the content of each article into a vector of numbers. TF-IDF statistic reflects how important a word is to a document in a corpus.\n",
    "\n",
    "$$\\text{tf-idf}_{v,d} = \\log(1+N_{v,d})× \\log⁡\\left(\\frac{D}{D_v}\\right)$$\n",
    "\n",
    "where\n",
    "$N_{v,d}$ – the count of the token $v$ in the the document $d$,\n",
    "$D_v$ – the number of documents that contain the term $v$, \n",
    "$D$ – the total number of documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da190272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer to convert the text into a matrix of TF-IDF features\n",
    "# smooth_idf=False: This prevents adding 1 to the document frequencies.\n",
    "# Since we are not using a predefined vocabulary, we don't need this smoothing adjustment.\n",
    "tf_idf = TfidfVectorizer(smooth_idf=False)\n",
    "\n",
    "# Fit the vectorizer on the training data and transform it into a document-term matrix\n",
    "# This learns the vocabulary and calculates the inverse document frequencies (IDF)\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same vocabulary and IDF values learnt from the training data\n",
    "X_test_tfidf = tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418baa13",
   "metadata": {},
   "source": [
    "We perform cross-validation to tune the hyperparameter `C`, which controls the hardness of the margin in the LinearSVM model. For each value of `C` between 0.1 and 2, the F1 score (weighted by class support) is calculated using stratified 5-fold cross-validation. The model with the highest average F1 score is selected as the best model.\n",
    "\n",
    "$$F1 = 2 * \\frac{precision * recall}{precision + recall}$$\n",
    "\n",
    "where $$precision = \\frac{\\text{True Positives}}{\\text{Predicted Yes}}$$\n",
    "\n",
    "$$recall = \\frac{\\text{True Positives}}{\\text{Actual Yes}}$$\n",
    "\n",
    "F1 score conveys the balance between the precision and the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d0b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def get_f1_CV(model):\n",
    "    \"\"\"\n",
    "    Calculate the average F1 score using cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    model: The machine learning model to evaluate.\n",
    "    \n",
    "    Returns:\n",
    "    float: The average F1 score from cross-validation.\n",
    "    \"\"\"\n",
    "    # Set up Stratified K-Folds cross-validator with 5 folds\n",
    "    # StratifiedKFold ensures that each fold maintains the class distribution\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    # Perform cross-validation and calculate the F1 scores\n",
    "    # scoring=\"f1_weighted\" calculates the F1 score, weighted by support (number of true instances for each label)\n",
    "    f1_scores = cross_val_score(model, X_train_tfidf, y_train, scoring=\"f1_weighted\", cv=kf)\n",
    "    \n",
    "    # Return the average F1 score across the folds\n",
    "    return f1_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df812e1e",
   "metadata": {},
   "source": [
    "In this case, a `C` value of 0.1 is found to yield the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617dbb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameter tuning for LinearSVC using cross-validation\n",
    "# class_weight='balanced': Adjusts weights inversely proportional to class frequencies in the input data\n",
    "\n",
    "# Perform cross-validation for each value of C in the range 0.1 to 2 (step size of 0.1)\n",
    "# Store the average F1 score for each C value\n",
    "res = pd.Series([get_f1_CV(LinearSVC(C=i, class_weight='balanced', dual=\"auto\")) \n",
    "                 for i in np.arange(0.1, 2, 0.1)],\n",
    "                index=np.arange(0.1, 2, 0.1))\n",
    "\n",
    "# Find the value of C that results in the highest F1 score\n",
    "best_c = np.round(res.idxmax(), 2)\n",
    "print('Best C: ', best_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843e95a",
   "metadata": {},
   "source": [
    "We then fit the model with the optimal `C` parameter found through cross-validation to the training data and use it to predict the labels for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d92ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LinearSVC model with the best hyperparameter C found earlier\n",
    "# 'C=best_c': Best regularization parameter selected from cross-validation\n",
    "svc_model = LinearSVC(C=best_c, class_weight='balanced', dual=\"auto\")\n",
    "\n",
    "# Fit the model to the training data (TF-IDF transformed)\n",
    "svc_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Use the trained model to predict the labels for the test data\n",
    "predictions = svc_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a8bf4",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance of the LSVM model using a classification report and confusion matrix. The overall accuracy is 66.4%, which is nearly identical to the 66.8% accuracy of the LSTM model. The F1 scores are also similar: 0.66 for LSVM and 0.67 for LSTM. This suggests that both models perform at a comparable level, likely because the training set (1920 articles) is relatively small, limiting the advantage neural networks typically have with larger datasets. However, there is one notable difference: LSVM performs better at classifying negative articles, achieving 70% accuracy for the negative class and 63% for the positive/no clear tone class. In contrast, the LSTM shows the opposite pattern, with 71% accuracy for positive/no clear tone articles and 62% for the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13dcd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67       124\n",
      "           1       0.69      0.63      0.66       132\n",
      "\n",
      "    accuracy                           0.66       256\n",
      "   macro avg       0.67      0.67      0.66       256\n",
      "weighted avg       0.67      0.66      0.66       256\n",
      "\n",
      "Confusion Matrix:\n",
      "[[87 37]\n",
      " [49 83]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Generate and print the classification report\n",
    "# This report includes precision, recall, F1-score, and support for each class\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Compute and display the confusion matrix\n",
    "# The matrix aligns true labels with rows and predicted labels with columns\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env_gpu",
   "language": "python",
   "name": "py3_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
